,Unnamed: 0.1,Unnamed: 0,PMID,DOIS,DESTINATION,ARTICLES
0,0,10,36822739,https://www.doi.org/10.1053/j.ajkd.2022.12.001,https://linkinghub.elsevier.com/,"The United States Renal Data System 2022 Annual Data Report (ADR) contains data from medical claims through 2020 and for some end-stage renal disease (ESRD)-related metrics through the first half of 2021. As such, this is the first year in which the wide-ranging effects of the coronavirus disease 2019 (COVID-19) pandemic on the chronic kidney disease (CKD) and ESRD populations can be placed into the full context of the years that preceded its onset. Another important aspect of this year’s ADR is a continued focus on racial, ethnic, and socioeconomic disparities in access to health care and outcomes. Unfortunately, these 2 topics intersect in the form of racial and ethnic disparities in vulnerability and susceptibility to COVID-19 that further exposed and exacerbated pre-existing disparities in incidence, prevalence, and care of patients with kidney disease.,The devastating effects of the COVID-19 pandemic on the CKD and ESRD populations permeate the 2022 ADR. The 2022 ADR includes data on medical care and outcomes through calendar year 2020 (although some data for the first half of 2021 for the ESRD population are included). Although we presented early views into the impact of the COVID-19 pandemic on the ESRD population in the 2020 ADR and expanded these analyses to include examination of COVID-19 diagnoses and outcomes in the CKD population in the 2021 ADR, the full magnitude of the direct and indirect effects of the pandemic on these populations comes into sharp focus throughout this year’s report.,The direct effects of COVID-19 can be measured by examining patterns of patient testing, hospitalization, and mortality. More than 10% of patients with CKD, 13% of patients with a kidney transplant, and 20% of patients receiving dialysis in January of 2020 were diagnosed with COVID-19 by the end of June 2021, rates that were approximately 50%, 100%, and 200% higher than that of Medicare beneficiaries without CKD, respectively. Although the incidence of COVID-19 testing was higher among individuals with CKD, the incidence of hospitalization after COVID-19 diagnosis among patients with CKD was more than double that of patients without CKD in 2020; patients receiving dialysis consistently had hospitalization rates higher still than those with CKD. Mortality at 14, 30, and 90 days after diagnosis of COVID-19 was more than twice as high among beneficiaries with CKD as among those without. Nearly one-quarter of patients with CKD who were diagnosed with COVID-19 died within 90 days. Mortality after COVID-19 diagnosis was even higher for patients with ESRD, reaching 40.5% for patients receiving dialysis and 44.1% among kidney transplant recipients 90 days after diagnosis.,The ultimate result of the higher incidence of COVID-19 and higher mortality after diagnosis of COVID-19 among patients with CKD and ESRD was the unprecedented shrinking of the prevalence of diagnosed CKD (CKD Volume, Fig 2.1) and ESRD (ESRD Volume, Fig 1.5) in 2020. For example, the absolute increase in mortality in 2020 among patients with CKD was 8.9 deaths per 1,000 person-years, compared with 6.1 per 1,000 person-years among individuals without CKD (Fig 3.2a). By the end of 2020, the population with diagnosed CKD had decreased by 4.3% from 2019. Some of the decline in the CKD population in 2020 may have been an artifact of fewer opportunities to accrue CKD diagnoses given the lower rates of hospitalization that occurred in 2020, combined with a general reduction in overall interaction with the health care system that occurred across the United States. However, this is not true of the ESRD population, whose tracking through EQRS (End Stage Renal Disease Quality Reporting System) and OPTN (Organ Procurement and Transplantation Network) does not rely on diagnosis claims. As a result of fewer patients reaching diagnosed ESRD and the increase in mortality rate among patients with ESRD attributable to the pandemic and its effects, the rate of prevalent ESRD decreased by almost 2% in 2020.,Indirect pandemic effects may have also affected patients with CKD to a greater extent than patients without kidney disease. Rates of emergency department encounters and hospitalizations decreased substantially in 2020 among patients with and without CKD and ESRD. However, the negative consequences of forgoing this medical care may have led to a disproportionate increase in mortality among patients with CKD because of their higher comorbidity burden and higher a priori risk of adverse outcomes. For example, mortality increased more in 2020 among patients with stage 4 and 5 CKD than among patients with stage 3 CKD.,Mortality after COVID-19 was higher among Black and Hispanic Medicare beneficiaries with CKD than among White beneficiaries (Fig 13.5). As a direct result of this higher COVID-19-related mortality and, possibly, more limited access to non-COVID-19-related medical care, mortality increased more among Black than among White beneficiaries with stage 4 and 5 CKD in 2020. This resulted in a reversal of the longstanding observation of lower mortality among Black patients. In other words, whereas Black beneficiaries had lower mortality than White ones in 2019 and prior years, they had higher mortality than their White counterparts in 2020 (Fig 3.2b). Although unadjusted mortality was not higher among Black than among White patients with ESRD, rates of COVID-19 diagnosis were higher (Fig 13.2b). A similar reversal of the Black-White mortality difference occurred in transplant recipients: mortality was higher among White recipients in 2019 but among Black recipients in 2020 (Fig 6.5b). The mortality difference did not reverse among patients treated with dialysis, but it did narrow from 43% higher mortality among White patients in 2019 to only 30% higher mortality in 2020.,Other direct and indirect effects of COVID-19 and the changes in availability and delivery of health care that occurred in 2020 can be seen throughout the ADR and in many metrics typically tracked in the CKD and ESRD populations.,Examples among patients with CKD include:•Among patients with CKD, there was a particularly steep reduction in the rate of all-cause hospitalization in 2020 (14.9%); this single-year decrease was larger than the cumulative change over the previous 6-year period from 2013 to 2019 (Fig 3.4). Rates of emergency department encounters were also considerably lower in 2020 (Fig 3.15d).•Although rates of readmission within 30 days of hospital discharge remained stable in 2020 among patients with CKD, the rate of death within 30 days of hospital discharge (without rehospitalization) increased by almost 20% (Figs 3.13 and 3.14d).•Although the overall rate of hospitalization with acute kidney injury (AKI) decreased in 2020 (likely related to the overall reduction in hospitalization rate; Fig 4.1), AKI events worsened, as indicated by a 16% increase in the need for dialysis among those with AKI (Fig 4.2). This increase occurred after a sustained period of decreasing need for dialysis during hospitalization with AKI.•Outcomes after AKI requiring dialysis (AKI-D) during a COVID-19 hospitalization in 2020-2021 were substantially worse than outcomes after AKI-D without COVID-19: almost three-quarters (74.1%) of those with COVID-19 and AKI-D died during hospitalization or were discharged to hospice care, compared with 35.2% of those without COVID-19 (Fig 4.4c).•Inflation-adjusted overall Medicare fee-for-service (FFS) spending for older (≥66 years) beneficiaries with non-dialysis-dependent CKD decreased by almost 3% in 2020, or by ∼$2.2B (Fig 6.3). Inpatient spending decreased by 4%, which was a combination of a ∼$2.4B decrease in spending on non-COVID-19 hospitalization plus approximately $1.4B in spending for COVID-19 hospitalization (Fig 6.4).•Hydroxychloroquine and chloroquine use among older Medicare Part D beneficiaries with CKD spiked in March and April 2020 (Fig 7.13), at a time when rates of receipt of many common drugs used to treat hypertension or CKD transiently declined (Fig 7.10). Rates of ivermectin use increased dramatically in November and December of 2020 among older Medicare Part D beneficiaries with CKD (Fig 7.14).,Among patients with CKD, there was a particularly steep reduction in the rate of all-cause hospitalization in 2020 (14.9%); this single-year decrease was larger than the cumulative change over the previous 6-year period from 2013 to 2019 (Fig 3.4). Rates of emergency department encounters were also considerably lower in 2020 (Fig 3.15d).,Although rates of readmission within 30 days of hospital discharge remained stable in 2020 among patients with CKD, the rate of death within 30 days of hospital discharge (without rehospitalization) increased by almost 20% (Figs 3.13 and 3.14d).,Although the overall rate of hospitalization with acute kidney injury (AKI) decreased in 2020 (likely related to the overall reduction in hospitalization rate; Fig 4.1), AKI events worsened, as indicated by a 16% increase in the need for dialysis among those with AKI (Fig 4.2). This increase occurred after a sustained period of decreasing need for dialysis during hospitalization with AKI.,Outcomes after AKI requiring dialysis (AKI-D) during a COVID-19 hospitalization in 2020-2021 were substantially worse than outcomes after AKI-D without COVID-19: almost three-quarters (74.1%) of those with COVID-19 and AKI-D died during hospitalization or were discharged to hospice care, compared with 35.2% of those without COVID-19 (Fig 4.4c).,Inflation-adjusted overall Medicare fee-for-service (FFS) spending for older (≥66 years) beneficiaries with non-dialysis-dependent CKD decreased by almost 3% in 2020, or by ∼$2.2B (Fig 6.3). Inpatient spending decreased by 4%, which was a combination of a ∼$2.4B decrease in spending on non-COVID-19 hospitalization plus approximately $1.4B in spending for COVID-19 hospitalization (Fig 6.4).,Hydroxychloroquine and chloroquine use among older Medicare Part D beneficiaries with CKD spiked in March and April 2020 (Fig 7.13), at a time when rates of receipt of many common drugs used to treat hypertension or CKD transiently declined (Fig 7.10). Rates of ivermectin use increased dramatically in November and December of 2020 among older Medicare Part D beneficiaries with CKD (Fig 7.14).,Additional examples of direct or indirect effects of COVID-19 on patients with ESRD are also numerous:•The percentage of patients with incident ESRD in 2020 starting in-center hemodialysis decreased, and the corresponding percentage starting peritoneal dialysis increased (Fig 1.2).•The percentage of older Medicare beneficiaries with ESRD and diabetes who received preventive care, including glycated hemoglobin (A1c) testing, lipid testing, and eye examination, decreased in 2020 (Fig 3.15). Only about 25.6% of patients received all 3 of these examinations, compared with 31.4% in 2019.•The percentage of patients initiating hemodialysis with a catheter increased in 2020 to 71.2%, and the corresponding percentage initiating with an arteriovenous fistula (AVF) decreased to 25% overall (including AVFs that were maturing or were in use, or 14.1% for AVFs used at dialysis initiation; Fig 4.1).•The number of patients with ESRD newly added to the kidney transplant waitlist in 2020 decreased by 12% (Fig 7.1). There was a corresponding decrease in the total number of patients with ESRD on the waitlist that was particularly pronounced for those listed with active status (Fig 7.2). The percentage of dialysis patients on the kidney transplant waitlist also declined in 2020 (Fig 7.3).•The rate of receipt of living donor kidney transplants among patients receiving dialysis decreased by 27.3% in 2020 (Fig 7.10a).•Adjusted 1-year post–kidney transplant patient and allograft survival decreased in 2020 among recipients of living and deceased donor kidney transplants (Figs 7.19a and 7.20a).•The number of children with incident ESRD decreased in 2020, driven primarily by a reduction in the number who received a preemptive kidney transplant (Figs 8.1 and 8.3b).•The rates of kidney transplantation among children receiving dialysis decreased by 6% in 2020 (Fig 8.13).•Total Medicare spending for beneficiaries with ESRD decreased by $2.2B in 2020 (Fig 9.1). Medicare FFS spending for ESRD beneficiaries as a percentage of total FFS spending decreased to 6.1% in 2020 after 10 years at 7.1% to 7.2% (Fig 9.3). The biggest (inflation-adjusted) decrease in FFS spending for ESRD was for outpatient spending (-$1B), and inpatient spending decreased by $0.6B (Fig 9.5).•Hydroxychloroquine and chloroquine use among Medicare Part D beneficiaries with ESRD spiked in March and April 2020 (Fig 10.13). Rates of ivermectin use increased dramatically in November and December of 2020 among Medicare Part D beneficiaries with ESRD (Fig 10.14).,The percentage of patients with incident ESRD in 2020 starting in-center hemodialysis decreased, and the corresponding percentage starting peritoneal dialysis increased (Fig 1.2).,The percentage of older Medicare beneficiaries with ESRD and diabetes who received preventive care, including glycated hemoglobin (A1c) testing, lipid testing, and eye examination, decreased in 2020 (Fig 3.15). Only about 25.6% of patients received all 3 of these examinations, compared with 31.4% in 2019.,The percentage of patients initiating hemodialysis with a catheter increased in 2020 to 71.2%, and the corresponding percentage initiating with an arteriovenous fistula (AVF) decreased to 25% overall (including AVFs that were maturing or were in use, or 14.1% for AVFs used at dialysis initiation; Fig 4.1).,The number of patients with ESRD newly added to the kidney transplant waitlist in 2020 decreased by 12% (Fig 7.1). There was a corresponding decrease in the total number of patients with ESRD on the waitlist that was particularly pronounced for those listed with active status (Fig 7.2). The percentage of dialysis patients on the kidney transplant waitlist also declined in 2020 (Fig 7.3).,The rate of receipt of living donor kidney transplants among patients receiving dialysis decreased by 27.3% in 2020 (Fig 7.10a).,Adjusted 1-year post–kidney transplant patient and allograft survival decreased in 2020 among recipients of living and deceased donor kidney transplants (Figs 7.19a and 7.20a).,The number of children with incident ESRD decreased in 2020, driven primarily by a reduction in the number who received a preemptive kidney transplant (Figs 8.1 and 8.3b).,The rates of kidney transplantation among children receiving dialysis decreased by 6% in 2020 (Fig 8.13).,Total Medicare spending for beneficiaries with ESRD decreased by $2.2B in 2020 (Fig 9.1). Medicare FFS spending for ESRD beneficiaries as a percentage of total FFS spending decreased to 6.1% in 2020 after 10 years at 7.1% to 7.2% (Fig 9.3). The biggest (inflation-adjusted) decrease in FFS spending for ESRD was for outpatient spending (-$1B), and inpatient spending decreased by $0.6B (Fig 9.5).,Hydroxychloroquine and chloroquine use among Medicare Part D beneficiaries with ESRD spiked in March and April 2020 (Fig 10.13). Rates of ivermectin use increased dramatically in November and December of 2020 among Medicare Part D beneficiaries with ESRD (Fig 10.14).,In the 2021 ADR, we did not observe disparities in rates of outpatient nephrology visits or receipt of medications to treat CKD or its complications, including angiotensin-converting enzyme inhibitors or angiotensin receptor blockers, oral potassium or phosphorus binders, or sodium-glucose cotransporter-2 inhibitors, by race/ethnicity. Rates of nephrology encounters also differed little by level of neighborhood deprivation. These results suggest that Medicare coverage, including Part D and the Low Income Subsidy, appeared to provide comparable access to care for CKD across race/ethnicity groups and across levels of neighborhood deprivation. We hypothesized that barriers to access to care before Medicare eligibility likely contribute to the higher rates and earlier onset of diabetes and hypertension among Black and Hispanic individuals as well as to the higher risk of subsequent CKD and ESRD. To address this question, this year’s ADR includes data on younger Medicaid beneficiaries aged 18 to 64 years, in which we examined access to medications and nephrology care in these younger patients. We again found little disparity by race/ethnicity or by neighborhood in receipt of medications or nephrology encounters. However, rates of nephrology encounters among younger Medicare beneficiaries were less than half those among older Medicare beneficiaries. Thus, the younger, more heavily Black, Hispanic, and lower-socioeconomic-status Medicaid population appeared to have considerably less access to nephrology care. Medicaid coverage may provide less access to nephrology care than Medicare coverage, or insurance coverage may be insufficient to overcome barriers to accessing care experienced by younger patients with low socioeconomic status, such as transportation or concerns about loss of work income. Furthermore, limitations in access to Medicaid (eg, across U.S. states) likely introduce further disparities that cannot be examined using medical claims, as uninsured patients almost certainly have more limited access to care. Further examination of these issues using more detailed data sources will be critical to developing and implementing strategies to address health care disparities."
1,1,33,37314770,https://www.doi.org/10.2215/CJN.0000000000000221,https://journals.lww.com/,"The clinical benefits of sodium‐glucose cotransporter‐2 (SGLT2) inhibitors in patients with CKD have been well established in major clinical outcome trials.1,2 Secondary analyses from these landmark trials demonstrated consistent benefits of SGLT2 inhibitors across many subgroups, supporting the widespread use and incorporation in clinical practice guidelines of this new kidney and cardioprotective drug class.3,4 Notwithstanding, there are three important groups of kidney patients in whom SGLT2 inhibitors are not indicated. First are patients with severely impaired kidney function. This seems sensible because the efficacy of SGLT2 inhibitors in these patients could well be diminished. SGLT2 inhibitors block the reabsorption of glucose and sodium in the proximal tubule and enhance glycosuria and natriuresis. In patients with a low number of functioning nephrons, it may be expected that SGLT2 transporters are less effective because the number of SGLT2 transporters containing tubules is reduced. Therefore, patients with an eGFR <20 ml/min per 1.73 m2 and those receiving dialysis were excluded from recent outcome trials. Yet, there are emerging data suggesting that SGLT2 inhibitors may reduce clinically relevant outcomes in these vulnerable high-risk patients.,The DAPA-CKD and EMPA-KIDNEY trials recruited 2906 patients with CKD stage 4 who had an eGFR>25 (DAPA-CKD; N=624) or >20 ml/min per 1.73 m2 (EMPA-Kidney; N=2282).1,2 In both trials, SGLT2 inhibition reduced the risk of the primary kidney outcome by 27% with no evidence that the magnitude of the effect estimate varied by baseline GFR. Moreover, in the DAPA-CKD trial, dapagliflozin reduced the relative risks of heart failure or cardiovascular death or all-cause mortality, both secondary outcomes, in participants with and without CKD stage 4.1 In the EMPA-KIDNEY trial, 245 participants had an eGFR below 20 ml/min per 1.73 m2 at randomization. Although this was a small subgroup, the 27% relative risk reduction in the incidence of the primary kidney outcome was consistent with the effect size in the overall population. These exploratory subgroup data support the hypothesis that SGLT2 inhibitors may exert beneficial effects in patients at advanced stages of CKD. However, the evidence in this specific subgroup is weak, and the benefits observed in exploratory analyses in small subgroups could be the result of chance.,Second, clinical experience with SGLT2 inhibitors in dialysis patients is also limited because of perceived lack of efficacy. However, emerging data from the DAPA-CKD trial, where patients continued study medication when they started dialysis, provide new insights. Safety assessment among participants continuing dapagliflozin or placebo during dialysis did not reveal any between-group differences in adverse events. Moreover, numerically fewer deaths were reported in the dapagliflozin compared with the placebo group.1 These data should, however, be cautiously interpreted because the dapagliflozin and placebo groups were not randomized at dialysis initiation and few participants initiated dialysis (N=68 and N=99, respectively), reducing the reliability of these results.,How could it be that the efficacy of SGLT2 inhibitors persists in patients with severely impaired kidney function or on dialysis? Experimental data have suggested direct effects on the kidney as well as the heart that are not mediated via SGLT2 inhibition as presented in Figure 1. For instance, in an experimental study, uremic serum from dialysis patients impaired endothelium-mediated enhancement of relaxation and contraction of cardiomyocytes. Incubation with empagliflozin reversed this process, suggesting direct effects on endothelial dysfunction in coronary microvascular beds.5 In another experimental study, it was shown that isolated endothelial cells of human coronary arteries produce on stimulation with the stressor TNFα more of vasoconstrictor reactive oxygen species and less of the vasodilator nitric oxide. These effects could be blocked by cotreating these isolated cells with the SGLT2 inhibitors empagliflozine and dapagliflozine.6 How exactly SGLT2 inhibitors exert these beneficial effects in the absence of a functioning proximal tubule requires further study. These drugs can also bind directly to sodium–hydrogen exchange 1 transporters in cardiac tissues to promote a cardioprotective effect.7 Moreover, Lee et al. showed that in the case of ischemia, SGLT2 is expressed in the myocardium and that inhibition of this cardiac SGLT2 leads to a smaller infarct size. Finally, binding of SGLT2 inhibitors to SGLT2 expressed in pancreatic α-cells may promote the release of glucagon and reduce insulin, which in turn increases ketone body levels. Increased ketone bodies exert a wide array of beneficial effects in cardiac tissues.8 This notion of SGLT2 transporter–independent cardiac benefits is further supported by a bioinformatic study that used a combined approach of in silico modeling of publicly available RNA sequence datasets coupled with RNA sequencing of cardiac tissues from an experimental model of diabetic rats with heart failure treated with empagliflozin.9 The results also suggested that the inhibition of sodium–hydrogen exchange 1 may explain the heart failure protective effects of SGLT2 inhibitors.9 Such direct cardiac and kidney effects that are independent of proximal tubular SGLT2 could play a role in patients with severe CKD and minimal diuresis, but confirmation in a clinical study is required. Thus, at present, there is no definitive evidence to support a positive benefit-to-risk ratio in patients with severe CKD.,The third group of patients who could benefit from SGLT2 inhibitors but were excluded from recent large-scale outcome trials are patients living with a kidney transplant. Besides the fact that many of these patients have (severely) decreased kidney function, there is another issue that deserves attention in this specific group. In general, SGLT2 inhibitors are well tolerated and safe. Main side effects are, however, an increased incidence of urinary tract and genital infections. Patients living with a kidney transplant have a higher chance of (severe) urinary tract infections, which may lead to kidney function loss. The benefit-to-risk ratio in kidney transplant recipients deserves therefore special attention. Another specific clinical dilemma in patients with kidney transplantation is post-transplant diabetes mellitus, which occurs in 10%–20% of kidney transplant recipients due to prednisolone and other immunosuppressive therapy. SGLT2 inhibitors may be salutary not only because of their kidney and cardiac benefits but also because they reduce the risk of new-onset diabetes.1 A number of studies have explored the effects of SGLT2 inhibition in kidney transplant recipients. Most of these studies were case reports or small retrospective case series, limiting the quality and reliability of the data. A small randomized controlled clinical trial in 44 kidney transplant recipients showed that 24-week treatment with empagliflozin reduced HbA1c and body weight compared with placebo. During the relatively short follow-up period, empagliflozin was well tolerated with no imbalance in adverse events or immunosuppressive drug levels, including calcineurin inhibitors.10 Kidney function over time or the incidence of clinical outcomes was not assessed. A more recent observational clinical practice study aimed to address this gap. In this study, 226 kidney transplant recipients with type 2 diabetes who received SGLT2 inhibitors >90 days were pair-matched with kidney transplant recipients with type 2 diabetes not using SGLT2 inhibitors. During a mean follow-up of 5.2 years, SGLT2 inhibition was associated with a reduction in the risk of all-cause mortality, death-censored graft survival, or doubling of serum creatinine.11 SGLT2 inhibitors were well tolerated with no difference in the incidence of bacterial or fungal urinary tract infections. Diabetic ketoacidosis did not occur during follow-up in both groups. Although these findings are reassuring, the retrospective character of this study, without randomization and without blinded end point collection, precludes a definitive assessment of benefits and risks.,In conclusion, although there are some preliminary promising data, it is at present unclear whether SGLT2 inhibition will be effective in preventing clinically relevant outcomes and be sufficiently safe in patients with severe CKD that are not included in the present label. The RENAL LIFECYCLE trial (NCT05374291) was recently started to investigate the potential benefit of SGLT2 inhibitors in these patients. The trial aims to address the efficacy and safety of dapagliflozin 10 mg versus placebo in at least 1500 patients with (1) an eGFR <25 ml/min per 1.73 m2; (2) dialysis patients with residual diuresis >500 ml/24-hour; or (3) kidney transplant recipients with an eGFR ≤45 ml/min per 1.73 m2. These patients are recruited at clinical practice sites in Europe and Australia. The primary study end point is a composite end point of kidney failure, heart failure hospitalization, or all-cause mortality. The results are awaited in 4 years. In the absence of reliable data from randomized controlled clinical trials, we recommend being restrictive with the use of SGLT2 inhibitors in patients with severe CKD, on dialysis, or living with a kidney transplant."
2,2,55,37081637,https://www.doi.org/10.2215/CJN.0000000000000183,https://journals.lww.com/,"CKD is a major public health concern worldwide, especially in low- and middle-income countries (LMICs) with limited access to health care and preventive measures. The Global Burden of Disease study estimates that Latin America and the Caribbean have the greatest burden of CKD, but nationally representative data are lacking in certain countries.1 This prompted Roberts et al. to conduct a population-based study, published in this issue of CJASN, in an urban setting in Haiti that aimed to determine the epidemiology of CKD and its prevalence by CKD stage, identify associated local risk factors, and evaluate the proportion of individuals receiving guideline-recommended treatment in a very difficult socioeconomic climate.2,In Haiti, the first public dialysis unit was established only in 2001; therefore, many did not survive their kidney failure because they did not have access to low-cost dialysis before that.3 Furthermore, Haiti has the lowest number of dialysis centers per million population; most patients with kidney failure are treated in private clinics.4 There is limited availability of peritoneal dialysis and transplantation as treatment options for kidney failure in the country. Fortunately, since establishing the first organ transplantation program in Haiti in 2008, efforts have been underway to expand these much-needed options.5,Roberts et al. included 2424 adults from the Haiti Cardiovascular Disease Cohort Study in Port-au-Prince, Haiti. Participants underwent a clinical examination; risk factor assessments; and laboratory measurements for serum creatinine, urinary albumin, and urinary creatinine. Most participants were female (57%), and the mean age was 42 years. Importantly, most of the participants (69%) reported an income of less than one US dollar per day. The age-standardized prevalence of CKD was 14%. On the basis of the Kidney Disease Improving Global Outcomes (KDIGO) risk categories, 87% of patients were at low risk, 12% at moderate risk, 1% at high risk, and 0.4% at very high risk. Older age (≥60 years), diabetes, and hypertension were found to be independent risk factors of CKD. In this analysis, only 12% of individuals with CKD and albuminuria received guideline-recommended angiotensin-converting enzyme inhibitors (ACEis) or angiotensin receptor blockers (ARBs).2,In an earlier prospective cross-sectional study conducted by Burkhalter et al. on 608 adults at the outpatient clinic of Hôpital Albert Schweitzer, in rural Haiti, most participants were female (64.5%) with a mean age of 54.2 years. However, this study found higher rates of CKD, with 27% of participants having the diagnosis. Similar to those in the study by Roberts et al., most participants had lower and moderate CKD risk (stages 1–2 in 15.3% and stage 3 in 10.4%). Hypertension, HIV infection, and age older than 60 years were independently associated with CKD, but, surprisingly, diabetes mellitus was not. The study had a smaller population, was conducted in a rural hospital setting, and had a higher prevalence of hypertension (49.2%) and diabetes (36.3%), which may explain some differences observed.6,Roberts et al. present a valuable comparison between the Haitian participants of their study and non-Hispanic Black American participants from the National Health and Nutrition Examination Survey (NHANES) study conducted between 2015 and 2018. In this process, they aimed to assess the effect of income level and access to care on CKD prevalence in populations of African descent. According to the NHANES study from 2013 to 2016, the prevalence of CKD in Black US adults was higher (17.8%) than that of the Haitian urban participants. This difference was also observed among adults older than 65 years, where CKD was present in 34% of US adults compared with 27% of Haitian adults older than 60 years. The authors noted that advanced stages of CKD or kidney failure were more prevalent in the NHANES non-Hispanic Black population than in the Haitian cohort. Specifically, eGFR <30 ml/min was seen in 0.9% and 0.3% of the NHANES and Haitian populations, respectively, while kidney failure was seen in 0.6% and 0.04% of the populations, respectively.2 The authors suggest that the observed differences in CKD prevalence may be because of higher mortality rates in the Haitian population, which can be attributed to limited access to care and lower life expectancy. Indeed, life expectancy at birth in Haiti was 64 years in 2020.7 In addition, differences in ethnic backgrounds may also contribute to the observed differences because Black American participants are more likely to have European admixture. Furthermore, because Black American participants in the NHANES cohort likely have greater access to health care compared with the Haitian cohort, they will have a higher CKD detection rate. Further valuable research could include a comparison of the Haitian population with other low-income countries in Latin America and the Caribbean with populations of African descent. Alternatively, comparing the Haitian population with Hispanic Black participants from the NHANES study could provide insight into the role of factors such as diet or access to care.,In addition to hypertension and diabetes, CKD prevalence in Haiti is likely influenced by several yet undetermined social determinants of health, including environmental factors. Aflatoxins, found in Haitian staple foods, such as peanut butter and maize, have been reported in high concentrations as have heavy metals, such as lead and chromium, in groundwater in Port-au-Prince.8 In addition, drug safety is crucial, as demonstrated in 1996 when over 100 children died from acute kidney failure caused by diethylene glycol–contaminated cough syrup.9 Unfortunately, Haiti like many LMICs does not have an organization dedicated to protecting public health with control of medical products (drugs, devices). In addition, it is unclear at this time whether the Haitian population is affected by CKD of unknown etiology as has been described in several agricultural communities and in lower socioeconomic regions of the world.10,The high prevalence of hypertension and diabetes as risk factors of CKD certainly emphasizes the importance of implementing preventative measures and optimizing management of these conditions in the Haitian population. Furthermore, the low proportion of individuals receiving guideline-recommended treatment highlights the need for improved access to health care and education on screening for and management of CKD.,The utilization of guideline-directed medical therapy for CKD, specifically ACEis/ARBs, was around 22% on the basis of United States Renal Data System data from 2019 to 2020, which is a higher percentage compared with Haiti's urban cohort at 12%. Factors contributing to this disparity include a significantly lower number of primary care providers/nephrologists per million people, possibly limited knowledge of CKD guidelines among local physicians, medication costs, and availability. The Prospective Urban Rural Epidemiology (PURE) Study indicated that 60% of low-income individuals could not afford β-blockers or ACEis, whereas only 25% and 3% of urban and rural communities in low-income countries had access to these medications, respectively.11,Like most observational studies, the study by Roberts et al. has limitations, as acknowledged by the authors. The cross-sectional data only include a single eGFR and urine albumin-to-creatinine ratio, which are insufficient to confirm a CKD diagnosis according to KDIGO guidelines. However, comparatively, other national registries, such as NHANES, do not have longitudinal data. The study used the 2021 Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) creatinine-based equation to estimate GFR and may still result in inaccurate CKD burden estimates, particularly using the Jaffe method to measure serum creatinine. The most accurate equation to date, the combined creatinine-cystatin CKD-EPI (2021) equation, could not be implemented because of the additional costs of obtaining cystatin C levels. Finally, owing to a shortage of reagents, 581 participants (19%) were excluded from the study; they were different compared with the study cohort as they were more likely to be younger, female, and poorer and less likely to have hypertension. Although the authors acknowledge this bias, excluding healthier participants instead of having randomly excluded participants may have led to a less accurate estimation of the true CKD prevalence in this cohort.,The authors should be commended for their valuable work in Haiti in such a very difficult socioeconomic environment. Certainly, more population-based studies in LMICs are needed to help with a better recognition of risk factors, such as race, effect of urban versus rural areas, diet, and environmental exposures. It will also be essential to collect information on the availability, affordability, and coverage of guideline-directed medical therapy, particularly ACEis/ARBs, similar to what was done in the PURE study.11,The study by Roberts et al. contributes significantly to our understanding of the epidemiology of CKD in Haiti and underscores the urgent need for increased efforts to prevent, identify, and manage this condition in LMICs. Findings of this study should serve as a call to action for the global community to prioritize CKD as a public health concern and work toward reducing its burden worldwide, especially in LMICs."
3,3,77,36428134,https://www.doi.org/10.1053/j.ajkd.2022.09.007,https://linkinghub.elsevier.com/,"Related Article, p. 261,Related Article, p. 261,The growing public health burden of nondialysis chronic kidney disease (CKD) as the population ages is mirrored by concomitant cerebrovascular and neurodegenerative aging changes on brain magnetic resonance imaging (MRI) and associated cognitive impairment in CKD. Cerebrovascular changes in CKD are demonstrated by microvascular white matter changes; decreased white matter integrity, reflecting microvascular disease in the kidney; and micro- and macroinfarcts. Neurodegenerative changes are represented by decreased cortical gray matter volume, or atrophy of the cerebral lobes (especially temporal and frontal in Alzheimer disease [AD] and related dementias), and enlarged ventricular volume. Multiple investigations of the relation between kidney function in nondialysis CKD, assessed using estimated glomerular filtration rate (eGFR) and urinary albumin-creatinine ratio (UACR), and cognitive impairment have demonstrated a consistent graded association between decreasing kidney function and cognitive decline.1 This relationship is especially strong below an eGFR of 45 mL/min/1.73 m2; also, moderate-to-severe cognitive impairment is more prevalent below an eGFR of 30 mL/min/1.73 m2 and, independently, with an elevated UACR (>30 mg/g).1, 2, 3 Recent brain imaging studies in patients with CKD have identified similar associations between these kidney biomarkers and the structural brain changes underlying cognitive impairment in CKD.4, 5, 6 Overall, while cerebrovascular and neurodegenerative brain changes on MRI are both observed in patients with CKD, a larger cerebrovascular component has been reported.6 Thus, CKD is often considered a model of accelerated vascular brain aging.7,In this issue of AJKD, Scheppach et al8 have made an important contribution toward further elucidation of the underlying brain changes that occur in patients with CKD in reporting the results of a large brain MRI study. The authors conducted a cross-sectional examination of the relationship between kidney function and brain MRI abnormalities in a subcohort of 1,527 participants from the Atherosclerosis Risk in Communities Neurocognitive Study (ARIC-NCS) with MRI scans. The mean age of the study cohort was 76 years, 58% were women, and 27% were African American. Less than half of the cohort was classified as having CKD, as the cohort’s median eGFR was 60.8 (IQR, 47.6-73.7) mL/min/1.73 m2 and median UACR was 10.5 (IQR, 6.4-22.6) mg/g. About 34% of the cohort had mild cognitive impairment and only 5.0% had dementia at baseline. Exposure measures were eGFR and log(UACR). Cortical volume reduction, infarcts, micro-hemorrhages, and white matter hyperintensities and white matter integrity (using diffusor tensor imaging measures) were used as brain MRI outcomes in multivariable linear and logistic regression models. They also evaluated whether the associations varied when different biomarkers (cystatin C, creatinine, combination of creatinine and cystatin C or β2-microglobulin) were used to estimate GFR.,The main finding of this study was that lower eGFR and higher UACR are associated with lower brain volume, or atrophy. However, the degree of atrophy seen in regions susceptible to neurodegenerative pathologies, including AD and limbic-predominant age-related TDP-43 encephalopathy (LATE, a disease characterized by onset at ages 80+, similar memory and word finding symptoms as in AD, and TDP-43 protein accumulations in amygdala and similar regions to AD), were similar to the degree of atrophy seen in the rest of the cortex. These findings suggest that some of the brain atrophy observed is not likely primarily due to neurodegenerative etiologies like AD and LATE but due to a combination of etiologies including cerebrovascular disease. This is not surprising, as the findings of any MRI study reflect the composition of the study population. In this case, slightly less than half of participants had CKD (most with mild CKD), corresponding to a relatively lower expected contribution of cerebrovascular disease pathologies. The relatively low (5%) prevalence of dementia for a cohort with a mean age of 76 years (compared to expected approximately 10% in non-CKD populations) also suggests a lower expected contribution of both advanced cerebrovascular disease neurodegeneration.9 Additionally, the authors confirmed previous reports that both lower eGFR and higher UACR were associated with greater burden of microvascular disease, reflected by greater white matter hyperintensities and impaired white matter microstructural integrity on diffusion MRI. Though UACR and eGFR are highly correlated, only higher levels of UACR (a measure of vascular endothelial inflammation) had increased odds of cortical and lacunar brain infarcts and brain micro-hemorrhages, also confirming some prior studies. Importantly, the authors also found no interaction between race and the observed associations, and found that effect sizes were generally similar in direction and magnitude in comparing the eGFRs calculated with each of the 4 biomarkers described above.,Even after accounting for common cardiovascular risk factors, Scheppach et al found that there was global brain atrophy—not specific to regions typically affected by AD or LATE—and that kidney disease was associated with greater amounts of macro- and microcerebrovascular disease (greater white matter disease, brain infarcts, and microhemorrhages).8 The findings regarding cortical atrophy build on our nascent understanding gained from previous studies of the complex mechanisms underlying the widespread cortical thinning (another measure of atrophy) seen in CKD, independent of vascular dysfunction. For example, in one study, the lowest quartile of eGFR (31-60 mL/min/1.73 m2), compared to the highest (>83 mL/min/1.73 m2), was associated with cortical thinning in patients with AD, independent of cerebrovascular pathologies.10 In another study, the risk of cortical thinning was higher in those with cognitive impairment and lower in those who did not carry the APOE4 allele (a gene strongly associated with increased risk of AD).11,The observation that the associations between kidney measures and structural brain MRI changes did not vary by race is an especially important contribution of this study, as brain MRI studies inclusive of substantial proportions of African American participants are lacking, despite the higher incidence of CKD12 and dementia in African American populations.13 Furthermore, the systematic comparison of different biomarkers used for eGFR calculation increases generalizability of the results and confirms that the observed associations are not substantially affected by how eGFR is calculated. In addition, the greater information provided by UACR in comparison to eGFR in associations with cerebrovascular brain changes reflects that higher UACR may be better at capturing endothelial damage and, thus, microvascular damage, confirming a prior study.5 Other strengths of this study are the large, well-characterized population including classification of cognitive impairment severity, as well as robust 3.0 T MRI methodology and statistical modeling.,There is widespread consensus that kidney disease and brain health are highly interlinked, and that these associations underlie the increased prevalence of cognitive impairment seen in CKD, driven primarily—but not solely—by cerebrovascular pathologies. However, 3 avenues of “next steps” research would help disentangle the multiple interrelated pathways that lead to cognitive impairment and associated cortical atrophy and cerebrovascular brain changes in CKD. First, large longitudinal MRI studies are needed to track the progression of these brain changes and shed light on the causal relationships between kidney and brain function. These studies must include diverse older populations, in which at least 50% have moderate-to-severe CKD and 10%-15% moderate-to-severe cognitive impairment, to enable better characterization of structural brain changes association with cognitive impairment in CKD populations. Second, longitudinal positron emission tomography (PET) imaging for amyloid and tau deposition in patients with CKD and cognitive impairment could be harnessed to more accurately capture the trajectory of regional changes due to AD and other neurodegenerative pathologies as CKD progresses; to our knowledge, only amyloid PET has been used thus far.14 Third, imaging studies that incorporate traditional and novel mechanistic biomarkers as exposures could advance our understanding of the pathways underlying the cortical thinning seen in CKD. These include biomarkers of both kidney metabolic mechanisms (inflammation and oxidative stress, anemia, altered bone-vascular axis) and of kidney tubular injury,15 and the newer blood-based biomarkers of AD and related pathologies. In that arena, CKD was associated with higher levels of the blood-based biomarker p-tau181 and 217 in a large cohort study of patients with AD and other neurodegenerative pathologies (as defined on PET scans), compared to those without CKD.16 Although these elevated levels may be due to their decreased clearance, similar studies with sequential biomarker levels could help clarify this association. Overall, large longitudinal studies incorporating improved methods are required to allow us to account for the complex contributions of kidney disease to the risk of dementia more precisely."
4,4,101,36759239,https://www.doi.org/10.1053/j.ajkd.2022.11.002,https://linkinghub.elsevier.com/,"Related Article, p. 507,Related Article, p. 507,The year 2022 provided a vivid illustration of the impacts of changing climate, with devastating floods in Pakistan and Australia, temperatures reaching 40 °C (104 °F) in the United Kingdom and over 50 °C (122 °F) in parts of India, and punishing drought and wildfires in Europe and North America. As overall temperatures rise and the frequency and intensity of extreme heat events surge, so too will morbidity and mortality from climate-sensitive kidney diseases. Facing an unprecedented challenge, we as nephrologists must both change the way we provide care to kidney patients and champion climate action.,In this issue, Qu et al1 demonstrate a clear association between extreme heat (defined as days when the maximum temperature exceeded the 90th centile for that month) and emergency department (ED) visits for kidney diseases. Their time-stratified case-crossover study analyzed 1,114,322 ED presentations having a primary kidney disease–related diagnosis in New York state with county-level temperature data in the warmer months of May to September, between 2005 and 2013. The elevated risk of kidney disease–related ED presentations peaked at 3.1% above baseline 2 days after an extreme heat day and a higher risk persisted up to a week. Interestingly, the authors found a greater risk during the transitional months of May and September (excess risk on day 2 of 5.1% in May vs 2.7% in summer), hinting perhaps at an effect of seasonal changes in behavior (eg, clothing, thermostat settings, activity levels) that amplify the effect of extreme heat occurring when temperatures are expected to be milder. Their data also pointed to a “heat wave” effect in the form of greater risk of ED visits with greater numbers of extreme heat days in the preceding week.,Consistent with previous reports, the excess risk was concentrated in 3 kidney disease–related presentations: acute kidney injury (AKI), nephrolithiasis, and urinary tract infections (UTI). The authors also identified a higher risk of presentation among male individuals and an age differential in that those aged over 65 were observed to have a higher excess rate of ED presentations in the first 2 days of exposure, while the bulk of the demographic captured (ages 5 to 65 years) showed a more sustained excess rate of presentation across the week.,This study’s strengths include the use of the well-established case-crossover methodology, which compares index presentations with reference days in the same month and asks, essentially, was heat a factor in that individual presenting on that day as opposed to 1 to 3 weeks before or after. By restricting analysis to within-individual comparisons, the method minimizes bias by neatly sidestepping the need to account for differences between patients. The authors were also able to control for potential confounding effects of 2 specific air pollutants known to impact the association of interest and they also adjusted for humidity, but did not explore whether the combination of temperature and humidity (so-called wet-bulb temperature) influenced presentations. Common to previous studies is the limitation owing to lack of data on individual heat exposure, fluid intake, and cooling behaviors. Overall, their findings echo multiple previous studies of hospital admissions and higher temperatures, such that the evidence that increasing temperature associates with an increase in acute kidney presentations (especially AKI, nephrolithiasis, and UTI) now seems incontrovertible. A 2021 meta-analysis that included 82 studies found 1% greater kidney morbidity (largely acute hospital presentations) with each 1 °C rise in temperature, a finding consistent across studies using a variety of methodologies and in both middle- and high-income countries.2,While the acute burden of heat-related kidney presentations is becoming increasingly clear, with projections of up to 2.2 million additional cases of nephrolithiasis in the United States alone by 2050,3 the potential impact on long-term kidney outcomes remains uncertain. The bidirectional relationship between AKI and chronic kidney disease (CKD) is well established,4,5 with a rise in incidence of AKI therefore potentially able to increase the incidence and progression of CKD. Furthermore, heat stress is believed to be a primary contributor to the epidemic of tubulointerstitial CKD in agricultural workers in tropical areas of Central America and South Asia, known as chronic kidney disease of uncertain etiology (CKDu).6 Proposed mechanisms include activation of vasopressin and the aldose reductase–fructokinase pathway in response to hyperosmolarity.6 Predictors of serum creatinine rise during work shifts include ambient temperature as well as elevated baseline blood pressure,7 which may unfortunately illustrate in microcosm the broader risk of rising global temperatures colliding with the rising burden of noncommunicable disease (especially hypertension and diabetes) in low- and middle-income countries.8 Yet transient rises in serum creatinine, especially when mild and rapidly reversible, do not necessarily translate into adverse long-term kidney outcomes.9 Ultimately, further studies are needed to describe the emerging impact of rising temperatures and acute kidney disease presentations on the burden of CKD and kidney failure.,Beyond describing the challenge, the question then becomes what to do to mitigate the impact of higher temperatures on kidney health. Education of patients seems essential, with a message of rest, shade, and hydration (drawn from studies of interventions in agricultural workers)10 being the obvious place to start. Those with CKD, or a history of nephrolithiasis or UTI, could be prioritized for such advice, although future studies should attempt to better characterize individual risk factors for heat-related kidney events. These results also imply a need to strengthen health care system resilience through planning for an expected rise in kidney presentations during heatwaves, as well as ensuring that vulnerable populations (such as those in residential care, in poor housing, or required to work outdoors) have access to adequate shelter during episodes of extreme heat. More broadly, rising temperatures add to the need to improve access to quality primary and secondary care to reduce the incidence and progression of CKD and its drivers, such as hypertension, diabetes, and vascular disease.,Fundamentally, urgent action is required to drastically reduce carbon emissions in order to limit global warming to 2 °C (3.6 °F). The health care sector is responsible for 1% to 5% of carbon emissions globally and, in the United States, for over 7.9%.11 Nephrology, especially dialysis care, has a disproportionally high environmental footprint, meaning nephrologists have an important role to play in advocating for and implementing a switch to more sustainable, low-carbon health care. Guides to implementing “green nephrology” are now available,12 and kidney units should consider where they can modify their practices to reduce the environmental impact of their care. The problem can, however, seem remote to individual practitioners, with the bulk of health care sector emissions stemming from supply chain and manufacturing,13 but it is worth noting that sustainable health care is also high-value health care. With up to 30% of all health care spending being low value,14 measures as simple as rationalization of pathology testing can meaningfully reduce both cost and carbon emissions.15 As an immediate step, there is ample scope for improving patient, financial, and environmental outcomes through evidence-based use of tests and treatments.,In summary, the impact of rising global temperatures on kidney health is clear. What is now required is an effort to better understand the long-term implications of these acute events and strategies to minimize the risk to vulnerable patients. That health care professionals, especially nephrologists, have a role in aiding and advocating for the transition to a low-carbon future is also clear. Change is underway but it cannot come fast enough."
5,5,106,37589626,https://www.doi.org/10.1053/j.ajkd.2023.07.004,https://linkinghub.elsevier.com/,"Related Article, p. 386,Related Article, p. 386,The rate of disease progression varies highly among individuals with chronic kidney disease (CKD). To better elucidate individual risks for progression to kidney failure and improve personalized medicine, patients and physicians have access to a multitude of clinical prediction models,1 which combine multiple prognostic factors into a single individual risk for kidney failure. One of the most used kidney failure prediction models is the 4-variable Kidney Failure Risk Equation (KFRE), developed by Tangri et al,2 which combines age, sex, urinary albumin-creatinine ratio, and estimated glomerular filtration rate (eGFR) to calculate the 2-year and 5-year risks of kidney failure for an individual patient. The KFRE has been validated in and works well across many populations.3 However, both patients and physicians may struggle in understanding and communicating the estimated risks.4 A predicted risk between 0 and 100% is always an incorrect projection of the future on an individual level, as the individual either experiences the outcome or not. Comprehending the meaning of a 20% risk of kidney failure within 2 years for an individual patient is tricky and not intuitive; the intrinsic uncertainty is difficult to communicate and it is easier to grasp the concept of when a patient is expected to progress. For a physician, estimating time until kidney failure is an essential part of daily practice, as important clinical interventions (eg, nephrologist referral, preparing for transplantation, and planning vascular access) must be planned on time. Because these key interventions in a patient’s CKD trajectory are primarily time-to-event dependent, not risk-of-event dependent, it seems long overdue to translate these timeframe-dependent probabilities (eg, 20% risk within 2 years) into an expected time to event (eg, expected to develop kidney failure within 12-15 months).,In this issue of AJKD, Chu et al5 make an important step in this direction (Fig 1). They converted risk estimates predicted by the 2-year KFRE to the median time until kidney failure in a cohort of 1,641 United States CKD patients and modeled the relationship between 2-year risks and time until kidney failure in months using accelerated failure time models.5 They focused on the 20%, 40%, and 50% 2-year risk thresholds owing to the relevance of these cut-offs for clinical decision making and the incorporation of these risks in various guidelines regarding kidney replacement therapy planning. Additionally, they converted different eGFR estimates to the median time until kidney failure, with a specific focus on 20, 15, and 10 mL/min/1.73 m2. Their main results are the median time until kidney failure based on the KFRE risk or eGFR. The authors also performed subgroup analyses to determine homogeneity of estimated times across various patient characteristics. They observed that the estimated time until kidney failure based on the KFRE compared to eGFR was both more consistent and less uncertain early in the disease trajectory. In advanced CKD there was little difference between the time estimations based on the KFRE or eGFR alone. This shows that the KFRE is especially useful in earlier stages of CKD, when time to kidney failure remains difficult to estimate based on eGFR alone. A noteworthy caveat, also mentioned by the authors, is the large competing risk of death when predicting kidney failure; many patients die of other causes before ever reaching kidney failure. Because this was not taken into account in the development of the KFRE or in the modeling strategies of Chu et al, predicted estimates may be overestimated. Therefore, these results should be interpreted as the expected time until kidney failure, conditional on the premise that the patient will not die. Lastly, the authors also externally validated the KFRE—a valuable analysis to undertake when using prediction models, as the performance is highly dependent on characteristics of the validation population.6 During the validation, they did account for the competing risk of death, allowing us to assess the model performance in a more realistic clinical setting in which patients may die before experiencing kidney failure.7 The model was able to discriminate well in their population and was reasonably calibrated.,By being able to determine the patient’s expected time until kidney failure, patients can be better informed and key interventions in a patient’s disease trajectory may be more precisely tailored to the individual. However, when treatment interventions are based on predicted risks, the use of the KFRE becomes an intervention in itself. Given the widespread use of the KFRE, going forward impact trials for various KFRE-based interventions (such as vascular access planning or risk communication) are necessary to evaluate whether using the KFRE actually benefits patient care. Such an impact trial was performed for the 5-year KFRE, where the primary outcome was performing recommended laboratory tests in the next 6 months and the secondary outcome was nephrologist referral.8 This trial found no advantage of the KFRE for these outcomes. Although there were some significant limitations in study design, this study exemplifies that model performance is not always related to model impact. After all, it may well be that physicians already take risk estimates into account sufficiently without the KFRE, or do not trust the KFRE enough to influence their decision making. Nevertheless, a single impact trial is not enough, as the KFRE might have a different impact for different applications. Outcomes of interest may also differ: one currently ongoing impact trial studies the effect of integration of KFRE into the health care system on markers of disease (albuminuria, eGFR), received care (patient management, medication usage), health care costs, and patients’ trust in physician care.9 These impact trials form a crucial part in solidifying the role of the KFRE into clinical practice.,The study by Chu et al. is a meaningful contribution to translating predicted risk into predicted time to event. Nonetheless, their estimated median time to event may be further refined. Even though the KFRE risks underlying the time-to-event estimates were validated extensively, the newly estimated times until kidney failure based on these KFRE risks need to be validated too. The current estimated median times, derived from a relatively small population from the United States, may not be readily applicable to other populations and the underlying assumptions of the modeling strategy may not hold in every population. Additionally, a model to directly estimate time to event instead of converting risk estimates might be more accurate and can include additional predictors to increase accuracy. These models should take competing risks into account. Going into more technical detail, adaptations of the Cox proportional hazard (PH) models for competing risks can be used (eg, Fine-Gray models). Although, in contrast to accelerated failure time models, Cox PH models and adaptations do not parametrically estimate the underlying hazard that prohibits direct prediction of individual survival times, indirectly, individual predicted time to event can be calculated by specifying the underlying distribution of the baseline hazard.10 Since the KFRE was originally developed as a Cox PH model, these times until kidney failure could even be derived directly from the KFRE itself. Because the KFRE has been shown to perform well across many populations, this might be the most pragmatic starting point for time-to-kidney-failure estimation, predominantly for early stages of CKD where the competing risk of death remains low.,Individualized predictions of time until kidney failure can be of great value to patient-centered health care, and it is time we rethink predicted probabilities. While we await further studies that provide validated predictions of time until kidney failure for different populations, that take competing risks into account, the study by Chu et al offers a solution to estimating an individual’s time until kidney failure based on their KFRE risk."
6,6,115,36528464,https://www.doi.org/10.1053/j.ajkd.2022.10.004,https://linkinghub.elsevier.com/,"Related Article, p. 281,Related Article, p. 281,Autosomal dominant polycystic kidney disease (ADPKD) is the most common genetic kidney disorder with a risk of significant morbidity, including kidney failure. The prognosis of the disease may differ between individuals, although loss of kidney function over time is inevitable. The quest for a treatment to slow down the loss of kidney function and progression to kidney failure has spanned decades. Three large phase 3 clinical trials—TEMPO 3:4 (Tolvaptan Efficacy and Safety in Management of Autosomal Dominant Polycystic Kidney Disease and Its Outcomes 3:4 Trial),1 TEMPO 4:4,2 and REPRISE (Replicating Evidence of Preserved Renal Function: an Investigation of Tolvaptan Safety and Efficacy in ADPKD)3—showed that tolvaptan decreased rate of kidney growth and kidney function loss. Its use for ADPKD was approved by the European Renal Association (ERA) in 2016 and US Food and Drug Administration (FDA) in 2018. However, the risk of tolvaptan-induced hepatic injury remains a major concern.,In this issue of AJKD, Alpers et al4 conducted a retrospective study of tolvaptan safety data from clinical trials to address this concern. They analyzed data from more than 2,900 tolvaptan-treated study participants, including more than 2,300 with ≥18 months of drug exposure. The intervention in all clinical trials was twice-daily dosing of tolvaptan. The main outcome was an elevation above the upper limit of normal (ULN) of ≥3× (in alanine aminotransferase [ALT] or aspartate aminotransferase [AST] level) or of ≥2× (in total bilirubin level). In TEMPO 3:4, there were 2 patients who experienced concurrent elevations in levels of ALT/AST and bilirubin. No one had persistent liver dysfunction. During the TEMPO 3:4 study period, 1.2% of the participants needed to discontinue the trial owing to liver injury. Later, Endo et al5 published a case report of a patient with tolvaptan-induced acute liver failure requiring liver transplantation, which proved that clinicians’ concerns were relevant. In the REPRISE trial, ALT elevations >3× ULN were more frequently seen in the tolvaptan arm compared to the placebo arm, but no participant in REPRISE had a concomitant increase in total bilirubin. The analysis by Alpers et al showed that liver enzyme abnormalities were more common in the first 18 months of tolvaptan use, and liver enzyme levels improved or returned to normal after stopping tolvaptan.,To date, our understanding of tolvaptan-associated drug-induced liver injury (DILI) is that this can occur in 4%-5% of patients and has an idiosyncratic time to onset. For the vast majority, these episodes of DILI will resolve without complication within 1-3 months; however, recurrence is common, and in general rechallenge should be done with caution. Tolvaptan presents a clear benefit to patients with ADPKD, and close monitoring will allow for a more informed decision of the patient-specific risks and benefits. Alpers et al provide data to inform several components of clinical decision-making: (1) the prevalence of DILI requiring discontinuation was 1.2%; (2) the majority of DILI cases occur before 18 months, although this may be limited by the duration of follow-up in this study; (3) few patients were rechallenged and among those who were, most had a “positive” rechallenge (meaning the increase in ALT was repeated), even when restarted at a lower dose of tolvaptan; (4) continuation despite elevated ALT was rare—too rare to make any meaningful observations—occurring in 1 person who demonstrated adaptation.6,7 Overall, the data presented by Alpers et al highlight that close monitoring (ie, monthly) of transaminases and bilirubin is paramount. Their data do support that monthly measurement of transaminase and bilirubin levels for 18 months and every 3 months thereafter is sufficient. This conclusion supports the FDA’s Risk Evaluation and Mitigation Strategy (REMS) requirements as well. In our practice, our highly informed patient population appreciates a conservative approach to temporary discontinuation of tolvaptan in the setting of abnormalities in liver function tests and possible DILI. We seek other explanations for such abnormalities and then rechallenge all willing patients at a lower dose of tolvaptan, in some cases at subtherapeutic levels (eg, 15 mg twice a day). If rechallenge is negative, we cautiously advance the dose back to standard dosing (45 mg upon waking plus 15 mg after 8 hours).,The authors went beyond these data to create a hepatic adjudication committee to analyze rechallenge assignments made in REPRISE and the long-term extension study—an attempt to define the clinical probability of the DILI reported being associated with tolvaptan. The 4 hepatologists on the committee assessed causality using “expert opinion” interpretation of comorbid conditions, concomitant medication use, onset, offset, and dose relationship. The causality groups were defined as “definite,” “highly likely” (75%-95%), “probable” (50%-74%), “possible” (25%-49%), and “unlikely” (<25%) according to the possibility of drug-induced injury as an underlying etiology. A total of 125 hepatic events met the criteria for adjudication in REPRISE and the long-term extension, but none of them were categorized into “highly likely” or “definite” groups per the retrospective analysis by Alpers et al. This appears reassuring on its surface; however, even with participant-level data, given the polypharmacy many patients with ADPKD experience and the idiosyncratic timing of tolvaptan-associated DILI, interpretation of this adjudication is difficult. For example, with statins being prescribed in at least 20% of ADPKD participants in clinical trials,8 as well as possible use of acetaminophen for pain and limited ability to quantify alcohol use, any post hoc interpretation of causality has limitations. Nevertheless, data exist to suggest safe use of tolvaptan with statins8 and in clinical practice, real-time assessment of competing explanations for liver function test abnormalities not meeting criteria for tolvaptan-associated DILI can be made.,In summary, tolvaptan is the only disease mechanism–specific FDA-approved drug for ADPKD patients at risk of rapid disease progression, slowing cyst growth and kidney function decline in patients with early and advanced chronic kidney disease. DILI has been consistently observed in tolvaptan-treated patients in clinical settings and clinical trials for drug safety. In this study by Alpers et al, the liver safety data from major tolvaptan trials in ADPKD patients were summarized. Monthly liver function test monitoring during the first 18 months of treatment and every 3 months thereafter is currently mandated by REMS and is appropriate and necessary to protect the safety of the rare patients who experience DILI from tolvaptan. Future studies examining different approaches to rechallenge patients after initial tolvaptan-induced liver injury will help to determine optimal management of those patients, while identification of risk factors for DILI susceptibility9 and the development of prediction models could improve the clinical approach.10 More genotype data from tolvaptan-exposed patients with pathogenic variants in PKD1 or PKD2,11 no variant detected, and variants of unknown significance could also help to gauge the risk-benefit ratio with respect to potential for clinical benefit and liver safety profile. Nevertheless, given the recent disappointments faced by the ADPKD community with tesevatinib, venglustat, and lixivaptan, we must not allow fear of tolvaptan-induced liver injury to prevent use of this important medication."
7,7,146,36775203,https://www.doi.org/10.1016/j.ajt.2023.02.013,https://linkinghub.elsevier.com/,
8,8,147,36153151,https://www.doi.org/10.1053/j.ajkd.2022.07.002,https://linkinghub.elsevier.com/,"Related Article, p. 15,Related Article, p. 15,Accurate measurement and classification of blood pressure (BP) are essential in managing hypertensive patients with chronic kidney disease (CKD). Guidelines and scientific statements recommend ambulatory BP monitoring (ABPM) or, when not available, home BP measurements (HBPM) to confirm the presence of hypertension and identify patients with white-coat or masked hypertension.1,2 Because BP follows a circadian pattern, ABPM can also be used to assess diurnal BP profile, with key parameters being nighttime (sleep) BP and nocturnal BP dipping pattern (sleep-awake BP cycle). The prognostic superiority of out-of-office BP measurements over standard office-based readings has been amply demonstrated in many observational studies.2 In patients with hypertension and CKD who are not receiving dialysis, ambulatory BP—whether it be mean 24-hour, daytime, or nighttime—is associated with hypertension-mediated target organ damage, cardiovascular (CV) events, progressive kidney disease (often leading to kidney failure), and mortality, and the adverse prognosis is robust even after adjustments for traditional risk factors including office BP.3 CKD patients with white-coat hypertension are not at substantially higher risk for a CV event than normotensive patients,3,4 while masked hypertension, a common finding in CKD cohorts, confers the same level of risk for adverse outcomes as sustained hypertension.4 Reduced nocturnal BP dipping (also referred to as nondipping) is highly prevalent in hypertensive patients with CKD, accounting for more than 60% of participants in most studies and almost 90% of those with advanced CKD.4, 5, 6 Loss of the normal diurnal variation in BP is associated with markers of kidney dysfunction, such as reduced glomerular filtration rate (GFR), albuminuria, and impaired sodium excretion, and predicts progressive deterioration in GFR (often defined as a 50% decrease) and kidney failure, even after adjusting for office BP.3,4 These relationships have been shown to be log-linear and continuous, with no threshold level where risk increases suddenly.2,To facilitate the clinical application of ABPM, threshold levels of ambulatory BP that are the risk equivalent to office BP levels were derived from prospective observational studies with CV end points2 and nondipping BP pattern was arbitrarily defined as a decrease in nocturnal BP relative to daytime BP of <10%.1 In the article by Borrelli et al7 published in the current issue of AJKD, the investigators explored the use of 2 closely related ABPM parameters, ambulatory BP (daytime and nighttime BP) and dipping status, expressed as categorical variables, to evaluate the prognosis of treated patients with hypertension and CKD who were not receiving kidney replacement therapy (dialysis or transplant). Few studies with CV and kidney outcomes have used both BP parameters in the same sample to determine their relative importance in predicting outcomes. Identifying the best BP index of risk and in what circumstances provides an opportunity to develop and test a more focused approach to treatment.,Borrelli et al categorized the 906 hypertensive patients with CKD (stages 2-5) in their study into 1 of 4 mutually exclusive groups: ambulatory BP above or at goal (defined as daytime and nighttime systolic BP <135 mm Hg and <120 mm Hg, respectively) and nondipping or dipping (defined as night-to-day ratio of systolic BP <0.9 based on a single 24-hour ABPM performed at baseline). Study participants were followed for a median of 7.8 years. Outcomes were a kidney progression outcome (defined as a composite of ≥50% decline in estimated GFR or initiation of maintenance dialysis) and all CV events. In total there were 315 kidney progression events and 220 CV events. In line with past studies4, 5, 6 the prevalence of nondipping was 70% and nondippers above ambulatory BP target were older, were predominantly men, had a history of diabetes or prior CV disease, and had lower estimated GFR, more proteinuria, and higher nighttime BP than dippers not at ambulatory BP goal. Not surprisingly, these patients were at highest risk for kidney disease progression and CV events. The novel findings of the study were the increased kidney as well as CV risk of nondippers at ambulatory BP goal compared to normotensive dippers, and that the increased level of risk was virtually identical to that of dippers with uncontrolled hypertension. The authors concluded that nocturnal nondipping BP pattern is an independent risk factor for adverse events among hypertensive patients with CKD and should be considered a target for therapeutic intervention to restore normal dipping status.,Despite the encouraging results, ascertaining dipping status as a treatment target is still controversial for several reasons. First, dipping status derived from a single 24-hour ABPM recording, as in the current study, is poorly reproducible.8 This has led guideline committees and others to recommend repeat testing or extending monitoring to 48 hours to confirm the presence of nondipping.1,2 Second, observation studies can only identify links; as such, they do not imply a reduction in adverse effects of nondipping with successful treatment.2,3 Moreover, successful treatment of nondipping, for example by bedtime administration of antihypertensive medications, is invariably accompanied by a decrease in nocturnal BP, making it difficult to tease out the relative contributions of these highly correlated variables to any observed benefits.9,10 Third, in a meta-analysis of prospective studies involving patients with hypertension, dipping status remained significantly associated with total mortality or composite CV end points even with adjustment for 24-hour BP, but its inclusion did not greatly improve the model fit beyond the 24-hour BP readings.11 In keeping with this finding, a recent randomized controlled trial of patients with advanced CKD and poorly controlled hypertension showed that the addition of chlorthalidone, a long-acting thiazide diuretic, to existing drug therapy significantly reduced ambulatory daytime and nighttime BP in parallel but did not substantially lower the high percentage of participants with nondipping BP pattern.6 Finally, nondipping expressed as a categorical variable is not a robust measure in assessing benefits. This was apparent in chronobiology studies in which bedtime dosing of antihypertensive medications greatly improved the dipping ratio (a continuous variable) and clinical outcomes but only re-established a normal nocturnal BP dipping pattern in about one-third of patients.9,10,There is a growing support for out-of-office BP measurements, not only for diagnosis, but, importantly, for monitoring antihypertensive treatment after its initiation. Measurement options include ABPM, HBPM, and, more recently, small wearable devices, although the latter is still hampered by issues around accuracy, reproducibility, and calibration.12 Guidelines recommend both ABPM and HBPM and view them as complementary procedures.1 HBPM allows patients to monitor their BP regularly over an extended period of time and is a powerful monitoring tool when coupled with a BP telemonitoring system that provides feedback support in real time from a multidisciplinary clinical team.1,13 It is particularly valuable in situations where there are frequent changes in antihypertensive drug treatment to improve BP control or in response to medication side effects. ABPM is less well suited for repeated monitoring because of inconvenience, cost (not a recoverable expense in many jurisdictions), and poorer patient acceptance.2,14 Indeed, position papers and scientific statements envisage a more limited role of ABPM in the long-term management of hypertension. Situations where ABPM seems to have advantages over HBPM include assessment for nocturnal hypertension in patients at risk such as those with more advanced CKD, diabetes, or obstructive sleep apnea; evaluation of patients with symptomatic hypotension, especially when associated with severe supine hypertension; and ascertaining therapeutic coverage over 24 hours.14,15 A major limitation of both ABPM and HBPM as the primary measurement tool in managing hypertension is the lack of clinical trials with hard outcomes using BP from these procedures to determine eligibility for treatment and goals of therapy. Current drug treatment parameters are founded on the results of multiple clinical trials that exclusively used office-based BP measurements. As such, they are widely accepted and will likely continue to inform clinical decision making until new evidence indicates otherwise.2,14,15"
9,9,156,37499685,https://www.doi.org/10.2215/CJN.0000000000000268,https://journals.lww.com/,"IgA nephropathy is the most common glomerular disease worldwide, with well-recognized geographic and racial/ethnic differences in prevalence and disease severity. The spectrum of disease progression is also broad, but IgA nephropathy generally follows a protracted course, with a median time to kidney failure reported to be >25 years.1 The slow rate of progression to recognized “hard end points” of halving of eGFR, kidney failure (dialysis or transplantation), or death poses challenges for drug development as such trials require lengthy follow-up to accumulate sufficient outcome events.,In 2016, the Kidney Health Initiative initiated a project to identify end points that could be used as the basis for drug approval for IgA nephropathy and specifically explored whether proteinuria reduction could be used as a surrogate end point. On the basis of US Food and Drug Administration's general considerations for surrogate end points and after careful review of key published clinical trials in IgA nephropathy available at the time, the Kidney Health Initiative workgroup concluded that proteinuria reduction could be considered a reasonably likely surrogate end point in IgA nephropathy.2 In the United States, a reasonably likely surrogate end point may be used for drug licensing on an accelerated pathway that requires postmarketing trials to confirm demonstrable benefit on hard end points before full approval.,In parallel, numerous advances improved understanding of the pathogenesis of IgA nephropathy, notably with respect to the intestinal mucosa–kidney axis.3 These important developments collectively led to an explosion of clinical research in IgA nephropathy. Over 25 clinical trials are ongoing worldwide, testing agents targeting inflammatory pathways, complement activation, immune system regulation, and mucosal immunity.4 In the past 2 years, the results from four large randomized, controlled clinical trials have been published: the Therapeutic Effects of Steroids in IgA Nephropathy Global (TESTING) study using methylprednisolone,5 the NefIgArd study using a gastrointestinal mucosa targeted-release formulation of budesonide (Nefecon),6 the Dapagliflozin and Prevention of Adverse Outcomes in Chronic Kidney Disease (DAPA-CKD) prespecified IgA nephropathy substudy using dapagliflozin,7 and the recent Study of the Effect and Safety of Sparsentan in the Treatment of Patients With IgA Nephropathy (PROTECT) using sparsentan8 (Table 1). We will briefly review these studies and provide a perspective on sparsentan.,The previous Supportive Versus Immunosuppressive Therapy for the Treatment Of Progressive IgA Nephropathy (STOP-IgAN) nephropathy study demonstrated that diligent nonimmunosuppressive therapy is efficacious in reducing proteinuria to <0.75g/d in one third of patients.9 Addition of immunosuppression with glucocorticoids alone or glucocorticoids and azathioprine was associated with improved likelihood of complete remission of proteinuria but not in reducing GFR decline. Against this backdrop, the four recent clinical trials evaluated efficacy and risks of two immunomodulatory and two nonimmunomodulatory approaches.,The TESTING study demonstrated that methylprednisolone significantly reduced the risk of a 40% reduction in eGFR, kidney failure, or death compared with supportive therapy alone. However, this study also provided a stark reminder of the serious infectious risks of high- or even moderate-dose glucocorticoids (7% versus 1%).5 NefIgArd demonstrated that targeted-release budesonide therapy significantly reduced proteinuria compared with placebo. The rationale for this budesonide formulation rests on the concept that intestinal mucosal immunity contributes to pathogenesis of IgA nephropathy. However, aside from overt inflammatory bowel disease, no reliable assessments currently exist to identify patients for whom this particular mechanism is important. The NefIgArd trial is still ongoing as a blinded observational phase with supportive care only (Part B) to evaluate the sustained effect of prior targeted-release budesonide treatment on an eGFR-based end point as is required for agents approved on the accelerated pathway.6 Without a head-to-head study, the relative benefit and risk profile of targeted-release budesonide compared with usual corticosteroids remains unanswered.,In the nonimmunomodulation camp, two relatively new agents have emerged: the sodium-glucose cotransporter-2 inhibitors (SGLT2i) dapagliflozin and sparsentan. The SGLT2i data are very compelling on the basis of a large population and on hard end points of the composite of 40% reduction of GFR, kidney failure, or death. SGLT2i are now well documented to provide significant benefit to patients with proteinuria and CKD, including patients with IgA nephropathy.7 Furthermore, the reduction in proteinuria is comparable with that of other medications discussed here, with a favorable safety profile.,Sparsentan is a dual endothelin receptor antagonist and angiotensin receptor blocker. This agent recently obtained US Food and Drug Administration accelerated pathway approval with required confirmatory postmarketing studies. The PROTECT study assessed the effect of sparsentan on proteinuria reduction in a randomized, double-blind, active-controlled study in 281 adults with biopsy-proven IgA nephropathy, eGFR ≥30 ml/min per 1.73 m2, and proteinuria ≥1.0 g/d receiving maximized renin-angiotensin-aldosterone system (RAAS) blockade.8 Patients were randomized to either sparsentan (400 mg once daily) or irbesartan (300 mg once daily). SGLT2i use was prohibited. The primary end point was the relative change from baseline in urine protein-to-creatinine ratio at week 36. The geometric mean of urine protein-to-creatinine ratio decreased from 1.2 g/g to 0.7 (−45% [−51% to −38%]) and to 1.0 (−15% [−24% to −4%]) in the sparsentan and irbesartan groups, respectively. In the reported phase of PROTECT, increases in aminotransferases to more than three times the upper limit of normal occurred in 2% of each group and were asymptomatic and reversible. Sparsentan is currently available through a Risk Evaluation and Mitigation Systems program because of observed hepatoxicity with some endothelin receptor antagonists. This program requires documenting serum aminotransferases and total bilirubin before treatment, monthly for 12 months, and then every 3 months thereafter in addition to pregnancy testing monthly during treatment. The double-blind treatment phase of PROTECT is ongoing to further evaluate the effect of sparsentan compared with irbesartan on the hard end point of eGFR, kidney failure, or death.,Multiple studies have identified the risk factors of progression of IgA nephropathy.10 Although predictive outcome models may identify those at risk, they do not help determine which patients may benefit from immunomodulation. IgA nephropathy is clearly not uniform across all patients with respect to mechanistic drivers of disease, risk of progression, and potential response to a specific therapy. Going forward, the development of clinical and/or biomarker-based models is needed to help predict which patients need immunotherapy in addition to maximized supportive therapy with RAAS or RAAS+endothelin blockade and/or SGLT2 inhibition. Identifying individuals for whom therapy targeting mucosal immunity is likely to be beneficial is similarly important as is identifying those who may need other systemic immunotherapy. The need for better predictive models of individualized therapy is highlighted by potential risks associated with the medications and their substantial costs. The current studies, which have evaluated therapies with very different mechanisms of action, raise important questions regarding what should constitute the comparator group in future trials in IgA nephropathy. In our opinion, the demonstrated benefits of nonimmunosuppressive therapy alone do not support corticosteroids as the standard against which all future investigational agents should be compared. Conversely, benefits of SGLT2i seem applicable to multiple underlying causes of kidney disease, are rapidly gaining use in clinical practice, and are increasingly allowed in currently ongoing clinical trials in IgA nephropathy. As such, their use may be de facto becoming part of “standard-of-care supportive therapy.” The benefit and safety of combined use of SGLT2i and dual endothelin–angiotensin blockade is currently under investigation (NCT05856760) and may become the basis of maximized supportive therapy in IgA nephropathy even when immunotherapy is likely warranted. An important consideration of the presented studies is the lack of histologic data, specifically the mesangial hypercellularity (M), endocapillary proliferation (E), segmental glomerulosclerosis or adhesions (S), tubular atrophy and interstitial fibrosis (T), and crescent (C) (MEST-C) scoring. Whether the benefit of a therapeutic intervention may differ depending on the degree of active inflammation versus scarring is therefore unknown, which may be particularly important for immunomodulatory therapies. Future studies may benefit from inclusion of MEST-C scoring when available within a reasonable period of time before trial enrollment. Relevantly, patients with crescentic IgA nephropathy or with exposure to immunotherapy were excluded from the current studies.,Importantly, as both the sparsentan and budesonide trials leveraged the accelerated approval pathway on the basis of a reasonably likely surrogate end point (proteinuria reduction), ongoing phases of these trials6,8 will test whether these agents are effective in reducing hard end points of kidney disease. In addition to direct assessment of efficacy for these agents themselves, final results of these trials will support or refute the hypothesis that a partial reduction in proteinuria can indeed be used as surrogate end point in IgA nephropathy."
10,10,164,37526983,https://www.doi.org/10.1681/ASN.0000000000000162,https://journals.lww.com/,"Evolving evidence supports that a shift from normal acid–base balance toward a more acid milieu is associated with increased risk of cardiovascular disease (CVD). Metabolic acidosis, the most extreme end of the continuum of acid stress,1 is associated with a further increased risk of major adverse cardiovascular events (MACEs)2 among patients with CKD whose MACE risk is already above those without CKD. At the lower end of the acid-stress continuum, high acid-producing diets are also associated with increased CVD risk.1 Furthermore, individuals eating acid-producing diets might maintain steady-state acid retention without reduced plasma total CO2, i.e., might have eubicarbonatemic acidosis.1 Whether these latter individuals with less acid stress than those with metabolic acidosis1 also have increased risk of MACEs remains unknown. Because such individuals potentially constitute a significant proportion of those with CKD and are easily treated with dietary alkali,1 determining whether they have increased risk of MACEs and whether their straightforward treatment reduces this risk carries important clinical implications.,These observational data raise questions that require interventional studies to establish whether there is a pathophysiologic connection between acid stress and adverse cardiovascular outcomes, and if so, how acid stress might be best managed to reduce or eliminate these associated adverse outcomes. Because adverse cardiovascular outcomes manifest over many years, such studies ideally begin with identifying surrogates of adverse cardiovascular outcomes that might indicate pathophysiologic connections between acid stress and these adverse outcomes. Kendrick and colleagues have chosen vascular reactivity and measures of arterial stiffness in human study participants as surrogates, thereby avoiding interpretive concerns that accompany animal models. The latest in their study series is in this issue of JASN.3,Kendrick et al. understandably began their series of interventional studies by examining study participants with the most extreme end of the continuum of acid stress, metabolic acidosis.4 They reported change in brachial artery flow-mediated dilation (FMD) in 18 participants with G3b-G4 and baseline plasma (HCO3−) 16–21 mEq/L treated for 6 weeks in an open-label, prospective, randomized, crossover fashion (2 weeks washout in between). Treated study participants received daily oral sodium bicarbonate (NaHCO3) in doses necessary to maintain plasma (HCO3−) ≥23 mEq/L. Treated participants increased plasma (HCO3−) by 2.7±2.9 mEq/L (19.3±2.3 mEq/L to 22.0±3.1 mEq/L, P < 0.001), but controls had no change (19.7±2.3 mEq/L to 19.6±3.2 mEq/L, P = 0.93). They reported that FMD increased in treated participants (4.1%±4.1% to 5.2%±2.9%; P = 0.04) at 6 weeks, but not in control (4.6%±3.1% to 4.1%±3.4%; P = 0.20). There were no significant adverse events in either group.,In their latest report, the authors studied participants with G3b-G4 eGFR and plasma (HCO3−) 22–27 mEq/L, i.e., without metabolic acidosis by plasma acid–base parameters. Individuals with this level of reduced eGFR who eat Western, acid-producing diets can have acid retention despite plasma (HCO3−) above the current criterion for metabolic acidosis,1i.e., they can have eubicarbonatemic acidosis. The authors examined change in FMD but also aortic pulse wave velocity (aPWV) and change in the left ventricular mass in 109 participants treated for 12 months in a randomized (1:1), double-blind, placebo-controlled trial with oral 0.5 mEq/kg bw per day oral NaHCO3. Plasma (HCO3−) increased significantly in the treatment, but not placebo group at 12 months (1.11±1.9 mEq/L versus −0.24±2.3 mEq/L; P = 0.003). Although FMD of treated but not placebo participants increased at 1 month (3.99%±4.8% versus 6.39%±7.3%, P = 0.003), this increase was not sustained at 6 and 12 months. Neither treated groups nor placebo groups showed aPWV improvement at either 6 or 12 months nor was there reduction in the left ventricular mass. Importantly, the authors supported the effectiveness of said NaHCO3 treatment to reduce acid stress by demonstrating that treated participants increased urine excretion of citrate and decreased urine ammonium excretion, consistent with reductions in acid retention1 and urine net acid excretion,1 respectively.,Like all important contributions to ongoing questions of pathophysiology and clinical medicine, this high-quality and well-done study by Kendrick and colleagues provides important insights and leads to additional questions that follow-up studies will address. Assuming that their earlier report4 showing that oral NaHCO3 therapy with a goal to increase plasma (HCO3−) ≥23 mEq/L in patients with reduced eGFR and metabolic acidosis improved vascular reactivity, assessed by FMD, is affirmed by future studies, preferably ones that are blinded and placebo controlled, these data suggest that NaHCO3 therapy as such in patients fitting these criteria is sufficient to improve vascular reactivity, at least up to 6 weeks. Because their most recent report3 showed that FMD improved with NaHCO3 therapy at 1 month in participants without metabolic acidosis but was not sustained at 6 and 12 months, follow-up studies in participants with metabolic acidosis should examine at least 1 year of treatment. As far as FMD is an adequate surrogate for subsequent CVD, these data suggest that NaHCO3 treatment of metabolic acidosis (i.e., with plasma [HCO3−] < 22 mEq/L) in patients with CKD holds promise to reduce their high risk of CVD. Such treatment aligns with current recommendations for patients with CKD and metabolic acidosis characterized by plasma (HCO3−) <22 mEq/L, intended to avoid the many other complications associated with this most extreme manifestation of acid stress.,This latest contribution by Kendrick and colleagues begins examining whether oral NaHCO3 in individuals with CKD, reduced eGFR, and plasma (HCO3−) >22 mEq/L but at risk of eubicarbonatemic acidosis might improve surrogates of subsequent CVD despite current guidelines not supporting sodium-based alkali treatment of such individuals. This is pertinent given the potential adverse effects of the obligate sodium load with NaHCO3 therapy. As indicated, this well-done study supports that NaHCO3 therapy up to 1 year did not improve FMD and aPWV or reduce the left ventricular mass.3 The authors appropriately qualify their results by offering that higher NaHCO3 doses might have been effective, but this dose reduced acid retention in participants with eubicarbonatemic acidosis,5 and the increased urine citrate excretion is consistent with reduced acid retention,1 supporting treatment effectiveness. In agreement with the authors, until additional studies say otherwise; these data do not support NaHCO3 therapy as cardiovascular protection in patients with CKD who do not have metabolic acidosis.,Another important question raised by these studies is the method(s) by which acid stress in patients with CKD can or should be treated to yield cardiovascular protection in addition to correcting acid stress. Sodium-based alkali, such as NaHCO3 and/or sodium citrate, obligates a sodium load with potential adverse consequences on CVD risk6 and reducing the cardiovascular7 benefits of angiotensin receptor antagonists, a mainstay therapy for patients with CKD. In addition, increased dietary sodium in participants reduced FMD8 and increased aPWV9 so that the obligate sodium load in participants studied by Kendrick et al.3 might have mitigated any beneficial effects of alkali on these potential surrogates of subsequent CVD. Base-producing fruits and vegetables (F+V) in patients with CKD effectively treated metabolic acidosis in patients with CKD without an obligate sodium load10 and reduced acid retention in those with eubicarbonatemic acidosis1 and so might be alternative treatments of acid stress in patients with CKD. Treating metabolic acidosis in participants with CKD using F+V was associated with improved cardiovascular risk parameters.10 Whether F+V treatment of acid stress or specific phases along its continuum reduces CVD risk awaits further study. That diets high in F+Vs are recommended first-line therapy for patients with diabetes and hypertension, the two most common US causes of CKD, additionally supports further study of F+V as cardiovascular protection in patients with CKD. Clinicians must use caution, however, prescribing diets high in F+V to patients with CKD because of the obligate potassium load that accompanies F+Vs, given the reduced potassium excretory capacity in patients with reduced eGFR.,These welcome studies by Kendrick et al. continue our understanding of the association of acid stress and adverse cardiovascular outcomes, which phases in the acid stress continuum might be causative, and how they might be optimally treated to simultaneously reduce acid stress but also reduce the risk of subsequent CVD. The kidney and cardiovascular communities await follow-up studies from this and other interested groups."
11,11,188,37801688,https://www.doi.org/10.2215/CJN.0000000000000338,https://journals.lww.com/,"IgA nephropathy is the commonest primary glomerulonephritis worldwide. It is often diagnosed in younger adults, and the socioeconomic burden is therefore high. Recent observational data from the United Kingdom indicate that most patients (even with modest proteinuria) are at risk of kidney failure unless their annual rate of kidney function decline can be maintained at ≤1 ml/min per 1.73 m2.1,Until recently, there has been a paucity of safe and effective treatments for this condition. The therapeutic landscape is rapidly changing, in large part due to advances in our understanding of its underlying pathogenesis and the acceptance of surrogate outcomes, such as proteinuria reduction and rate of change (slope) of eGFR by regulatory authorities as the basis for accelerated approval of new therapies. This has made conducting clinical trials more feasible and has transformed drug development in IgA nephropathy. There are now two treatments that recently received conditional approval: Nefecon (Tarpeyo/Kinpeygo) and Sparsentan (Filspari). In addition, more than 20 clinical trials are under way evaluating drugs that target B cells, the complement system, the endothelin system, and other pathways. On the basis of recent trial data, sodium-glucose cotransporter-2 inhibitors are being used frequently in IgA nephropathy.,It is widely accepted that multiple hits must occur for IgA nephropathy to develop. First, there is an increase in levels of circulating IgA1 that lack galactose from its hinge region (Gd-IgA1) (hit 1). IgG and IgA autoantibodies are generated against Gd-IgA1 in susceptible individuals (hit 2), leading to the formation of circulating immune complexes (hit 3). These have a propensity to deposit in the glomerular mesangium and trigger mesangial cell activation, cytokine and extracellular matrix release, and podocyte injury (hit 4). The complement system is activated, accelerating glomerular damage, and ultimately, tubulointerstitial fibrosis and loss of kidney function ensues. Multiple lines of evidence point toward B cells derived from the mucosa-associated lymphoid tissue, especially within the gut, as being the major source of circulating Gd-IgA1 in IgA nephropathy (Figure 1).2,Early studies using phenytoin to suppress total serum IgA levels did not alter the natural evolution of IgA nephropathy or IgA immune complex levels, indicating that a more specific approach is required. Targeting B-cell production of Gd-IgA1 has recently become a major area of interest. An early study demonstrated that treatment with rituximab did not reduce proteinuria or stabilize eGFR in patients with IgA nephropathy and importantly did not decrease Gd-IgA1 levels and the corresponding autoantibodies, despite depletion of peripheral B cells.3 This may be due to the inability of rituximab to deplete tissue resident B cells or CD20-negative plasma cells.,There is now significant interest in targeting the B-cell survival factors, a proliferation-inducing ligand (APRIL) and B-cell activating factor (BAFF), not only in IgA nephropathy but also in other autoimmune diseases. These are related cytokines of the TNF superfamily, produced by antigen-exposed dendritic cells, myeloid cells, and mucosal epithelial cells. Both act via the transmembrane activator and calcium modulator and cyclophilin ligand interactor (TACI) and B-cell maturation antigen (BCMA) receptors. BAFF, but not APRIL, also acts via the BAFF-receptor. Signaling via these receptors is essential for B-cell survival, maturation, and proliferation and for immunoglobulin class switching. At mucosal surfaces, APRIL via TACI promotes class switching of naïve B cells to IgA-producing B cells, where IgA plays a critical role in the host defense against pathogenic organisms and in promoting immune tolerance to host microbiota. APRIL also plays a key role in the maintenance of plasma cells, the primary source of secreted antibodies.,APRIL is, therefore, a key mediator of IgA production and thereby Gd-IgA1. In observational studies, patients with IgA nephropathy had increased APRIL levels that correlated with levels of Gd-IgA1 and risk of kidney disease progression.4 A single-nucleotide polymorphism in the gene encoding APRIL (TNFSF13) is associated with the risk of developing IgA nephropathy.5 In the ddY mouse model of IgA nephropathy, use of an anti-APRIL mAb suppressed serum IgA levels; reduced circulating immune complexes; significantly lowered the spontaneous glomerular deposition of IgA, IgG, and C3; and abrogated the development of proteinuria.6,Sibeprenlimab (VIS649) is a humanized IgG2 mAb directed against APRIL. In nonhuman primate studies, treatment with intravenous sibeprenlimab led to a dose-dependent reduction in serum IgA of up to 70%, and a reduction in IgA+, IgM+, and IgG+ cells was observed in the gut-associated lymphoid tissue.6 In healthy volunteers, sibeprenlimab reduced serum IgA, Gd-IgA1, IgM, and to a lesser extent IgG, the latter being important given the known association of IgG hypogammaglobulinemia with risk of infection.7 Sibeprenlimab is now being evaluated in IgA nephropathy in a dose-finding phase 2 randomized controlled trial (enVISion; NCT04287985), which has completed enrollment and is in follow-up, and a global phase 3 randomized controlled trial (VISIONARY; NCT05248646) that is currently recruiting.,A prespecified interim analysis of 77 participants in the enVISion trial showed that treatment with sibeprenlimab resulted in a 43% mean placebo-adjusted reduction in 24-hour urine protein-creatinine ratio by 9 months, with proteinuria reduction being evident by approximately 30 days after starting treatment. Promising preliminary eGFR data were also reported.8 These changes corresponded with marked reductions of free APRIL and in Gd-IgA1 (by 68%), total IgA (by 63%), and to a lesser extent IgG (by 35%). A total of 155 participants have now been enrolled, and full results are expected in late 2023.,Of importance when targeting B cells and immunoglobulin production is the impact on infection risk and vaccination response. Eleven serious adverse events were reported in the interim analysis of enVISion, but none were due to infection or deemed to be related to the study drug. A substudy of enVISion showed that those treated with sibeprenlimab had preserved serological responses to coronavirus disease 2019 mRNA vaccination above the predicted protective threshold and declines in severe acute respiratory syndrome coronavirus 2 antibody titers were similar to those treated with placebo, providing reassurance in this regard.9 Longer-term data are required to assess the safety of targeting B cells and plasma cells by this approach and any potential impact on immunogenicity.,Other therapies targeting APRIL±BAFF in IgA nephropathy also suggest promise, including one approved in lupus nephritis. Zigakibart (BION-1301) is being studied in a single-arm phase 1/2 trial of up to 40 patients with IgA nephropathy (NCT03945318).10 Significant reductions in free APRIL, serum IgA, and Gd-IgA1 have been reported, with a mean reduction of 54% in proteinuria observed at week 24 compared with baseline, and this agent will be studied in a forthcoming phase 3 trial (BEYOND; NCT05852938). Atacicept is a fusion protein that contains the extracellular portion of TACI, hence binding and inhibiting both APRIL and BAFF. After positive findings from the dose-finding JANUS study, interim 36-week data from the phase 2 ORIGIN trial demonstrated that treatment with atacicept 150 mg resulted in substantial reductions in Gd-IgA1 levels and a mean reduction of 43% in proteinuria. The early eGFR data were also reassuring.11 A phase 3 trial of atacicept in IgA nephropathy is now being planned (ORIGIN 3; NCT04716231). Other agents targeting both APRIL and BAFF include telitacicept, for which positive findings were reported from a smaller phase 2 trial in IgA nephropathy (NCT04291781), and povetacicept, which is being explored in an open-label basket trial of IgA nephropathy, membranous nephropathy, and lupus nephritis (NCT05732402).,Targeting APRIL alone or in combination with BAFF holds great potential for the treatment of IgA nephropathy, and we have now observed that lowering production of Gd-IgA1 may result in rapid clinical improvements for proteinuria reduction, indicating a renoprotective effect. Successful recruitment into ongoing phase 3 trials, and strategies to guarantee representation of patients of different ethnicities in those studies, will ensure our ability to deliver promising therapies to all patients with IgA nephropathy in the foreseeable future."
12,12,219,37737748,https://www.doi.org/10.1053/j.ajkd.2023.08.001,https://linkinghub.elsevier.com/,"Related Article, p. 534,Related Article, p. 534,Cystatin C–based estimated glomerular filtration rate (eGFRcystatin C) has been routinely used clinically in Sweden since 1994 to monitor kidney disease.1 A strategy to best estimate glomerular filtration rate (GFR) by simultaneously analyzing eGFRcystatin C and creatinine-based estimated GFR (eGFRcreatinine) and then comparing these estimates has been used since 2010.2 In doing so, a large repository of data has been accumulated, revealing discrepancies between eGFRcystatin C and eGFRcreatinine.,Interestingly, the discrepancy between eGFRcystatin C and eGFRcreatinine has gained interest outside Sweden over the last few years. In this issue of AJKD articles by Carrero et al3 and by Pinsino et al4 extensively and carefully describe the associations between eGFRcystatin C being lower than eGFRcreatinine and kidney failure with replacement therapy, acute kidney injury, atherosclerotic cardiovascular disease, heart failure, cardiovascular mortality, and all-cause mortality. The investigation by Carrero et al is an observational study of a population followed for outpatient care in the region of Stockholm, Sweden, and a strength of this study includes that it comprises 158,601 adults, the biggest population so far studied concerning the clinical associations of eGFRcystatin C being lower than eGFRcreatinine. A key strength of the study by Pinsino et al is that it concerns a very well characterized population of 1,970 patients with heart failure and reduced ejection fraction enrolled in the PARADIGM-HF project.4,Both articles refer to the influence of nonrenal factors as the most likely cause of a selective relative decrease in eGFRcystatin C compared with eGFRcreatinine and the associated clinical outcomes listed above. However, Carrero et al3 also refer to the alternative hypothesis that intrarenal selective glomerular hypofiltration syndrome may contribute to discrepancies in eGFRcystatin C and eGFRcreatinine.5, 6, 7, 8, 9, 10, 11 Selective hypofiltration syndromes refer to shrunken and/or elongated pores in the glomerular basement membrane, whereby glomerular filtration of 5-30 kDa molecules such as cystatin C is selectively reduced compared with molecules <1 kDa, such as creatinine.5, 6, 7, 8, 9, 10, 11 Consequently, the eGFRcystatin C to eGFRcreatinine ratio can fall significantly below 1, which has been associated with increased morbidity and mortality. These hypofiltration syndromes are based on the pore theory of hypofiltration, initially proposed in 2015.5, 6, 7, 8, 9, 10, 11 Further, in the elongated pore syndrome the pathophysiological mechanism is also supported by experimental physical measurement of the thickness of the glomerular basement membrane.8 Interestingly, selective hypofiltration is suggested by direct invasive measurements of glomerular permselectivity during the third trimester of pregnancy using neutral dextrans, demonstrating selective decrease in the filtration of substances corresponding to approximately 5-30 kDa.12 This finding corresponds to observations of the accumulation of proteins in the plasma the size of cystatin C in the third trimester.13 As a matter of fact, invasive determination of permselectivity was conducted as early as 1952, when selective hypofiltration of a grass polysaccharide of about 5 kDa compared to substances <1 kDa was demonstrated.14,The studies by Pinsino et al and Carrero et al both lack gold-standard assessments of GFR and patient comorbidities are not fully described. The implications of these study limitations are that they cannot evaluate the presence of selective glomerular hypofiltration syndromes or pertinent nonrenal factors among healthy individuals with normal GFR and without known comorbidities. However, in a study comprising 2,781 individuals with measured GFR (mGFR), known diagnoses, and known causes of death during a median of 5.6 years,15 an eGFRcystatin C to eGFRcreatinine ratio <0.70 was used to identify individuals with selective glomerular hypofiltration syndromes. The hazard ratio for all-cause mortality of individuals with the syndrome compared to individuals without the syndrome was 3.0 (95% CI 2.4-3.7) in the total population, 3.7 (95% CI 2.2-6.1) in the sub-cohort of individuals with no diagnosis, 4.1 (95% CI 2.6-6.5) in the sub-cohort with normal mGFR, and, finally, 7.3 (95% CI 2.3-23.2) in the sub-cohort with normal mGFR and no diagnosis.15 The high hazard ratio for all-cause mortality in this sub-cohort representing the most healthy group might seem surprising, but in the absence of other known comorbidities it suggests that selective glomerular hypofiltration syndromes may play a role. The specific death causes in this group were similar to those of individuals characterized using the KDIGO criteria as having kidney disorders, yet when the present KDIGO criteria are applied to this sub-cohort of individuals, they are found not to have any kidney disorder.15 Thus we speculate that there is a significant population of individuals without kidney disease according to the present KDIGO criteria and who yet display a very significant increase in all-cause mortality, possibly related to selective glomerular hypofiltration syndromes.9,15,Although selectively raised levels of cystatin C are used to identify selective glomerular hypofiltration syndromes,5, 6, 7, 8, 9, 10, 11 studies of the plasma proteomes in these conditions show that many other 10-40 kDa proteins are specifically raised,5,9,16 with a number described as promoting atherosclerosis.9,16 It is noteworthy that the only connection between most of these proteins is their elimination by glomerular filtration.5,9 Of note, even if selective glomerular hypofiltration syndromes with normal mGFR are connected to specific plasma proteome changes,5,9 a decrease in mGFR is generally associated with many more changes in the human plasma proteome, probably comprising about 36% of it.9 In our opinion, it seems more likely to attribute the many clinical consequences of reduced mGFR, or selective glomerular hypofiltration syndromes, to large-scale changes in the human proteome than to a few specific nonrenal factors.9,From our perspective, more than 30 investigations published after the initial report of shrunken pore syndrome in 20155,9 strongly support that further development of diagnosing kidney disorders is not to try to find nonrenal factors explaining the superiority of cystatin C over creatinine in predicting the mortality and morbidity associated with kidney disorders. We believe that describing the effects of alterations in the plasma proteome connected to selective glomerular hypofiltration syndromes will be a more fruitful way of developing the diagnosis of kidney disorders and ways to treat them.5,9 This suggestion also challenges the present KDIGO criteria for diagnosing kidney disorders. For example, the assertion that cystatin C should only be used when one suspects a bias in the creatinine-based estimation of GFR is clearly insufficient, as most causes of discrepancies between eGFRcystatin C and eGFRcreatinine are unknown and are likely often owing to the presence of selective glomerular hypofiltration syndromes. Simultaneous use of creatinine and cystatin C will also allow identification of selective glomerular hypofiltration syndromes, which is not possible with the present KDIGO criteria. Selective glomerular hypofiltration syndromes may represent 0.3%-36% of the individuals in the patient cohorts so far studied.9 Another reason that eGFRcystatin C equations should be more than supplementary to eGFRcreatinine equations is that they do not require coefficients for race or sex.9,17 Box 1 lists the reasons why analysis of cystatin C should be considered as an integral part of updated KDIGO criteria.,•It may allow a more reliable estimation of GFR1,2,9•It may omit problems connected to the use of race and sex coefficients in estimation of GFR9,17•It will facilitate diagnosing selective glomerular hypofiltration syndromes5, 6, 7, 8, 9, 10, 11•It will facilitate efforts to refocus nephrology from looking for extrarenal substances explaining the superiority of cystatin C over creatinine in diagnosing kidney disease to other studies, such as those of plasma proteomes, which may further enhance understanding of the pathophysiological effects of kidney disease and how to counteract such effects5,9,16,It may allow a more reliable estimation of GFR1,2,9,It may omit problems connected to the use of race and sex coefficients in estimation of GFR9,17,It will facilitate diagnosing selective glomerular hypofiltration syndromes5, 6, 7, 8, 9, 10, 11,It will facilitate efforts to refocus nephrology from looking for extrarenal substances explaining the superiority of cystatin C over creatinine in diagnosing kidney disease to other studies, such as those of plasma proteomes, which may further enhance understanding of the pathophysiological effects of kidney disease and how to counteract such effects5,9,16,Several internet tools to analyze the presence of selective glomerular hypofiltration syndromes are available (eg, https://www.egfrcalculator.com)."
13,13,282,37318401,https://www.doi.org/10.1053/j.ajkd.2023.05.001,https://linkinghub.elsevier.com/,"Related Article, p. 179,Related Article, p. 179,The kidney community has increasingly recognized the need for interventions to support shared decision-making (SDM) and improve the quality of education and counseling for persons with advanced chronic kidney disease (CKD). High-quality decision-making is particularly important for older adults facing intensive lifelong treatments like dialysis. For older adults, dialysis is often a destination therapy and is associated with substantial quality-of-life burdens and sometimes limited survival benefits.1 As such, decisions about dialysis initiation should be guided by patients’ preferences and values. However, few patients have sufficient knowledge about prognosis and treatment options to meaningfully consider tradeoffs between treatments.2 For some, this has led to regret following dialysis initiation.3,4 Although SDM can help align patient values and preferences for care with treatment selected, few older adults with CKD experience SDM.2,In this issue of AJKD, Wong et al5 describe a randomized pilot trial to test the acceptability and feasibility of a decision aid for conservative management among older patients with advanced CKD and their care partners. Decision aids can facilitate SDM by educating patients and empowering them to engage actively in decision-making. Decision aids should provide balanced information about options, clarify risks and benefits, describe prognosis, encourage patients to reflect on values and goals (related to medical care and more broadly), connect preferences to treatment options, and help patients communicate preferences to providers.2,6,7 In a randomized pilot trial of patients aged 75 years and older with CKD stages 4 and 5 and their care partners, 92 patients and 56 family members were randomized to usual care with or without a decision aid about conservative management. Wong et al5 found that the decision aid was well accepted, implementation was feasible, and participants randomized to the decision aid were significantly more likely to discuss conservative management with a clinician. The increase in discussions suggests that patients are largely open to learning more about conservative management. However, discussions remained uncommon, occurring in 26% of decision aid recipients compared to just 3% of usual care recipients.,With increased attention to patient activation and SDM in nephrology, Wong’s study highlights important questions about designing and testing interventions to improve education and decision-making for patients with advanced CKD. Best practices for the design of clinical trials for behavioral interventions draw upon human-centered design. This process entails several steps: First is discovery, partnership with diverse stakeholders, and clear articulation of the gap. As an illustrative example, when our team set out to examine how and why older patients in the United States, including those in their 80s and 90s, were opting for dialysis, we engaged in formative interviews with patients. This was a first step in the development of the Decision Aid for Renal Therapy (DART), a web-based, interactive decision aid to support older adults facing dialysis initiation decisions.8 The DART trial tested the comparative effectiveness of DART compared to usual education among 400 patients with advanced CKD at 4 geographically diverse sites within the United States. During this important first step, it quickly became clear that our framing missed an important gap. Overwhelmingly, patients had not recognized that they had a choice to initiate dialysis, either not identifying dialysis initiation as a choice (perceiving instead that it was necessary to prevent imminent death) or not recognizing dialysis as their choice (instead stating often that their doctor had decided).9 This example clarifies the importance of formative research and of longstanding partnerships with stakeholders, including patients, care partners, and clinicians, in refining research questions and identifying relevant outcomes.,Second, human-centered design calls for iterative development of interventions with feedback from diverse stakeholders at each stage. This includes review of the peer-reviewed and gray literature, as well as formative interviews or focus groups with patients, care partners, clinicians, and other experts. Trained medical writers and health communication experts should help ensure that the prototype conforms to health literacy standards, such as the principles outlined in the Centers for Medicare and Medicaid Services Toolkit for Making Written Material Clear and Effective. These standards include not relying heavily on medical terminology, using graphics in context, and using short, concise phrasing.10,11 This “alpha” version should be pretested with patients, care partners, clinicians, and other experts and assessed using interviews and focus groups, with the goal of obtaining information about the intervention’s clarity, value, usability, and acceptability. Strategies like cognitive testing and “think aloud” approaches to better understand participant experience and burden are crucial. The intervention should then be revised for a “beta” version and piloted, with reporting on acceptability, usability, and preliminary effectiveness. In designing clinical trials, stakeholder advisory boards can help investigators identify key outcomes, improve recruitment approaches, and consider future implementation in real-world settings.12,13 Advisory boards should include patients, advocacy organizations, multidisciplinary clinicians, community leaders, and other relevant experts, where input and expertise is sought and integrated throughout the life cycle of the study.,Selecting meaningful outcomes is crucial for SDM intervention trials in CKD. This includes process measures, which assess prognostic understanding, knowledge, patient activation, and SDM. These process-based measures clarify the quality of informed decision-making. By contrast, rates of dialysis initiation, conservative management, hospitalization, goal-concordant decisions, and quality of life are considered outcomes-based measures. It is worth noting that some measures are challenging to interpret in isolation. For example, focusing only on decisional conflict (the level of uncertainty a patient feels related to their decision) as a measure of decisional quality can be misleading without information about patients’ knowledge. A patient well informed of their treatment options, prognosis, and values may report low decisional conflict (low uncertainty in their decision), reflecting high-quality decision-making. Conversely, a patient could report low decisional conflict (high certainty in their decision) if they are unaware that they have more than 1 treatment option, or if they do not recognize a decision to be theirs. High-quality decision-making requires low decisional conflict, improvement in knowledge, clarification of treatment preferences, and ultimately goal-concordant care, including related clinical outcomes.14 In their paper, Wong et al5 explored process measures: patient and care partner discussions with clinicians related to conservative management as their primary outcome, and decisional conflict together with treatment preferences (dialysis, conservative management, and unsure). Including knowledge and additional clinical outcomes measures could strengthen future studies.,Generalizability remains a key challenge in interpreting findings from SDM trials in nephrology. The kidney community should come together to consider how to better engage patients in clinical trials, especially those from backgrounds historically under-represented in research. This includes better explaining and consenting patients in plain language (and in multiple languages), reducing respondent burden, and strategies to improve trustworthiness. Partnership with referring nephrologists is critical. Clinician champions at each site can help motivate colleagues and garner support for the intervention. However, reliance on clinicians to approve patient recruitment and participation can pose challenges, including clinicians’ hesitancy to refer patients that meet criteria because of perceptions that viewing a decision aid would upset or overwhelm their patients. Strategies to overcome this hesitancy are needed and often can be informed by clinician champions and the stakeholder board.,Best practices for generalizability also should prioritize designing decision aids in multiple languages and for diverse populations. Wong et al note that their study participants were predominantly white, generally well educated, and recruited from a single geographic region, and therefore study findings may not be generalizable. Cultural tailoring can improve the effectiveness of health communications.15,16 Future studies of this intervention and of other decision aids, including DART, should use cultural-tailoring frameworks to ensure that all older adults with CKD are able to engage in high-quality SDM.,How important are decision aids if patient preferences for dialysis are likely to change as their symptoms progress? Although nephrologists frequently cite instability of patient preferences, the DART trial found that most patients, once informed, expressed stable treatment preferences.8 Wong et al could examine this too. Understanding how and when patients form their final treatment preferences is crucial to informing the ideal timing for a decision aid. Investigators should determine how to characterize decision-making, specifically how to distinguish between lack of decision-making and decisions to pursue conservative management. Given that progression in CKD is unpredictable, data collection should occur at regular intervals (about every 3 months), coinciding with regularly occurring nephrology visits.,Wong et al highlight the importance of decision aids in promoting patient-centered decision-making surrounding advanced CKD. With many decision aids being currently developed and tested, including the Yorkshire Dialysis Decision Aid (YODDA) Booklet,17 DART, and Wong et al’s “A Guide to Conservative Management,” there is increasing promise for improving decision-making for older patients with CKD."
14,14,291,35842013,https://www.doi.org/10.1053/j.ajkd.2022.05.014,https://linkinghub.elsevier.com/,"Efforts to reassess the inclusion of race in kidney function estimating equations led to momentous action that included a National Kidney Foundation and American Society of Nephrology Task Force recommendation for a unifying approach to the estimation of glomerular filtration rate (GFR) through the implementation of the CKD-EPI creatinine and creatinine–cystatin C equations refit without the race variable in a population with 30%-40% Black persons.1, 2, 3, 4 Compared to the Cockcroft-Gault formula, a major strength of modern estimating equations, including the Modification of Diet and Renal Disease (MDRD) Study and 2009 CKD-EPI creatinine (herein CKD-EPI2009) equations, is improved diversity of the study populations.3,5,6 Still, it is unclear what drove the non-GFR determinants of higher serum creatinine levels in US Black versus White persons observed during CKD-EPI2009 development and whether the dataset assembled for equation creation (which was also used to develop the new refit CKD-EPI creatinine equation) achieved ideal representation of the US population.,Twenty-six studies (10 for equation development and internal validation [n = 8,254] and 16 for external validation [n = 3,896]) were used to devise CKD-EPI2009. Criteria for study eligibility included having measured GFR using exogenous filtration markers and calibratable serum creatinine assays.3 Cross-sectional analyses used pooled data spanning years 1983-2006. Each uniquely designed study consisted of participants with distinct clinical and demographic characteristics (Table 1). For instance, the Diabetes Control and Complications Trial (DCCT) and Diabetic Renal Disease Study (DRDS) evaluated the natural progression of diabetic nephropathy and mainly included younger participants without CKD.3,7,8 Similarly, the Mayo and Cleveland Clinic Foundation (CCF) transplant donor populations were younger and healthier.3 Others (ie, Chronic Renal Insufficiency Cohort [CRIC], MDRD, the African American Study of Kidney Disease and Hypertension [AASK], and the Collaborative Study Group [CSG] Captopril Trial) were designed to identify modifiable risk factors to prevent or delay CKD progression and had older populations with comorbidities including obesity.3,9, 10, 11,Apart from demographic information, socioeconomic status (SES) may offer better interpretation of differences among populations used in kidney function estimation research.12, 13, 14 SES has been linked with CKD and progression to kidney failure.15,16 SES could also influence serum creatinine levels through non-GFR mechanisms for which biological mechanisms are not well elucidated. For example, SES could impact creatinine generation, as individuals with low-to-moderate SES are more likely to engage in occupations with higher physical activity demands and/or consume diets high in cooked meats, which then leads to higher serum creatinine levels.17,18 Moreover, individuals with lower SES may experience increased levels of chronic stress and dysregulated physiologic response to stress owing to cumulative chronic stress (ie, allostatic load) than those more socioeconomically privileged, which, hypothetically, via physiologic or epigenetic mechanisms could contribute to dysregulation of tubular creatinine handling, such as that seen with some medications.12,19,20,CKD-EPI2009 development data (n = 5,504) comprised mostly participants categorized as White/other (63%), Black (32%), Hispanic (5%), and Asian (1%), with race and ethnicity chiefly obtained by self-report.3,4 Among 10 studies used for equation development and internal validation (n = 8,254), MDRD, DCCT, CSG, CCF, and Mayo included over 80% non-Hispanic White participants (Table 1). Conversely, AASK included only Black participants. CRIC had comparable proportions of Black and White participants and had the highest Asian representation (8%) compared to the other studies (0%-2% Asian). Hispanic representation was highest in the MDRD Study (6%), the same in CRIC and CSG (3% each), and lower in other studies (0%-2%). Only DRDS had a high percentage (100%) of Native Americans (ie, Pima Indians from the Gila River Indian community).7 Native Hawaiian/Pacific Islanders, a group with an extremely high kidney failure incidence, were not represented.15 The proportion of non-White participants was lower among studies used for external validation (13 of the 16 studies had ≥85% White/other participants, and 8 included European populations).3,Mean ages of study populations used for equation development ranged between 29 and 55 years (Table 1). DCCT, CSG, DRDS, and the CCF and Mayo donor study populations were younger (≥98% were <65 years old) and had the largest percentage of participants under the age of 40 (100% of DCCT, 78% of CSG, and 49% of Mayo and 46% of CCF donor participants).3 The remaining study populations largely comprised middle-aged adults (53%-74% were 40-65 years old), with even fewer studies including adults over 65 years old. Studies generally had more men than women, except for DRDS and the donor studies (58%-61% women). Studies comprising younger populations had higher GFRs, with a mean measured GFR of 78 mL/min/1.73 m2 for CSG and >100 mL/min/1.73 m2 for DCCT and the donor studies. Among most studies used for external validation, participants were mostly middle-aged adults (mean ages: 32-59 years), with varying proportions of female participants (19%-71%).3,SES data were publicly available for 4 studies (MDRD, AASK, CRIC, and DCCT) on subsets of participants. These studies had participants from multiple US sites. SES data were unavailable for the remaining studies, which recruited predominantly from single communities.3,4 We compared differences in education, annual household income (AHI), and employment of study participants with corresponding national data from the US Census and US Bureau of Labor and Statistics and a National Health and Nutrition Examination Survey (NHANES) CKD population (1999-2006)16,21,22 (Table 1).,Among MDRD Study participants (1989-1991), of which 85% were White, 64% had at least 13 years of education, which was lower than the coinciding national data (among White US adults, 79% were high school [HS] graduates).21,23 A lower percentage of AASK participants (1995-2001) graduated HS (60%),9 compared to national averages during this period (73% of Black US adults) and Black adults with CKD from NHANES (69%).16 Among CRIC participants (2003-2006), a higher proportion (92%) of White adults were HS graduates compared to White US adults (84%) in 2003, whereas 75% of Black participants reported graduating HS, slightly lower than Black US adults but higher than Black adults with CKD (69%).14,16,21 For DCCT (1983-1989), with mostly White participants (97%), nearly 50% had graduated college, compared to 17% of White US adults.8,21,White CRIC participants had AHI comparable to national estimates; 50% reported an AHI >$50,000 (median household income [MHI] of White US adults: $49,061). However, Black CRIC participants had lower income levels than national estimates; nearly half of Black participants reported an AHI <$20,000, well below the MHI for Black US adults of $30,442.14 Among MDRD participants, 52% had an AHI of <$25,000 (compared to 44% of those with CKD, NHANES 1999-2002),23 which was lower than the MHI of $28,906 for all US adults. Nearly 50% of AASK participants reported an AHI <$15,000,9 whereas the MHI for Black US adults was $22,393. Limitations of these comparisons are that 19% of AASK participants declined to report AHI, cost of living varies across communities, and employment status influences income.,Among CRIC participants,14 Black and White participants had lower levels of employment (Black: 29%; White: 48%) compared to both US adults (Black: 57%; White: 63%) and adults with CKD from NHANES 1999-2006 (Black: 62%; White: 66%). Among MDRD participants,23 ∼70% were employed, generally higher than among US adults (White: 64%; Black: 56%) and adults with CKD (White: 66%; Black: 62%).16,22,23 Only 37% of AASK participants9 reported being employed, compared to 62% of Black adults with CKD.22 Notably, there were high percentages of retirees in some studies (MDRD: 30%; AASK: 25%; CRIC: 34% of Black and 32% of White participants).,Data on SES were not universally obtainable for the studies used to validate the CKD-EPI equations.,CKD-EPI2009 was the first GFR estimation equation derived using data assembled from multiple studies, each of which captured persons in 1 or more selected US locations. In part, age, sex, and race were used to capture non-GFR determinants of serum creatinine. Although AASK and DRDS were entirely composed of underrepresented populations, there was still low representation of several racial and ethnic groups, particularly among studies with young participants. Among the few studies that did report SES, White and Black participant SES may have been representative of the communities from which participants were recruited but was not entirely reflective of SES in US adults. Recognizing limitations in assessing SES, compared to national statistics, there was lower SES in AASK and CRIC Black participants; high proportions of retired participants in MDRD, AASK, and CRIC; higher employment in MDRD; and higher education attainment in the mostly White DCCT participants. This is important if SES is a non-GFR determinant of serum creatinine levels. These observations highlight the difficulty for researchers in achieving representativeness and the need to better characterize study populations in clinical research.,Researchers aspire to recruit participants representative of the patients to whom the findings will be applied. While progress was made in including more diverse participants in estimating equation research (and this examination does not invalidate equations), exploring the characteristics of the participants in the studies used for derivation of CKD-EPI2009 identifies opportunities, such as improved national sampling of participants and collection of SES data, to enhance future estimating equation study population diversity. Although we examined study participant representation focused on SES-related factors, investigators should also capture other social determinants of health (SDoH) in the future to allow for similar comparisons and interpretation of health information. SDoH include the circumstances in which individuals are born, grow, work, live, and age that influence their health. Such factors include economic stability, health care access and quality, neighborhood and built environment, and social community in addition to education.13,24 A National Institute on Minority Health and Health Disparities initiative established standard SDoH measures with new protocols added to the Phenotypes and eXposures (PhenX) Toolkit’s assessments.25 Application of these well-validated and low-burden measures may facilitate data harmonization between studies. Capturing and reporting more comprehensive SDoH will enhance the interpretation and utility of future studies impacting the diverse US population to which research results are applied."
15,15,301,38081406,https://www.doi.org/10.1053/j.ajkd.2023.12.001,https://linkinghub.elsevier.com/,"Commentary on: Grimm PR, Tatomir A, Rosenbaek LL, et al. Dietary potassium stimulates Ppp1Ca-Ppp1r1a dephosphorylation of kidney NaCl co-transporter and reduces blood pressure. J Clin Invest. 2023;133(21):e158498. doi:10.1172/JCI158498,Commentary on: Grimm PR, Tatomir A, Rosenbaek LL, et al. Dietary potassium stimulates Ppp1Ca-Ppp1r1a dephosphorylation of kidney NaCl co-transporter and reduces blood pressure. J Clin Invest. 2023;133(21):e158498. doi:10.1172/JCI158498,Dietary sodium reduction is the cornerstone of nonpharmacological modifications for lowering blood pressure (BP) in individuals with hypertension.1 In recent years, more attention has also focused on increasing dietary potassium and other micronutrients, such as with the Dietary Approach to Stop Hypertension (DASH) diet, which provides about 4,700 mg (120 mmol) of potassium in a day.2 The exact mechanism of the BP-lowering effect of potassium intake is being elucidated in the last decade or so, with an elegant body of research describing a “potassium switch” wherein high potassium intake inactivates the thiazide-sensitive sodium chloride cotransporter (NCC) via the with-no-lysine (WNK) and STE20/SPS1-related proline-alanine-rich protein kinase (SPAK) pathway to induce natriuresis and decrease BP. In a recent study by Grimm et al,3 more dots are connected to complete the understanding of potassium intake, natriuresis, and BP-lowering effects.,In wild-type mice, high potassium levels cause NCC dephosphorylation, and subsequent natriuresis, similar to the effect of a thiazide diuretic. Grimm et al used mice with an activating mutation in SPAK, to maintain NCC phosphorylation even at higher serum potassium levels.3 However, after a 4-day high potassium diet, the constitutive SPAK phosphorylation of NCC was overridden, suggesting the presence of a protein phosphatase that directly dephosphorylates NCC. Next, transcriptomics was used to identify potassium-regulated genes profiling all 71 serine/threonine phosphatases and regulatory subunits known to be renally expressed. This identified Ppp1Ca encoding catalytic protein phosphatase PP1A as the most potassium-upregulated gene and Ppp1r1a encoding a protein phosphatase inhibitory subunit as the only potassium downregulated gene. The Ppp1Ca gene and PP1A protein were tightly upregulated in the distal convoluted tubule (DCT) over a physiological range of plasma potassium (∼3.8 to 4.8 mM) correlating with the titration of NCC dephosphorylation. With immunofluorescent confocal microscopy, the authors revealed that the increase in PP1A occurs specifically in the DCT, and the change is most evident at the apical membrane, correlating with NCC dephosphorylation. They also demonstrated that using immunoprecipitated phosphorylated NCC as a substrate, its phosphorylation was significantly reduced after incubation with PP1A. The potassium-dependent NCC dephosphorylation was inhibited by tautomycetin, an inhibitor of PP1A that is 20-40 times more selective for PP1A than other phosphatases. Lastly, the authors compared NCC phosphorylation and BP (as measured by radiotelemetry) in wild-type and the SPAK-activated mice as dietary potassium was increased. In the wild-type mice, there was a sigmoidal dephosphorylation response in NCC from a plasma potassium of 3 to 4.5 mM, paralleled by similar-shaped BP responses, with the systolic BP decreasing by 8 mm Hg as K+ increased. In the constitutionally SPAK-activated mice, this relationship was right-shifted, with no dephosphorylation in NCC observed until the plasma K+ reached 5 mM, and then a precipitous decrease in systolic BP by 20 mm Hg. In these mice, profound hyperkalemia drives an increase in PP1A activity to overcome constitutive phosphorylation of NCC. In summary, PP1A and I1 act as components of the signaling pathway that turns NCC off directly and reduces BP in response to a potassium-rich diet.,As is apparent from the description above, this is a meticulous and thorough body of work demonstrating how high potassium intake drives natriuresis and lowers BP via the direct action of the PP1A phosphatase. However, as the authors point out, the DCT is a small part of the kidney cortex, and their transcriptomics screen may have missed other less common phosphatases. Techniques such as single-cell RNA sequencing may help identify more specific DCT transcriptional responses to changes in potassium. There is likely additional modulation of the PP1A-NCC relationship either by WNK-SPAK or by other phosphatases. Though this study addressed acute and perhaps subacute effect of high potassium intake on NCC phosphorylation status, would this effect be chronically sustained under the same diet? Indeed, data from DASH and from other randomized controlled trials (RCTs) indicate that the effect of high potassium intake persists throughout the time of study follow-up (many months). Additionally, translating the changes in plasma potassium concentration that were observed in this study to humans may not be straightforward. There might be a risk of hyperkalemia, particularly in those with limitations in excreting potassium, that needs to be studied.,A key aspect to understand is whether these mouse studies correlate with human physiology. In a recent crossover study, oral potassium supplementation resulted in lower urinary extracellular vesicle levels of NCC and phosphorylated NCC among healthy adults on a baseline high-sodium, low-potassium diet.4 This finding may provide indirect mechanistic evidence of the “potassium switch” mechanism in humans, with reduced shedding of NCC with a change from a low-potassium to a high-potassium diet.,The BP-lowering effect of potassium itself has been well established. A systematic review conducted by the World Health Organization included 22 RCTs with 1,606 participants and 11 cohort studies with 127,038 participants.5 The effect of increased potassium intake from the RCTs overall was a reduction in BP by 5.9/3.8 mm Hg. Though a clear dose-response linear relationship could not be demonstrated, systolic BP was reduced by 7.2 mm Hg (95% CI, –12.4 to −1.9 mm Hg) in those who achieved potassium intake 91-120 mmol/d as compared to a more modest decrease of 3.5 mm Hg (95% CI, –5.2 to −1.8 mm Hg) in those with a lower achieved daily potassium intake of 71-90 mmol/d. Indeed, another meta-analysis also reported a U-shaped effect of high potassium intake on systolic and diastolic BP, with the largest decreases observed in those with an achieved daily potassium intake up to 90 mmol, plateauing thereafter.6 Furthermore, the effect of high potassium intake on BP was larger in untreated hypertensive versus normotensive individuals, and in hypertensive individuals with high sodium intake (≥4 g/d) as compared to those with sodium intake <3 g/d and 3-4 g/d. This appears highly relevant to findings by Grimm et al confirming that the “potassium switch” (ie, dephosphorylation of NCC) is indeed induced by “higher” potassium intake, independent of sodium intake per se, and that the combination of low potassium and high sodium intake in hypertensive individuals could result in the largest BP decreases.3,These findings can also put to rest arguments about the role of increasing potassium intake on BP, which has been somewhat contentious.7, 8, 9, 10 Diets such as DASH, which are rich in potassium, also result in a lower sodium intake, and many efforts have been made to tease out the respective contributions of lowering sodium intake and increasing potassium intake. Though sodium and potassium are inextricably linked together, since the “potassium switch” fundamentally works by inducing natriuresis, this body of work clearly establishes the independent mechanism of BP lowering with higher potassium intake.,We salute these important findings. They clearly suggest that desirable and clinically meaningful decreases in BP can be achieved by increasing potassium in the diet. This may be possible without a requirement to change sodium intake at the individual level, and possibly the societal level as well. The Salt Substitute and Stroke Study (SSASS), which was a cluster RCT conducted in 20,995 participants in 600 villages in China, used a salt substitute (75% sodium chloride with 25% potassium chloride) and reported a small decrease in BP (−3.3/−0.7 mm Hg), but also a decrease in cardiovascular (relative risk [RR], 0.87; 95% CI, 0.79-0.96) and all-cause mortality (RR, 0.88; 95% CI, 0.82-0.95).11 Regardless of the sodium/potassium primacy debates, these salt substitutes provide a path for implementation where sodium reduction alone has stumbled so far, at least in settings such as China, India, and South America, where addition of salt occurs commonly in household cooking.12, 13, 14 In other settings such as North America and Western Europe, where sodium addition is common in food preparation outside the household, these sodium substitutes could be added by the food industry.,With potassium supplementation in mind, should we be concerned about a higher potassium in the food supply for patients with severe chronic kidney disease (CKD), who may have decreased ability to excrete potassium? The SSASS trial, which excluded participants with “serious renal impairment,” did not report higher rates of serious hyperkalemia.11 An RCT of a potassium supplement (KCl 40 mmol/d for 2 weeks) in 191 patients with a mean glomerular filtration rate of 31 mL/min/1.73 m2 reported a small but significant increase in serum potassium (from 4.3 ± 0.5 to 4.7 ± 0.6 mmol/L).15 The small proportion (11%) that developed hyperkalemia (plasma potassium 5.9 ± 0.4 mmol/L) were all older and had higher baseline plasma potassium. This small risk of biochemical hyperkalemia with very high intakes of potassium in people with CKD must be balanced against the cardiovascular benefits of higher potassium intake. A modeling study indeed suggests that despite factoring in potential harms of hyperkalemia, there would still be a net benefit of potassium-substituted salt usage in severe CKD, given the high burden of cardiovascular morbidity and mortality in this population.16 Ongoing studies will provide more information on effective methods to increase dietary potassium.17,18,The approach of increasing potassium intake in the food supply has always felt more achievable, at least from our perspective, as pressure on the food industry to just reduce sodium shifts towards replacing some of the sodium with potassium. Similarly, this approach does not impose undesired social and dietary restriction on individuals, but merely requires adding a bit more potassium in the daily diet. With these mechanistic data adding to the existing epidemiological and trial evidences, it is the right time to shift the sodium-based strategy to a sodium- and potassium-based one."
16,16,315,37907451,https://www.doi.org/10.1681/ASN.0000000000000251,https://journals.lww.com/,"The coronavirus disease 2019 (COVID-19) not only had a major impact on patient care and on the mental health of patients and caregivers but also posed specific challenges to medical journals, which faced a substantial number of submissions and publications, literally updating knowledge about this disease in real time.,Early in the pandemic, issues affecting patient care delivery, such as shortages of personal protective equipment (PPE), insufficient testing availability, and lack of effective therapies, uniquely affected patients with kidney diseases. PPE demand was particularly high in hemodialysis units where social distancing was not possible. One study reported case clustering in dialysis units and shifts, underlining the importance of staff and patient screening to limit the spread of the virus. Moreover, fewer infections were recorded when patients used modalities allowing for dialysis at home (home hemodialysis or peritoneal dialysis).1 Inequity was evident internationally, with hoarding of PPE and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) tests by wealthier countries.2 Proposed early therapies were derived from prior coronavirus outbreaks or on the basis of in vitro data showing improved viral clearance or inhibition of entrance of SARS-CoV-2 into cells. Yet, safety and efficacy among patients with kidney diseases was largely untested.,During the initial phase of the pandemic, mortality among patients on dialysis was particularly high. One study reported that 18 of 59 patients died as a consequence of COVID-19 infection. Among them, however, 11 had an advanced directive against intubation.3 Those dying of COVID-19 were also generally older and were less likely to be active on the deceased donor transplant register.1 Subsequent widespread implementation of testing and screening contributed to reduced mortality rates in these vulnerable populations.,Disruptions in transplant services were also common in the initial stage of the pandemic. According to the United Network for Organ Sharing (UNOS), the number of kidney transplants in the United States decreased in 2020 compared with 2019; in 2020, the number of waitlisted patients also fell by -9.97%. Moreover, fewer patients started KRT. Data from the Centers for Medicare & Medicaid Services indicate a 25% decrease in incident ESKD cases during 2020. This was mainly explained by individuals aged 75 years or older who did not start KRT.4,The introduction of vaccines in December 2020 raised additional challenges. Although few patients with kidney diseases were included in the landmark trials leading to the emergency use authorization of commonly used vaccines, patients with CKD and solid organ transplant recipients were prioritized to receive vaccines in most countries. The Journal contributed significantly to our understanding of vaccine effectiveness, regarding both clinical outcomes and humoral and cellular responses of vaccinated and unvaccinated patient populations. In patients receiving maintenance dialysis, an initial two-dose regimen reduced the risk of infection by 70% and the risk of a severe disease course by almost 85%.5 Early experience in patients undergoing chronic hemodialysis, peritoneal dialysis, or after kidney transplantation revealed vast differences in antibody response. Only a minority of kidney transplant recipients mounted a sufficient humoral response after receiving two doses of an mRNA platform-based vaccine.6 The quality of response of hemodialysis and peritoneal dialysis patients was substantially worse compared with healthy controls, indicating weaker mounted humoral immunity and perhaps more frequent future boosters necessary for this vulnerable population.7 Although a majority of kidney transplant recipients had no or only a weak humoral response, most mounted a cellular response that did not differ from the response of patients on hemodialysis.6 However, uncertainty remains as to whether a cellular response alone is sufficient to prevent severe COVID-19 disease. Adding to the uncertainty about vaccine efficacy, therapies the nephrology community deployed, such as rituximab and mycophenolate mofetil, not only significantly reduced response to vaccines but also increased risk of severe COVID-19 disease.2,Patients with immune-mediated kidney diseases remain an understudied, yet relevant, population. The largest study focusing on COVID-19 outcomes included 125 patients with glomerular diseases.8 Half of them were hospitalized, and patients with a lower eGFR at baseline had a higher likelihood of developing AKI and a lower probability of recovering kidney function after infection. Another major concern in this patient population was whether administration of a potent vaccine that subsequently stimulated the immune system would induce de novo autoimmunity or relapse of an established glomerular disease. A population-level cohort study from British Columbia addressed the latter, including 1105 adult patients with a glomerular disease. A first vaccine dose, generally eliciting a relatively mild humoral response, was not associated with a higher relapse risk while subsequent administrations of COVID-19 vaccines increased the risk of glomerular disease relapse by around two-fold. These results, however, require critical interpretation because only 17% of relapsing patients had altered immunosuppression,9 indicating that most relapses were mild or self-limited.,The rate of AKI as a consequence of COVID-19 was substantial during the first waves of the pandemic. A large observational study from New York City found that 56.9% of 3345 adults with COVID-19 presented with AKI, and the risk was 1.5-fold higher compared with hospitalized patients without COVID-19. The need for KRT in this population was substantial, with a frequency of 4.9% and a 3.1-fold greater risk compared with other hospitalized patients without COVID-19.10 Other studies reported even higher rates of KRT, with frequencies near 9%.11 These findings emphasize the impact of the SARS-CoV-2 variant of the early pandemic and the surge of hospitalized patients on dialysis.,Early reports revealed racial disparities in COVID-19 renal outcomes; AKI was more frequent among non-Hispanic Black patients.10 In a high proportion of Black patients with AKI and de novo nephrotic range proteinuria, kidney biopsies revealed collapsing glomerulopathy, and genotyping identified APOL1 high-risk alleles in many cases.12 Kidney biopsies from patients without signs of concomitant glomerular lesions generally found nonspecific changes, such as acute tubular necrosis and an inflammatory infiltrate comprising lymphocytes and macrophages. SARS-CoV-2 binds the ACE2 protein when entering cells, and ACE2 expression is even higher in the kidneys than in the lungs.13 Several lines of evidence subsequently revealed that SARS-CoV-2 can directly infect the kidney: electron microscopy–imaged virus-like particles, PCR-detected viral RNA, and immunohistochemistry directly detected viral proteins.14 The high burden of AKI might thus be multifactorial, including genetic predisposition to COVID-19–associated glomerular disease, direct infection of kidney tissue, and sepsis, hemodynamic instability, or other factors leading to nonspecific but clinically relevant kidney damage.,As the prognosis of COVID-19 improved because of earlier recognition, therapeutic options, mass vaccination strategies, and reduced virulence when the omicron variant became dominant, postacute sequelae of the viral infection became a focus, and this was termed long COVID. A large database analysis among 1,726,683 US veterans focused on 30-day survivors of COVID-19 and compared these patients with noninfected controls. The study found higher risks of AKI, eGFR decline, ESKD, and major adverse kidney events (the composite of eGFR decline ≥50%, ESKD, or all-cause mortality) among patients surviving a SARS-CoV-2 infection. The subsequent annual eGFR decline of the patients was associated with the severity of infection, with an eGFR reduction of −3.26, −5.20, and −7.69 ml/min per 1.73 m2 per year in those without hospitalization, hospitalized, and those admitted to an intensive care unit, respectively.15,Throughout the pandemic, unique challenges related to disease prevention, acute infection, chronic consequences, and other effects have affected the nephrology community (Figure 1). Additional indirect consequences without a measurable impact on patient outcomes have not been as extensively studied, such as reduced visitor access in hospitals and dialysis units, which was especially relevant in end-of-life care provision. Furthermore, nearly one third of nephrology clinicians reported burnout and mental health distress.7 Many providers have left the nephrology community, and many workplaces could not fill the staff shortage, forcing departments to operate with reduced capacities.,Clinicians have had to remain constantly vigilant for new developments because of the rapid accumulation of new knowledge and frequently changing testing and treatment protocols. Submissions to medical journals increased; published academic papers increased by 7% in 2021. JASN contributed to the dissemination of knowledge relevant to the nephrology community when treating, diagnosing, and following patients with COVID-19; response to vaccines; and whether SARS-CoV-2 directly affects kidney structures. Clinicians depend on trusted sources with high standards for validity to keep abreast of new knowledge, particularly under such extraordinary circumstances. In the past 3 years, JASN has played an important role in disseminating high-quality, important research that advanced scientific understanding of the impact of COVID-19 on patients with kidney disease and on care delivery within the nephrology community."
17,17,319,36795086,https://www.doi.org/10.2215/CJN.0000000000000091,https://journals.lww.com/,"Coronavirus disease 2019 (COVID-19) has profoundly affected outpatient dialysis. In the United States, patients requiring maintenance dialysis share many risk factors associated with poor outcomes from COVID-19: lower socioeconomic status, older age, hypertension, and diabetes mellitus.1 Many dialysis-dependent patients have reduced immune responses, and the 85% of patients treating by in-center hemodialysis must congregate for care three times weekly.2 Existing social disparities among the disproportionate number of patients from underrepresented minority groups and low-income settings were exacerbated by the pandemic.3 Owing to fewer patients starting dialysis combined with the high mortality rate from COVID-19, for the first time in the history of the Medicare End Stage Renal Disease program, the population of maintenance dialysis patients declined in 2020.4 We review our experiences in the United States and make suggestions to mitigate the impact of future events on patients receiving maintenance dialysis.,Immediately after the first reported US death from COVID-19, a patient receiving maintenance hemodialysis, dialysis facilities took steps to minimize the risk to patients and those caring for them. Recognizing the respiratory spread, providers required masks for all individuals in their facilities well before the Centers for Disease Control and Prevention (CDC) recommended it. Safety protocols were hampered by limited availability of personal protective equipment (PPE) and inconsistencies in governmental regulations. US manufacturing was inadequate to meet the acute demand.5 Although the CDC recommended reserving PPE for the care of those with known or suspected COVID-19, the Occupational Safety and Health Administration required N-95 masks for all those providing care in dialysis facilities. Lessons learned: A pipeline of PPE is essential; US manufacturing capacity must increase and must be sufficiently flexible to accommodate demand in a public health emergency (PHE). Governmental agencies must provide consistent consensus communication.,Surveyor visits to ensure safety and adherence to infection prevention standards ironically hampered facilities from adhering to such standards. Survey activities required additional personnel in the dialysis facility, reduced physical distancing, and diverted staff time and attention away from caring for patients. Lesson learned: During a PHE, facility inspections should be limited to assessing patient safety and should be conducted remotely.,In response to the highly contagious nature of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), facilities needed to implement physical strategies to reduce the risk of transmission. Protocols to limit the presence of staff members who were not absolutely required in the treatment area included the use of telemedicine for patient encounters. Uncertainties persisted including whether plexiglass barriers provide sufficient protection against SARS-CoV-2 or whether facilities should be retrofitted with high-efficiency particulate absorbing filters.6,7Lessons learned: Protocols must be flexible and financial support must be available to make changes to protect patients and staff. Physical plant flexibility is needed to provide the right care at the right location.,Needed protocols were rapidly implemented to identify individuals with COVID-19 who had to be separated from the general population. Strategies depended on facility infrastructure, local prevalence of COVID-19, and partnerships with local or regional facilities. The goal was to provide the right care at the right location, allowing clinically stable patients to receive care at outpatient facilities rather than acute care hospitals. Depending on infection rates, the last treatment shift in a day could be designated for patients with COVID-19, allowing deep cleaning after the treatment day. Reflecting their inability to cohort patients, some facilities turned away patients with upper respiratory symptoms if they could not provide a negative PCR test. Early in the PHE, testing availability was limited and results were delayed, so patients missed treatments. Lesson learned: Priority is needed for assessing the infectious status of patients and staff in dialysis facilities.,Providers collaborated to develop the Dialysis Community Response Network, facilities dedicated to treating infected patients. Initially, two negative PCR tests were required to return to the general dialysis population (test-based strategy). Testing demonstrated continued positive PCR tests after symptoms resolved.8 Data sharing by Chief Medical Officers (CMOs) that patients shed incompetent virus particles confirmed the safety of a time-based protocol, allowing asymptomatic patients to return 10–20 days after symptom onset before confirmatory data were published. Lesson learned: Data sharing is critical to managing infectious outbreaks.,When vaccines and antiviral treatments became available, their use in kidney patients was hampered by a lack of safety and efficacy data, in part because patients with advanced kidney disease were excluded from clinical trials. Vaccine hesitancy was compounded by this lack of data and existing mistrust of the medical system and of new medications among many patients.9Lessons learned: A mandatory congregate population, patients with kidney failure are moderately immunocompromised and should be treated like group home residents. The US Food and Drug Administration (FDA) must take active measures to facilitate inclusion of kidney patients in clinical trials.,Vaccines were distributed to all dialysis providers through a model established for this purpose with the support of the CDC and American Society of Nephrology (ASN) in March 2021. The CDC provided vaccines to DaVita and Fresenius who distributed them to all other US dialysis organizations who requested them. This exceptionally successful model leverages patients' thrice weekly treatments at clinics by staff with whom they have a trusting relationship.10Lessons learned: The model should be replicated and extended for booster doses and reformulated vaccines to ensure that all patients have equal access.,A lack of data specific to kidney patients affected patient, provider, and clinician education, with national data sources such as the United States Renal Data System unable to provide contemporary data. ASN sponsored numerous webinars throughout the pandemic with experts presenting early data. Dialysis organizations published data on preprint servers and in peer-reviewed journals, which expedited review. At meetings of the CMOs under ASN sponsorship, observational data were shared openly. Lesson learned: We need a data registry to allow real-time data sharing and to build infrastructure for rapid, pragmatic clinical trials.,Communication with experts is critical to facilitate preparation for and management of emergencies. Infectious outbreaks and noninfectious emergencies such as hurricanes and the ice storm that affected Tennessee and Texas in 2021 demonstrate that regular communication is essential to protect the health of patients requiring maintenance dialysis. Lessons learned: Regular meetings of dialysis leaders should continue. Meetings with broader governmental representation such as the FDA and Administration for Strategic Preparedness and Response are critical.,Shortages of dialysis supplies and medications were exacerbated by global factors, including the war in Europe. A shortage of dialysis fluids threatened providers' ability to treat patients early in the pandemic, requiring some facilities to reduce flow rates. Lesson learned: Collaboration with government officials is essential in anticipating and addressing shortages.,The mission of providing safe dialysis to all those who require it depends on dedicated, skilled, highly specialized professional health care workers (HCWs). The pandemic has challenged the dialysis workforce greatly because of the compounded stressors of concerns for their own safety, challenges of providing safe care and ongoing staffing shortages resulting in morale decline, compassion fatigue, and burnout. COVID-19 exacerbated preexisting mental health issues because of isolation and widespread illness. HCWs disregarded their own needs to care for the high volume of patients at times with unsafe staffing ratios. HCWs must care for patients who refuse vaccination. HCWs felt a lack of respect despite the praise heaped on them, and the impact on HCW numbers and morale will be felt for years to come. ASN developed mental health modules to support the nephrology work force.11,Compounding the stress, many HCWs experienced COVID-19 or had to care for family members with COVID-19. Some HCWs refused to be vaccinated despite mandates, exacerbating staffing shortages. Some outpatient facilities are unable to accommodate patients starting dialysis because of insufficient staffing.,During the pandemic, shortages were exacerbated by financial incentives for nurses to travel. Many experienced nurses left outpatient dialysis in favor of inpatient care where salaries were higher. The paucity of dialysis nurses further stresses our vulnerable system and increases the risk of complications for our complicated patient population. It is unlikely that there will be sufficient numbers of HCWs to meet future emergencies any time soon. Lessons learned: Support for the financial, physical, and mental health of HCWs is vital. The finances of outpatient dialysis must allow for competitive salaries. Incentivizing self-care and home dialysis through patient empowerment and financial support is critical to provide broad access to care.,Emergency planning for dialysis must consider lessons learned from COVID-19. Flexibility is critical. Capacity for PPE manufacturing must be expanded. Facility inspections must be modified on the basis of emergency situations. Financial support must be provided to allow dialysis facilities to modify their physical plant to protect patients and HCWs. Dialysis organizations need to coordinate for real-time data analysis. Because they are immunocompromised and must congregate for life-sustaining dialysis treatments, patients with kidney failure must be part of clinical trials. The network administrator model must be expanded. Collaboration among dialysis providers and government representatives is essential. Support must be provided for the financial, physical, and mental health of HCWs."
18,18,343,37523307,https://www.doi.org/10.2215/CJN.0000000000000272,https://journals.lww.com/,"Kidney failure is a rapidly growing global public health problem and fatal without dialysis or transplantation. CKD is reported to affect >10% of the general population worldwide, which amounts to >800 million individuals.1 Although the global prevalence of kidney failure is uncertain, it is estimated to exceed 5 million people with concerns for capacity to meet the growing demand.2 Dialysis is lifesaving but at the same time burdensome and intrusive for patients and care partners and expensive for health care systems. In the United States, treatment of kidney failure consumes more than 6% of the total Medicare budget to care for <1% of the covered population.3 CKD (including kidney failure) is not only one of the leading causes of death, but it has also become the 12th leading cause of years of life lost (which is calculated from the number of deaths attributable to CKD and the life expectancy of individuals in various age groups at the time of their death from CKD) in 2017 and predicted to be the fifth highest cause of years of life lost globally by 2040.2,4 Worldwide, facility-based hemodialysis remains the most common form of dialysis despite numerous shortcomings including travel burden to patients, high ultrafiltration rates, inflexibility of treatment schedules, and its association with myriad dialysis-related symptoms including postdialysis fatigue.5,Home dialysis modalities, including peritoneal dialysis (PD) and home hemodialysis, offer greater patient autonomy, flexibility, and treatment satisfaction and can be less costly compared with facility-based hemodialysis. In recognition of these advantages, various efforts around the world have been implemented with a goal to enhance home dialysis uptake, with mixed results. Such efforts have included policy and payment innovations such as the Advancing American Kidney Health Initiatives, as well as governmental prioritization of home dialysis such as the PD-first policy in Hong Kong. There are multiple reasons underpinning low uptake of home dialysis globally, which may relate to health care system (e.g., home dialysis–preferred policies, relative delivery costs, reimbursement of center versus home dialysis), clinic/facility (e.g., clinician bias about home dialysis, education and expertise of health care professionals, reimbursement to patients for dialysis-related costs), and patients (e.g., suitable home environment, physical and cognitive limitations, health literacy). The Kidney Disease Improving Global Outcomes Controversies Conference on home dialysis held in 2021 recommended alignment of policy, resources at the center level, and clear leadership from informed and motivated clinical teams to facilitate greater access to home dialysis.6,In this context, we are pleased to announce a new CJASN series titled Home Dialysis: Fundamentals and Beyond. In this series, we have curated state-of-the-art, practice-centered reviews on home dialysis to highlight the most cogent issues needed for the nephrologist providing primary or consultative care for patients receiving home dialysis, with a focus on recent advances and innovations. The series will comprise a number of sets of review articles authored by clinical experts in the field of PD and home hemodialysis. The first set of articles will serve as a primer on home dialysis for the practicing nephrologist and will cover metrics interpretation and targets, quality and policy initiatives, approaches to setting up programs, and systems innovations to increase home dialysis utilization. The second set of articles will separately cover core topics in PD and home hemodialysis and will include physiology and basic principles in providing high-quality dialysis, approaches to the dialysis prescription, and modality-specific issues, such as PD-related infections and vascular access considerations in home hemodialysis. The final set of articles will cover the impact of recent advances in remote and virtual technology, the development and future potential of wearable dialysis technologies, and home dialysis management issues in special populations.,We believe a change in the treatment paradigm for kidney failure is necessary where home dialysis becomes the standard of care to meet holistic goals of patients with kidney failure and where systems innovations lead the way for growing availability and accessibility of home dialysis modalities. We hope this collection of articles will facilitate a deeper understanding in how to best deliver home dialysis and achieve optimal outcomes for all key stakeholders, as well as inspire readers about the current state and future growth potential for home dialysis modalities."
19,19,345,38127033,https://www.doi.org/10.1053/j.ajkd.2023.11.001,https://linkinghub.elsevier.com/,"Related Article, p. 173,Related Article, p. 173,Caring for patients with graft loss forces us to recognize the limits of medical science, explore the boundaries of our duties of care, and confront our own and our patients’ emotional burdens. Palliative care can help patients and providers navigate this difficult terrain.,The number of patients living with graft loss is rising, accounting for approximately 5% of the 100,000 Americans who initiate dialysis each year.1 Patients with graft failure report worse quality of life and have higher mortality on dialysis than end-stage kidney disease patients without a prior transplant.2,3 Although there is growing attention to immunosuppression weaning, timing of dialysis reinitiation, and considerations of retransplantation following graft failure, more work needs to be done to assess and meet the holistic needs of patients with graft loss.4,5 This population could greatly benefit from the integrated symptom management, psychosocial support, and spiritual guidance that palliative care services provide. Palliative care clinicians also excel in helping patients cope with the implications of an evolving serious illness and in applying patients’ values and preferences to complex, high-stakes medical decisions.6 Despite this, patients with graft loss are rarely referred to palliative care programs. Few centers have established palliative care programs for transplant candidates or recipients.7, 8, 6,In this issue of AJKD, Murakami et al9 explore transplant clinician attitudes toward palliative care for patients with kidney graft loss and identify provider perceptions of barriers to palliative care. The authors employ mixed-method techniques to integrate data from a national survey of 149 clinicians at 80 centers and interviews with 19 of the survey respondents. Half (54%) of survey respondents thought palliative care would benefit all patients with graft loss. Only 19% of survey respondents referred patients to outpatient palliative care despite the availability of such services at their centers. The survey suggests that obstacles to palliative care utilization exist at multiple levels: institutional, clinician, and patient/caregiver. Interviews expanded upon these barriers, identifying the transplant team’s sense of responsibility to the patient, lack of clarity regarding optimal timing for palliative care referral, and concerns about communication with patients in light of cultural and linguistic heterogeneity.,Unsurprisingly, Murakami et al’s results reflect misconceptions about palliative care that are common throughout the entire health care system. More than half of survey respondents believed that palliative care consults were only appropriate near the end of life. Similarly, many of the barriers to referral identified by transplant providers echo clinician-level barriers to palliative care referral in other contexts. For example, a recent study by Woodrell et al10 identified resource constraints, concerns that palliative care referral signaled “giving up” on a patient, and the sense that palliative care might be in conflict with oncologic therapies as barriers to palliative care for patients with hepatocellular carcinoma.,Murakami et al’s results also hint at complexities unique to the transplant context that the authors could not fully explore within their study’s constraints. First, as the authors note, there is a great deal of variation in the clinical context of graft loss. Graft loss can occur in the immediate postoperative period, within several years of transplant, or after many years of excellent function. It can develop after a slow decline in kidney function or after an acute illness. The qualitative results about optimal entry points for palliative care suggest that clinicians’ views about palliative care may be more complex than indicated by the survey findings and may vary significantly depending on the circumstances surrounding the graft failure. Surveys incorporating vignettes describing several graft loss scenarios might be used to draw out some of these more nuanced views.,Second, the significance of the longitudinal relationship between the transplant clinician and the patient merits further attention. While some participants reported that a longitudinal relationship with a patient facilitated their ability to transcend cultural differences, others worried that a long-term relationship could make it more difficult for a clinician to discuss graft loss with a patient in whom they feel invested. Palliative care clinicians can contribute to patient care without disrupting doctor-patient relationships. Rather than competing with the deep connection a patient may have with their transplant team, the palliative care clinician is likely to discuss separate but complementary aspects of care to help patients and their families cope with a progressive illness.,Third, the results suggest that clinicians’ emotions may mediate their comfort with and willingness to make palliative care referrals. Clinicians’ worries that patients would be “scared” or “depressed” with a discussion of palliative care may partially reflect their own anxieties. Furthermore, the authors note that clinicians may “grieve” a graft loss or consider graft failure a “clinical or personal failure of their duty to both donor and recipient.” It would be interesting to delve into the relationship between feelings of personal responsibility and ambivalence toward palliative care. If a clinician decides to accept the donor organ that has primary nonfunction or has an operative complication that contributed to graft loss, do their feelings of personal responsibility for the poor outcome interfere with their ability to discuss graft failure with the patient or decrease their openness to palliative care referral? Palliative care providers can remain sensitive to these complexities while offering fresh insight, unburdened by the transplant clinician’s emotions.,Finally, 1 participant noted that mandatory 1-year graft survival reporting could impact a clinician’s willingness to acknowledge early graft failure. While outcomes metrics facilitate patients’ informed selection of a transplant center and are widely desired by patients, the potential unintended consequences of this metric on patient care should be explored.11 Such investigation would be well timed given the Scientific Registry of Transplant Recipients’ ongoing Task 5 Initiative to re-evaluate the role and metrics in supporting transplant stakeholders’ decisions.12,The authors suggest the development of an integrative service model to meet the palliative care needs of transplant patients. Such services should be encouraged and can be based on suggestions by Wentlandt et al8,13 or the few existing transplant-specific palliative care programs.7 However, activities on both a smaller and a grander scale may also benefit patients. Centers with limited resources can implement simple measures to encourage interaction between the transplant and palliative care teams. This could begin with something as simple as holding a joint transplant palliative care grand rounds talk once a year. At a national level, optimizing the care of patients with poor prognoses following graft loss may require advocating for hospice programs to offer the option of dialysis continuation.14,Murakami et al’s article exemplifies the value of a mixed-methods approach in transplant health services research.15 The combination of surveys offering generalizable data from a larger sample size and interviews providing nuanced, granular data from a smaller number of participants can yield richer understanding than either approach alone. Even more extensive methodologic and analytic integration of qualitative and quantitative approaches could further enhance future studies. For example, sequential explanatory mixed-methods study designs would benefit from the purposeful sampling of survey respondents to ensure a range of views captured in the qualitative interviews. Individual-level linkage between survey and interview data might also reveal meaningful patterns.,Murakami and her colleagues offer a valuable partial roadmap to overcome the multilevel barriers to palliative care. We hope that future work will expand on this important foundation by giving voice to patients with graft loss and their care partners."
20,20,356,36750034,https://www.doi.org/10.2215/CJN.0000000000000084,https://journals.lww.com/,"Reinforcement learning formalizes the concept of learning from interactions.1 Broadly, reinforcement learning focuses on a setting in which an agent (decision maker) sequentially interacts with an environment that is partially unknown to them. At each stage, the agent takes an action and receives a reward. The objective of the agent is to maximize rewards accumulated in the long run. There are many situations in health care where decisions are made sequentially for which reinforcement learning approaches could prove useful for decision making. Throughout this article, we consider treatment prescription as an archetypical example to connect reinforcement learning concepts to a health care setting. In this setting, the care provider, the prescribed treatment, and the patients can be viewed as the agent, the action, and the environment, respectively, as depicted in Figure 1.,In this section, with the objective of making reinforcement learning literature more accessible to a clinical audience, we briefly introduce related fundamental concepts and approaches. We refer the interested reader to Sutton and Barto1 for a comprehensive introduction to reinforcement learning.,Markov decision processes (MDPs) are a formalism of the sequential decision-making problem that has been central to the theoretical and practical advancements of reinforcement learning. In each stage of an MDP, the agent observes the state of the environment and takes an action, which, in turn, results in a change of the state. This change of state is assumed to be probabilistic with the next state being determined only by the preceding state, the chosen action, and the transition probability. The agent also receives a reward that is a function of the taken action, the preceding state, and the subsequent state.,In an MDP, the objective of the agent is to maximize the return defined as the reward accumulated over a time horizon. In some applications, it is common to consider the horizon to be infinite, in which case the future rewards are discounted by a factor smaller than one. The selection of action by the agent on the basis of the observed state is known as the policy. More formally, a policy is a probabilistic mapping from states to each possible action. Because the policy and the reward are a function of the state, it is critical to estimate the utility of being in a certain state. More specifically, the value function is defined as the expected return starting from a given state under the chosen policy. Under this formalism, the objective of the agent is to find the optimal policy that maximizes the value function for all states.,Action-value methods are a class of reinforcement learning methods in which the actions are chosen on the basis of the estimation of their long-term value. A prominent example of an action-value method is Q-learning in which the agent iteratively takes actions with the highest estimated values and updates the action-state value function on the basis of new observations. Policy gradient methods are another class of reinforcement learning methods that seek to optimize the policy directly instead of choosing actions on the basis of their respective estimated value. Such methods could be advantageous in health care applications that entail a large number of possible actions, e.g., when recommending a wide range of drug dosages or treatment options.,Reinforcement learning frameworks and methods are broadly applicable to clinical settings in which decisions are made sequentially. A prominent clinical application of reinforcement learning is for treatment recommendation, which has been studied across a variety of diseases and treatments including radiation and chemotherapy for cancer, brain stimulation for epilepsy, and treatment strategies for sepsis.2–5 In such treatment recommendation settings, a policy is commonly known as a dynamic treatment regime. There are various other clinical applications of reinforcement learning including diagnosis, medical imaging, and decision support tools (see refs. 2–5 and the references therein).,Although there have been recent applications of machine learning in nephrology,6,7 to the best of the authors' knowledge, the application of reinforcement learning to nephrology has been primarily limited to optimizing the erythropoietin dosage in hemodialysis patients.8,9 However, there are other settings where reinforcement learning has the potential to improve patient care in nephrology. For example, reinforcement learning methods can be adopted in the treatment of the complications of AKI or CKD (Figure 1). In this problem, the state models the conditions of the patient (e.g., vital signs, laboratory test results including urine and blood tests, and urine output measurements). The action refers to the treatment options (e.g., the dosage of medications such as sodium polystyrene sulfonate, and hemodialysis). The reward models the improvement in patient conditions. Similarly, reinforcement learning can help automate and optimize the dosage of immunosuppressive drugs in kidney transplants.,Despite the success of reinforcement learning in several simplified clinical settings, their large-scale application to patient care faces several open challenges. The complexity of human biology complicates modeling clinical decision making as a reinforcement learning problem. The state space in such settings is often enormous, which could make a purely computational approach infeasible. Moreover, modeling all potential objectives a priori as a reward function may not be feasible. To overcome these challenges and realize the potential of reinforcement learning, clinical insight can play a pivotal role. More specifically, restricting the state space to only include highly relevant clinical variables could greatly reduce the computational complexity. Furthermore, using inverse reinforcement learning,2 relevant reward functions can be learned from retrospective studies assuming the optimality of clinical decisions.,Another critical challenge is addressing moral and ethical concerns. It is imperative to ensure that reinforcement learning methods do not cause harm to the patient. To this end, there exists a need for a thorough validation of such methods before their use in patient care. Hence, there is a need to go beyond retrospective studies that have been used for the proof of concept of most existing reinforcement learning methods in health care applications.2,3 The lessons learned from the success of reinforcement learning in other application areas (e.g., self-driving cars) can help navigate the path to realizing its potential in health care.,Accessible open-source simulation environments that enable researchers to compare various approaches are essential to the field of reinforcement learning. OpenAI Gym is currently the leading toolkit containing a wide range of simulated environments, e.g., surgical robotics.10 The development of high-quality and reliable simulation environments for nephrology and other health care applications can facilitate the development and validation of reinforcement learning methods beyond limited retrospective studies. The adoption of methods validated in such simulation environments in actual clinical settings will require clinicians' oversight. Similar to how self-driving cars require a human driver to ensure collision avoidance, clinicians' oversight is critical to ensure the safety of the patients, especially in the early stages of the adoption of reinforcement learning methods. The data from clinicians' decisions (e.g., overruling the automated treatment recommendation) can be used to improve the reliability of autonomous systems over time and reduce the burden of clinicians' oversight."
21,21,360,37024155,https://www.doi.org/10.1016/j.ajt.2023.01.014,https://linkinghub.elsevier.com/,
22,22,366,36914585,https://www.doi.org/10.2215/CJN.0000000000000125,https://journals.lww.com/,"Glycemia management is a perilous task for people with CKD, especially those with kidney failure. For patients with diabetes, individualization of glycemic control is a fundamental component of care.1 Long-term benefits of intensive glycemic therapy to reduce the risks of retinopathy, neuropathy, cardiovascular diseases, and progression of kidney disease (if still relevant) must be balanced against short-term risks of hypoglycemia and financial costs. However, rigorous trial data quantifying these benefits and risks for people with established CKD are scant, and several substantial barriers currently limit intensive therapy.1,2 First, severe hypoglycemia, a condition with high morbidity and mortality, is known to occur substantially more frequently with lower eGFR and to occur in the absence of diabetes among people with kidney failure (for reasons that remain incompletely understood). Second, hemoglobin A1c, a basic tool for guiding glucose-lowering therapies, loses precision at low eGFR and becomes inaccurate in kidney failure because of abnormalities in red blood cell turnover. Third, some medications that safely and effectively lower blood glucose are contraindicated at low eGFR (such as metformin). As a result, for patients with diabetes and kidney failure, managing glycemia could be compared with driving a curvy mountain road at night, without a map or headlights, in an old Mack truck.,Continuous glucose monitoring (CGM) is a rapidly evolving new technology that has revolutionized the treatment of diabetes, particularly for patients treated with insulin.2 CGM offers a new tool to quantify glycemia with unprecedented granularity, closely titrate therapy, and understand previously poorly characterized pathophysiology. Subcutaneous CGM sensors measure interstitial glucose concentrations that correlate well with blood glucose, with a fidelity that has progressively improved with each successive version. “Factory-calibrated” models allow patients to avoid finger sticks for self-monitored blood glucose measurements. Collection of data that is recorded but not immediately accessible to the CGM wearer (blinded mode, also called “professional CGM”) is particularly useful for research and allows quantification of an internationally standardized set of glycemic metrics and an ambulatory glucose profile.3 Real-time visualization of CGM data additionally allows patients to actively titrate their activities, food intake, and medication use to optimize glycemia. Rigorous clinical trials have demonstrated that real-time CGM can improve glycemic control without increasing risk of hypoglycemia for patients with type 1 and 2 diabetes using multiple daily insulin injections.4,5 Combining real-time CGM with an insulin pump in a hybrid closed-loop system further improves glycemic control, and this has become a preferred treatment of type 1 diabetes.,CGM may be particularly useful for patients with CKD, given the challenges of glycemia management in this population. Theoretical concerns about interfering substances and expanded extracellular fluid appear not to compromise modern CGM function. Among hemodialysis patients, the DexCom G6 was recently reported to have excellent performance compared with venous blood and capillary blood glucose.6 However, published studies using CGM to understand glycemic patterns and to assess the effect of CGM on clinical care in CKD have so far been limited to case series, small cohort studies, and pilot trials of relatively few participants.,In this issue of CJASN, Ushiogi et al. present what may be the largest study of CGM among people with CKD to date.7 A total of 366 participants recruited from nephrology and endocrinology departments in a single center in Japan, including 131 with kidney failure treated with hemodialysis (with and without diabetes); 132 with eGFR <60 ml/min per 1.73 m2 (with and without diabetes); and 103 control participants with diabetes, eGFR ≥60 ml/min per 1.73 m2, and normal urine albumin excretion, were included. Each participant wore the Abbot Freestyle Libre for 10 days in blinded mode. The proportion of participants with at least one observed glucose <70 mg/dl (or <54 mg/dl) in 7 days was significantly higher with lower kidney function (with or without diabetes), as was a metric that combines the amount of time with glucose <70 mg/dl and its severity (area above the curve). Only two participants reported symptoms of hypoglycemia, suggesting that these episodes of biochemical hypoglycemia were largely subclinical.,This study provides new data suggesting that the incidence of hypoglycemia, defined biochemically, increases with lower kidney function. These results parallel population-based studies that have reported higher incidence rates of severe hypoglycemia with lower eGFR, particularly among patients treated with dialysis.8,9 Like the population-based studies, the new study highlights the occurrence of hypoglycemia among dialysis patients without diabetes. In fact, among participants treated with dialysis, the proportion with any biochemical hypoglycemia was just as high for those without diabetes as for those with diabetes,7 which has not been observed for severe hypoglycemia in population-based studies.8,9 Overall, these data provide a compelling case to expedite evaluation of the application of CGM to patients with CKD to understand underlying pathophysiology and identify opportunities to improve care.,The precise estimates of prevalence, time, and severity of hypoglycemia reported in the new study should be viewed with some skepticism. First, the study used an early CGM system (Abbott FreeStyle Libre Pro) that lacks the precision of more recent models, especially in the lower range of blood glucose (a common limitation for CGM devices that is improving over time). CGM precision is routinely measured as the mean absolute relative difference (MARD) comparing CGM with capillary or plasma blood glucose, with a modern-day target of <10%. While one study of the Abbott FreeStyle Libre Pro reported a MARD of 13%,10 another reported a MARD of 18%, with a MARD of 36% for glucose values <70 mg/dl.11 Second, only a single CGM reading <70 mg/dl (or <54 mg/dl) during 7 days was used to define the presence of hypoglycemia. Together, these limitations likely led to an overestimation of the proportions of people with hypoglycemia, although trends across kidney function are likely correct.,There is much more to learn using CGM about glycemia and its management in CKD. A key first step is fully understanding the glycemic topography in this population, including unique subsets of patients such as those treated with hemodialysis or peritoneal dialysis, with or without diabetes. In addition to hypoglycemia, understanding the extent of uncontrolled hyperglycemia is also important because this may be substantially underestimated by hemoglobin A1c among patients with kidney failure and may still relate to short- and long-term complications (even if kidney protection is no longer a goal). Such studies would ideally use the most modern, accurate, and precise CGM sensors; quantify glycemia using the full range of established international standards3; explore unique glycemic patterns and their relationships to potentially modifiable clinical characteristics (including nutrition, diabetes medications, and dialysis procedures); and relate glycemia findings to relevant clinical outcomes. Large observational studies may also be useful to help establish which patients with CKD could derive clinically useful information from CGM, used either in blinded mode (to assess risks of hyperglycemia and hypoglycemia or establish an unbiased measure of average glycemic control) or in real time (to guide application of therapy). Moreover, trials are needed to test the effect of CGM on clinical care in CKD, potentially including a trial comparing the effect of different CGM targets on clinical outcomes. Notably, CGM metrics, such as glucose time in range, are now established and have been used as end points in clinical trials with acceptance by regulatory agencies,3 also facilitating the evaluation of new therapies to manage glycemia in CKD.,Clinically, we recommend that patients with diabetes and CKD who have established indications strongly consider using CGM. In the United States, Medicare and many state Medicaids currently provide reimbursement for CGM for patients with type 1 or 2 diabetes who use three or more daily insulin injections or an insulin pump. Given limitations to accuracy in the hypoglycemic range, finger-stick glucose testing should be considered to verify hypoglycemia noted with CGM. We recommend that nephrologists advocate for patients to use CGM when indicated and partner with endocrinologists who are knowledgeable about the latest technologies and trained in the interpretation and application of CGM data. Technology and data are evolving rapidly in this field. Hopefully high-quality studies of this exciting technology will lead to a better understanding of glycemia in CKD, new opportunities to shine light on the curvy road of glycemic management, and ultimately a map that guides patients and clinicians to improved clinical outcomes."
23,23,385,35810826,https://www.doi.org/10.1053/j.ajkd.2022.05.013,https://linkinghub.elsevier.com/,"Antineutrophil cytoplasmic antibody (ANCA)-associated vasculitides are rare disorders in childhood with a variable clinical presentation. Given ANCA vasculitides’ rarity, data informing clinical practice and treatment are mainly based on adult data. The purpose of this study was to characterize clinical characteristics of ANCA-associated glomerulonephritis (AAGN) in childhood to determine factors associated with adverse renal outcome and the requirement for kidney replacement therapy (KRT) across a global study population.,This was a retrospective cross-sectional international survey distributed through professional pediatric nephrology organizations from December 2019 to March 2020 intended to create a registry of children with AAGN to understand clinical practices. Through an online form, pediatric nephrologists entered demographic and clinical information on all children with AAGN in their center in de-identified fashion. All centers were required to obtain their own institutional ethics or governance approval. Inclusion criteria were patients under 20 years at presentation who were diagnosed with AAGN in 2000-2019 and had kidney involvement. Data elements that were collected included baseline demographic data, clinical features at presentation, treatment received (maintenance and induction), and data on 3 clinical outcomes: requirement for KRT, serum creatinine concentration (Scr), and death. Specifically, nephrologists were asked to report the peak Scr and any requirement for KRT in the first 3 months after presentation during the induction treatment period. Nephrologists were also asked to report on the need for KRT and vital status at last known follow-up. Further methodological details are in Item S1.,Based on responses received from 114 different clinicians, 337 children from 41 different countries were included in the final analysis. Median duration between initial presentation and last known follow-up was 26 (IQR, 11-57) months. Table S1 details baseline characteristics of included children. Table S2 shows the organ involvement at presentation across different clinical phenotypes.,Table 1 shows the most frequent induction and maintenance treatments used in the entire cohort. A total of 113 children (34%) received plasma exchange at induction. Sixteen deaths were reported in this cohort (5% mortality), with a mean age at death of 13.7 ±5.7 years.,Table 2 characterizes the clinical factors and treatment according to KRT requirements at initial presentation and at last known follow-up. We found a high prevalence of adverse renal outcomes, with 40% of children requiring KRT at last known follow-up, slightly higher than previously published data.1, 2, 3, 4 Children who did (vs did not) require KRT at last known follow-up had a higher peak Scr during the first 3 months after their initial presentation. There was a higher proportion of girls and a higher proportion of myeloperoxidase-ANCA positivity in children who required KRT at last known follow-up, compared to those who did not require KRT. We note that children who required KRT at last known follow-up were more likely to have received plasma exchange as induction treatment, but this may simply be because children with more severe kidney involvement at presentation were more likely to be treated with plasma exchange. We note that mycophenolate mofetil was used more commonly as maintenance treatment for children compared to azathioprine, which differs from suggestions in the adult literature.5,This study is unique, as it was conducted across 41 countries, giving a broad cross-sectional assessment of demographics and baseline characteristics of children affected by AAGN. Potential limitations of this study include an over-representation of children with severe renal outcomes, given the collection of data through a survey of pediatric nephrologists; but this is also a strength, as this study focuses on the subgroup of children with ANCA vasculitis who have kidney involvement. Data were only available at disease presentation and latest follow-up, which limits our ability to comment on kidney function over time. In addition, we did not collect data on histology, meaning that the diagnosis of AAGN was clinical rather than histological and we had limited ability to relate histology to clinical outcomes. We also do not have data on proteinuria at presentation, which is a further limitation. In conclusion, this large international cohort of children with AAGN demonstrates the high risk of chronic kidney disease and requirement for KRT in this population.,Download : Download Acrobat PDF file (300KB)Supplementary File (PDF). Item S1; Tables S1, S2."
24,24,394,36735373,https://www.doi.org/10.1681/ASN.0000000000000032,https://journals.lww.com/,"The first successful pregnancy in a kidney transplant recipient occurred in 1958,1 only 4 years after the first successful kidney transplant. Since then, nearly 2300 pregnancies in kidney transplant recipients have been reported to the Transplant Pregnancy Registry International (TPRI), a worldwide, voluntary registry of pregnancy outcomes in persons with solid organ transplants.2 The TPRI likely underestimates the true rate of post-transplant pregnancies given that 15% of kidney transplant recipients in the United States alone are persons of child-bearing age.3 The desire for parenthood in kidney transplant recipients approximates that in the general population, and for many transplant recipients, having children is an essential component of recovering a sense of normalcy after transplantation.4 The live birth rate (i.e., the proportion of clinically detectable pregnancies that result in a living child), among kidney transplant recipients reported in the TPRI, is comparable with that in the general US population.2,Despite the many successful pregnancies that have occurred, post-transplant pregnancy can pose significant risks to the gestational parent, the fetus, and the allograft. It is imperative that patients are appropriately counseled on potential pregnancy-associated risks to their allograft and general health so that they can make informed choices. Specifically, rates of preeclampsia are higher in kidney transplant recipients than in the general population.1 Mycophenolic acid (MPA) products, which are a common component of post-transplant immunosuppressive regimens, are teratogenic and associated with increased rates of first-term miscarriage.2 Infants born to kidney transplant recipients are frequently delivered preterm and small for gestational age.2 Exposure to fetal antigens and fluctuations in immunosuppressive drug levels can increase the potential for rejection of the allograft.5,6 Based on a patient's individual circumstance, post-transplant pregnancy may represent an unacceptably high health risk for some including those with decreased allograft function and proteinuria. Pregnancy risks are even greater in patients with an additional nonkidney transplant or multiorgan (i.e., combined kidney pancreas) transplant. Therefore, access to specialized reproductive health resources is essential for the proper care of pregnant transplant recipients and those considering future pregnancy.,Regrettably, some parts of the United States have recently reduced access to the full range of family planning options available, and notably, some of these areas have the worst gestational parent and fetal outcomes observed in the general population. These restrictions threaten outcomes for transplant recipients and preclude health care workers in these areas from providing guideline-based care to their patients and will disproportionally affect transplant recipients from minority racial and ethnic groups and lower socioeconomic groups. In response to the overturning of Roe versus Wade, the American Society of Transplantation issued a statement7 stressing the importance of providing access to routine and emergency contraception as well as safe abortion care when required.,Preconception counseling and comprehensive care are essential for patients with kidney disease, including those undergoing transplant workup and at various stages of post-transplant follow-up. In recognition of patient desires for post-transplant pregnancy and to mitigate the associated risks to the extent possible, best practice statements8 have made specific recommendations regarding appropriate patient counseling and management. Pregnancy should be avoided for at least 1 year post-transplant or 1 year after treatment for rejection. One registry study9 demonstrated additional benefit among those who waited to conceive until at least 2 years post-transplant. All post-transplant pregnancies should be planned and MPA discontinued at least 6 weeks before conception8; it is frequently replaced with azathioprine. Patients who have abnormal allograft function (elevated serum creatinine or significant proteinuria) at conception may face even greater risk of pregnancy and transplant complications.10 Consultation and comanagement with high-risk obstetrics or maternal fetal medicine specialists as part of multidisciplinary teams is recommended. It is essential for transplant recipients to be counseled about and have access to all options to prevent a poorly timed pregnancy.,Despite identification of these risk reduction strategies, as physicians we recognize that there will still be kidney transplant recipients who have medically complicated pregnancies as well as pregnancies that are unplanned. In the TPRI, 30% of all pregnancies among kidney transplant recipients were considered “unplanned” and 8% had exposure to MPA in the first trimester.2 Nearly 30% of kidney recipients experience preeclampsia, and approximately half are diagnosed with hypertensive disorders of pregnancy.2 While acute rejection rates during pregnancy were low (3%), unfortunately 5.4% of kidney recipients experienced graft loss within 2 years postpartum.2 Consistent with trends observed in the general US population,11 pregnancy terminations appear to have declined among kidney transplant recipients12 and represent only 4% of all pregnancy outcomes reported in the TPRI.2 A paucity of research likely leads to gaps in clinical care. In the experience of one of the authors of this viewpoint article as a patient, the risks and benefits, safety, and efficacy of infertility treatments were inadequately discussed. Likewise, physicians may not feel sufficiently informed to discuss alternative routes to parenthood for patients in whom the risk of post-transplant pregnancy is prohibitively high.,There are broader implications to access to reproductive health care beyond routine clinical care. A full range of reproductive health choices, including highly effective contraception, is also necessary for equitable participation in clinical trials. Transplant recipients must demonstrate that they are taking measures to prevent pregnancy and avoid fetal exposure to potentially teratogenic investigational medication to participate in clinical studies. Unequal access to reliable contraception will exacerbate preexisting sex bias and gender imbalance in clinical trials in transplantation and potentially exacerbate disparities in access to novel therapeutics. Depending on the clinical context, maintaining equitable access to reproductive options may include educating patients about fertility preservation, intrauterine insemination or in vitro fertilization as treatments for infertility, and surrogacy as an alternative to a medically complicated pregnancy and is an essential component of comprehensive post-transplant care.,For over 50 years, transplant physicians have supported transplant recipients through successful post-transplant pregnancies. Central to this success has been identifying the right time for pregnancy in the right patient and to provide appropriate therapeutic options when pregnancy is unsafe. As transplant physicians, we are obligated to provide the best care for our patients. We are deeply concerned about the serious risk to patient care in locations where access to reproductive options is now legally restricted or compromised. We urge policy makers to recognize the unique considerations of transplant recipients and believe that failure to provide access to essential and optimal reproductive health care will lead to adverse birth and patient outcomes including allograft loss and death."
25,25,395,37656451,https://www.doi.org/10.2215/CJN.0000000000000311,https://journals.lww.com/,"Historically, there has been poor support for physicians wanting to start a family during their training.1–5 For those pursuing parenthood, challenges include rigorous clinical schedules during pregnancy, limited time off for postpartum recovery, obstacles to breastfeeding, and stigma.4 Furthermore, female physicians experience a higher rate of infertility than the general population, likely because they often defer childbearing because of these barriers.1,In 2022, the Accreditation Council for Graduate Medical Education mandated a minimum 6-week paid medical, parental, and caregiver leave for residents/fellows, and the federal government enacted workplace lactation protections.6,7 The American Society of Nephrology (ASN)’s Data Subcommittee (including four training program directors/associate training program directors) surveyed US nephrology fellows to assess awareness of institutional family planning/caregiver policies (including support for lactating mothers) and whether such policies/program culture influenced fellows' choice of training program.,Questions assessing fellows' experiences with pregnancy and family planning were developed, validated, and disseminated in ASN's Nephrology Fellow Survey (Johns Hopkins IRB00205206). Adult, pediatric, and adult/pediatric fellows (N=954) were surveyed during May 2–23, 2023. Respondents could skip questions they preferred not to answer. Percentages were calculated using question-specific totals. Chi-squared tests for independence evaluated differences between subgroups aggregated by sex, binned age (<30, 30–35, and >35 years), fellowship (pediatric versus adult), and visa status.,Four hundred fifty fellows (400 adult, 41 pediatric, and nine adult/pediatric) participated (47% response). A majority (59%) were international medical graduates, male (56%), either Asian or White (42% each), and non-Hispanic/Latina/Latino (84%), with a median age of 33 years (Table 1). Skip rates ranged between 0.5% (n=1) and 5% (n=11) per question.,Family planning and childcare concerns influenced 27% (n=122) of respondents' training program selection, but only 19% (n=83) had considered taking/had taken parental leave. Forty-nine percent (n=218) indicated awareness of parental leave, lactation, and childcare policies. Of these, 83% (n=171) noted their programs offered 6-week parental/caregiver leave (Accreditation Council for Graduate Medical Education minimum). However, only 56% (n=116) reported availability of lactation facilities and 42% (n=86) reported clinic breaks for lactating mothers, both of which are mandated by federal law.,Of 83 respondents who had considered taking/had taken family leave, most (66%) felt their program's leadership were “very”/“extremely supportive.” However, 17 respondents (21%) perceived microaggressions and seven (8%) explicit bias as a result of considering or taking family leave during fellowship. Subgroup comparisons by sex, binned age, and fellowship were not statistically significant, although more US citizen/permanent resident fellows than fellows training on visas knew their program's pregnancy/family planning policies (53% versus 38%; P = 0.00515) and had considered taking/had taken parental leave (22% versus 11%; P = 0.00826).,Our nationally representative study shows that family planning policies are important to nephrology fellows. Over a quarter of respondents, irrespective of sex, weighed these policies when selecting a training program. Unfortunately, more than half were unaware of their program's policies, including nationally mandated requirements—minimum 6-week paid leave for pregnancy/adoption and break time and secured private spaces for pumping breast milk.6,7,Fellows' ignorance, or their program's failure to ensure fellows are fully informed, of these policies may drive this knowledge deficit. This underscores the responsibility of program leadership to ensure awareness, and enforcement, of family planning, caretaker, and lactation policies. Nevertheless, most respondents who considered taking/had taken parental leave felt that their programs were supportive. Although only a few respondents perceived microaggressions or explicit bias related to their decision to take leave, any such behavior is unacceptable.,This study is potentially limited by recall bias and does not evaluate individual programs' family planning/caregiver policies but rather fellows' awareness of them. Knowledge of state-specific laws mandating ≥6-week leave was not evaluated. Although we only asked about lactation breaks in clinics, federal law applies to all clinical and educational settings. The question skip rate was minimal, but because it was highest for the question about program-specific policies, this strengthens the hypothesis that many respondents were unaware of or could not remember them. Despite these limitations, the study offers key data to inform a national conversation about strategies to enhance awareness of mandated policies and implementation of others, including onsite childcare and insurance coverage for infertility treatments, noted by a minority of respondents. Furthermore, it demonstrates that family-friendly policies are valued by nephrology fellows regardless of sex or age—there were no significant subgroup differences, except for visa status. Respondents on visas were less likely to consider parental leave during training. Because a large proportion of our workforce trains on a visa, this disparity deserves further attention in future studies. Factors, such as program size, are practical barriers to implementation of these benefits. That is why innovative, collective problem solving by the nephrology community is key to creating work environments that attract, support, and sustain the next generation of nephrologists.,In conclusion, family-friendly policies are important to nephrology fellows. It is incumbent on institutional/training program leaders and organizations, including ASN, to close the knowledge gap regarding national parental leave and lactation policies and institute additional policies that support fellows who want to start/grow their family during training."
26,26,397,37778869,https://www.doi.org/10.1016/j.ajt.2023.05.028,https://linkinghub.elsevier.com/,
27,27,400,37314774,https://www.doi.org/10.2215/CJN.0000000000000227,https://journals.lww.com/,"The prevalence of kidney failure continues to increase, with more than 480,000 patients treated with hemodialysis as their KRT modality in the United States in 2020.1 Since its inception, patients receiving maintenance hemodialysis have been plagued by concomitant cardiovascular risk, which remains the major cause of morbidity and mortality, accounting for approximately 40% of deaths.1 Therapies that have proven beneficial to reduce cardiovascular risk among patients without CKD have often not extrapolated to (or been tested in) those receiving maintenance hemodialysis,2 suggesting the need to target alternative or nontraditional risk factors unique to the hemodialysis population.,BP typically declines during the course of a hemodialysis session for most patients,3 with a mean decline of approximately 30 mm Hg.4 For many patients, this is thought to be appropriate and correlates with the removal of excess volume that has accumulated since the prior session. However, some patients experience an excessive BP decline, a phenomenon called intradialytic hypotension. This entity has been challenging to define, which is reflected by the plethora of definitions in the literature. Notwithstanding these issues, several studies have described associations of intradialytic hypotension with adverse outcomes (hypothesized to be primarily mediated through end-organ hypoperfusion), including access thrombosis, myocardial stunning, cerebral atrophy, deterioration in residual kidney function, and mortality.5,Therefore, greater emphasis has been placed on exploring interventions with the potential to prevent intradialytic hypotension. Although several approaches have been tried, many are accompanied by significant downsides,5 and few have been tested in randomized trials that are powered to detect differences in important clinical outcomes.,Cooling of the dialysate has been suggested as a method to promote hemodynamic stability for decades, with hypothesized mechanisms including activation of the sympathetic nervous system and peripheral vasoconstriction.6 Dialysate cooling has enjoyed a resurgence of interest, with smaller studies suggesting benefits in terms of optimizing myocardial perfusion7 and a modest-sized trial suggesting benefits in minimizing cerebral white matter changes, compared with a dialysate temperature of 37°C.8 Dialysate cooling also has favorable characteristics such as its low cost, ease of implementation, and general acceptance as a mostly safe form of intervention—all advantageous for development of large pragmatic trials. However, a deeper dive uncovers multiple nuances that should be considered. These include various cooling options such as fixed, programmed, isothermic dialysis, thermoneutral dialysis, and negative energy dialysis.6 Owing to practical limitations and ease of use, perhaps the most widely advocated approach is cooling to a fixed difference below the measured body temperature, with many studies choosing 0.5°C below that of the predialysis body temperature.,Enter the MyTEMP trial, which was a pragmatic, two-arm, parallel-group, registry-based, open-label, cluster-randomized superiority trial performed from 2017 to 2021 in Ontario, Canada.9 This study enrolled 84 centers and 15,413 patients, representing the largest trial of patients receiving hemodialysis to date. For this, the investigators, the entire study team, and the patients deserve tremendous credit. The intervention group was prescribed a dialysate temperature 0.5°C below body temperature, with a minimum of 35.5°C and a maximum of 36.5°C; the standard group was prescribed a dialysate temperature of 36.5°C. At the end of follow-up, there was no difference in the arms for the primary outcome (composite of cardiovascular death, or hospitalization for myocardial infarction, ischemic stroke, or heart failure; adjusted hazard ratio, 1.00; 96% confidence interval, 0.89 to 1.11). Similar findings were reported for several sensitivity analyses, as well as for the examination of the decline in systolic BP from pre to nadir during the hemodialysis session, while patients in the lower temperature arm were more likely to report feeling uncomfortably cold during dialysis. Given the promising results of physiological studies and preliminary trials that appeared to support cooled dialysate as an intervention to prevent intradialytic hypotension, the nephrology community has been left pondering the reasons underlying the null findings from this trial. In this respect, there are a number of features of the study design and execution that warrant further discussion.,First, one should consider the generalizability of the results to other populations. This trial was performed in Canada, a country with a publicly funded health care system, and included patients with a mean age of 66 years, 8% were Black, and around 59% had diabetes. Interestingly, around 70% dialyzed with a catheter, despite a median dialysis vintage of 3.3 years. There were some minor differences between randomized groups in racial subgroups, some cardiovascular comorbidities, and use of various BP medications, with a slightly larger proportion taking four or more antihypertensives in the standard arm compared with the cooled arm.,The unit of clustering was individual dialysis units, and individual patients were given the option to opt out of the intervention (although all patients were included in the intention-to-treat approaches). Adherence to the dialysate temperature intervention was assessed by random sampling of deidentified patient data (predialysis patient and prescribed dialysate temperature) from 15 treatments from each center on a monthly basis, with different patients sampled each month. These were all taken from Friday or Saturday sessions at the end of each month and also served as the sampling mechanism for the ascertainment of the intradialytic BP measurements (a key secondary outcome). Of note, some reports have suggested that the frequency of intradialytic hypotension is higher as the dialysis week progresses, so the lack of sampling from earlier weekdays seems less likely to have missed hypotensive events.10 However, the proportion of patients who opted out at each center was not reported, and all patients at a given center were eligible for data sampling (even those who opted out of the intervention). Considered in addition to the nonadherence rate of 20% and crossover of some patients, the sampling of approximately 1% of the dialysis sessions introduces the potential for bias in the data collected and the possibility that an effect of the intervention on BP was missed.,The standard group was prescribed a dialysate temperature of 36.5°C, which is lower than that used in many preceding studies.11 If we assume for a moment that cooling the dialysate below 37°C is beneficial, setting the standard temperature of the comparator group at 36.5°C may have already conferred benefits and thus limited the ability to detect additional risk reduction from even further temperature lowering in the intervention group. Indeed, the mean prescribed dialysate temperature of the intervention group was only 0.6°C lower than the standard group, while the lack of data on the delivered dialysate temperature or the effect of the intervention on body temperature also leaves us uncertain if the cooled group was cool enough. Furthermore, as the authors alluded to, the lack of patient-level data precluded the ability to examine the effects of the intervention in hypotensive-prone patients.,The trial was 80% powered to detect a 20% reduction in the primary composite outcome. In some respects, this is a relatively optimistic effect size to hope for. Examination of the lower confidence interval around the results of the primary composite outcome (adjusted hazard ratio, 1.00; 96% confidence interval, 0.89 to 1.11) allows for the possibility of a risk reduction of around 11%. To be adequately powered to detect such an effect would have required a sample size around four times as large as the current trial (or much longer follow-up), which seems beyond reach in the current paradigm. Again, one is left wondering if a smaller effect size than the anticipated 20% reduction truly exists and whether a comparator of 37°C may have helped demonstrate this. Of course, opposing arguments can also be made, including that there is truly no treatment effect or even the possibility that the cooled intervention could result in a 11% higher risk of the composite outcome.,This was an open-label study, meaning that neither patients nor providers were masked to the intervention. This is perhaps the most important limitation of this study, as a lack of blinding has the potential to introduce bias in clinical treatment. For example, providers (and patients) in the standard arm could have paid extra attention to the review of BP medications and dry weight in an effort to minimize the risk of hypotension, thereby diluting any potential changes in BP and adverse sequelae. Furthermore, owing to the pragmatic nature of the trial, the frequency of interventions in response to low BP events or patient symptoms was not recorded. For example, it is possible that providers in the standard arm could have had a lower threshold for intervening, limiting the ability to detect differences in BP changes between groups. Again, assuming that cooled dialysate does lower the risk of the primary outcome, whether the existence of the described bias would be enough to dilute a true treatment effect is likely to remain a matter of much debate.,So, where do we go from here? This well-executed pragmatic trial has answered the question that facility-wide lowering of the dialysate temperature 0.5°C below body temperature versus 36.5°C does not result in a 20% risk reduction in the cardiovascular composite outcome. However, it does not exclude the possibility that a smaller and still clinically meaningful risk reduction may be present (and does not answer the comparison with a standard dialysate temperature of 37°C or with a comparator group with greater separation [e.g., 36.5°C versus 35.5°C]). Future studies may wish to focus on higher-risk subgroups, such as those prone to intradialytic hypotension, and consider innovative approaches to blinding for these interventions. The massive effort for this trial has unquestionably advanced our knowledge in this area but, as always, more needs to (and must) be done."
28,28,401,37782546,https://www.doi.org/10.1681/ASN.0000000000000206,https://journals.lww.com/,"We read with great interest the article recently published in JASN by Hirt-Minkowski et al. exploring the clinical utility of urinary CXCL10 monitoring in kidney transplant recipients.1,In the sumptuous garden of research on noninvasive biomarkers for kidney transplantation, urinary CXCL10 monitoring stands out with good diagnostic accuracy for acute rejection.2 As the first randomized trial, the study by Hirt-Minkowski et al. is aimed to assess the clinical utility of this marker concerning clinical outcome at 1 year after kidney transplantation. Their conclusion was essentially negative: Urinary CXCL10-informed decision to proceed with a kidney biopsy in stable kidney transplant recipients does not improve clinical outcome at 1 year, when considering a complex composite end point (graft loss, rejection, de novo donor-specific anti-human leukocyte antigen antibodies, kidney function, interstitial fibrosis/tubular atrophy, and immunosuppression-related comorbidities).,We have to be very careful in interpreting this study, considering the context of use of noninvasive biomarkers for transplant rejection.,In this study, CXCL10-triggered biopsies in stable kidney transplant recipients showed a significantly higher yield for rejection diagnosis compared with 1-year surveillance biopsies. This confirms that urinary CXCL10 captures subclinical rejection, is useful to prompt informative biopsies, and allows timely adaptation of the immunosuppressive strategy. However, performing biopsies per se does not affect short-term outcomes; it is the therapeutic intervention based on the biopsy results that may have an effect. The lack of standardized intervention after detecting subclinical rejection in this trial potentially undermined the effect on the composite short-term end point. Longer follow-up is also needed to draw conclusions on the potential effect of timely treatment of subclinical rejection.,Next to potential relevance to detect subclinical rejection, noninvasive biomarker monitoring could aid in safely avoiding numerous noninformative surveillance biopsies.2 This clinically relevant context of use, supported by the high negative predictive value of urinary CXCL10, was not evaluated in this study.,Detection of subclinical rejection and avoidance of noninformative biopsies is the proposed context of use for other noninvasive biomarkers, such as donor-derived cell-free DNA, which has already been approved and implemented in several transplant centers in the United States, despite the lack of solid evidence of clinical utility.3,Awaiting longer term follow-up, the study by Hirt-Minkowski et al. demonstrates the importance for future clinical trials of establishing a clear definition of rejection (regarding histological diagnosis and subsequent therapeutic action) and of choosing the most appropriate trial end points to assess the performance of a biomarker considering the prespecified context of use.4"
29,29,414,37934632,https://www.doi.org/10.2215/CJN.0000000000000372,https://journals.lww.com/,"Rhabdomyolysis is a condition characterized by the destruction of skeletal muscle fibers and the release of intracellular contents into the bloodstream. In the United States, approximately 26,000 cases of rhabdomyolysis are reported each year, and the number has been increasing over the past 20 years.1 There are systemic complications associated with rhabdomyolysis, including AKI, which occurs in approximately 50% of cases. Older age and comorbidity are risk factors of rhabdomyolysis-induced AKI, which associates with prolonged hospitalization, high mortality (up to 30%), and increased health care costs.2,A 40-year-old man without medical history presented with fatigue, nausea/vomiting, decreased urine output, and diarrhea for the past week. He denied recent trauma or surgery. His social history included use of tobacco, drinking alcohol on weekends, and no use of illicit drugs. He was working as a painter and reported recent outdoor work under severe heat conditions. On arrival, his BP was 150/90 mm Hg, heart rate 71 bpm, and oxygen saturation 100% at room air, and he was afebrile. His physical examination was unremarkable. Laboratory data included serum sodium of 126 mmol/L, potassium 4.4 mmol/L, high anion gap metabolic acidosis with bicarbonate of 10 mmol/L, blood urea nitrogen 198 mg/dl, serum creatinine 22.5 mg/dl, hypocalcemia at 5.9 mg/dl (iCa 0.82 mmol/L), and hyperphosphatemia at 10.9 mg/dl. Serum creatine kinase (CK) was 23,138 U/L. Urinalysis revealed pH of 6 and large blood. Urine microscopy showed hyaline casts, pigmented granular casts, renal tubular epithelial cells, and few normomorphic erythrocytes. Urine protein-to-creatinine ratio was 1.2 g/g. Serologic/autoimmune workup was negative or normal. On further evaluation, kidney biopsy revealed severe acute tubular injury with myoglobin casts, confirming the diagnosis of rhabdomyolysis-induced AKI. During the first 24 hours, the patient received 6 L of sodium chloride 0.9%, but his kidney function failed to improve and remained oliguric. Standard calcium replacement was given for hypocalcemia. The patient received three daily sessions of hemodialysis with 2.5 mEq/L calcium baths. Subsequently, his kidney function started to improve, but the patient developed severe hypercalcemia with short QT corrected for heart rate on electrocardiogram and was transferred to the intensive care unit due to altered mental status attributed to hypercalcemia-induced posterior reversible encephalopathy syndrome as per neurologic and head CT findings. Two additional daily hemodialysis sessions (2 mEq/L calcium baths) were provided to treat hypercalcemia concomitantly with isotonic intravenous fluids (IVFs) and calcitonin administration. Once hypercalcemia resolved, his mental status returned to baseline, and his kidney function continued to recover until discharge.,The pathophysiology of rhabdomyolysis-induced AKI is multifactorial and involves various mechanisms starting with hypovolemia from fluid sequestration in the injured muscle (Figure 1). Myoglobin is a heme-containing protein in muscle cells that once released from the muscle due to injury is toxic to the renal tubular epithelium and causes tubular injury. Myoglobin also causes vasoconstriction in the kidney microcirculation, resulting in ischemic injury. Furthermore, myoglobin precipitates with Tamm–Horsfall protein in the setting of aciduria to form intratubular casts, which obstruct and further damage the kidneys. Mitochondrial dysfunction and oxidative stress further contribute to kidney injury. Systemic inflammation and proinflammatory cytokines triggered by muscle injury can also exacerbate kidney injury and contribute to AKI progression.3,Rhabdomyolysis can occur due to physical causes, such as polytrauma, prolonged immobilization, or strenuous muscular exercise, as well as nontraumatic causes, including alcohol drinking, use of recreational or prescribed drugs (e.g., statins), infections, severe dehydration, electrolyte abnormalities, myopathies, and autoimmune processes. In the presented case, the cause was likely a combination of viral illness, dehydration, and work-related muscular activity under severe heat conditions. Clinical symptoms, such as myalgia, weakness, and dark urine (myoglobinuria), are indicative of rhabdomyolysis. However, the clinical presentation is variable, and some patients, particularly elderly individuals, present with few signs and symptoms. High-risk parameters include CK levels >15,000 U/L, hypoalbuminemia, metabolic acidosis, and coagulopathy suggestive of disseminated intravascular coagulation.4 Kidney biopsy is not routinely performed to diagnose rhabdomyolysis-induced AKI.,Given the multiorgan involvement of rhabdomyolysis, a multidisciplinary approach should be used for management. The use of isotonic IVF (sodium chloride 0.9% or Ringer lactate) is the cornerstone of treatment to revert hypovolemia, restore renal perfusion, and prevent further kidney damage (Figure 1). Initial IVF rate could range from 200 to 1000 ml/h with a urine output goal of 2–3 ml/kg per hour (i.e., 200 ml/h). IVF should be maintained—albeit not at initial responsive rates—until plasma CK levels are <5000 U/L but promptly stopped if the patient does not exhibit a response in urine output (i.e., remains or becomes oliguric) or develops signs of volume overload. Similar to other conditions that require acute IVF administration, recognition of fluid responsiveness and selection of type of fluid (e.g., balanced solutions) after initial resuscitation are advised. Furthermore, the clinical context of the patient is vital and should be taken into consideration when determining IVF therapy, such as renal perfusion may be further compromised in patients with underlying heart failure that become decompensated with iatrogenic fluid overload.,The use of isotonic IV sodium bicarbonate (i.e., 15 mmol/100 ml) to alkalinize the urine has been suggested when urine pH is <6, CK >30,000 U/L, and if severe metabolic acidosis and hyperkalemia are present. Increasing urine pH may promote the solubility and excretion of myoglobin but could also increase the risk of intratubular calcium-phosphate deposition, and therefore, bicarbonate should be stopped when urine pH is >7.5 Furthermore, careful monitoring of hypocalcemia and hypomagnesemia is recommended when bicarbonate is administered. The use of osmotic diuretics, such as mannitol, could reduce the swelling of skeletal muscles and decrease myoglobin–Tamm–Horsfall protein cast formation. However, it can also result in AKI with high doses (daily dose >200 g or cumulative dose >800 g), particularly in patients with hypovolemia. Current data on the protocol-based use of bicarbonate and mannitol to prevent or manage rhabdomyolysis-induced AKI are insufficient, and therefore, their use is not routinely recommended.6,KRT should be considered for patients with severe AKI and evolving life-threatening complications (hyperkalemia, dyscalcemia, hyperazotemia, volume overload, etc.). The exclusive use of KRT to remove myoglobin from circulation and prevent or mitigate kidney injury lacks solid evidence and is, therefore, not recommended as the standard of care.7 Given that myoglobin has a molecular weight of 17 kDa, its removal from circulation by conventional dialysis is limited. Methods, such as extended hemodialysis using polysulphone high-flux dialyzers, continuous veno-venous hemodialysis using high-cutoff dialyzers, and continous veno-venous hemodiafiltration using high-flux dialyzers, have yielded variable results.8,9 While clearance of myoglobin could be approximately 20 ml/min with sieving coefficients ranging from 0.1 to 0.3 within the first 24 hours of treatment, its effect on clinical outcomes is unknown.7 In the aforementioned case, hemodialysis was provided for solute control, initially due to azotemia, metabolic acidosis, and hyperphosphatemia, and later due to hypercalcemia.,In approximately one third of patients with rhabdomyolysis-induced AKI, a biphasic calcium trajectory pattern occurs. This is characterized by two phases with initial hypocalcemia, followed by hypercalcemia over the course of acute illness.10 The initial phase of hypocalcemia occurs during the oliguric phase of AKI due to extensive calcium deposition in the necrotic muscle. Hypercalcemia later occurs during the polyuric phase of AKI due to massive calcium release from the muscle (remobilization), typically in the context of frequent and aggressive calcium supplementation during the earlier hypocalcemic phase. This sequential dyscalcemia also results in altered levels of parathyroid hormone and vitamin D metabolites. For management, gentle calcium supplementation is recommended for hypocalcemia, keeping in mind that patients are not calcium deficient but hypocalcemic because of calcium sequestration in the muscle. If hypercalcemia occurs, isotonic IVF, calcitonin, and/or KRT should be considered on the basis of severity and clinical manifestations.,In summary, AKI is a common complication of rhabdomyolysis and is associated with high morbidity and mortality. Early recognition is critical for the management of this condition. Isotonic IVF administration is the cornerstone of treatment. Specific high-risk patients (e.g., CK >30,000 U/L, severe metabolic acidosis, hyperkalemia, and urine pH <6) may benefit from urine alkalinization, but this should be performed with caution. The routine use of mannitol is not recommended. The role of KRT should be supportive and according to solute and volume goals, similar to AKI of other etiologies. Removal of myoglobin by convection or other methods is of limited/variable efficacy and unclear clinical benefits. As highlighted in the case, careful electrolyte monitoring is advised during the course of rhabdomyolysis-induced AKI to recognize and manage complications such as biphasic dyscalcemia."
30,30,417,36723176,https://www.doi.org/10.2215/CJN.0000000000000078,https://journals.lww.com/,"Recent advances in machine learning and artificial intelligence (AI) brought unprecedented promises across the fields of medicine, including nephrology. The clinical complexity and challenges in patient management highlight the potential benefit of data-driven, algorithmic approaches in nephrology.1 For example, neural networks and other deep learning methods have been applied, from analyzing kidney biopsy specimens to predicting kidney failures.2,Along with hopes and hype comes the increasing concern that data- and model-based decision making can in fact exacerbate bias and inequity in health care. Researchers have shown that a model-driven prediction of eGFR that has been used for decades could be racially biased by assigning higher eGFR estimates to patients identifying as Black, although uncertainty remains in the biological explanation underlying the race correction.3 Another model implemented in practice, the Kidney Donor Risk Index, assigns unwarrantedly higher predicted risk of kidney graft failure in patients identifying as Black, which can potentially exacerbate inequality in access to organs for transplantation.3,When a single race variable has significant potential to create bias, the likelihood of the presence of bias is much greater in black box AI models that often blindly take in a large number of variables. It is imperative, therefore, that both the developers and end users of AI-based clinical applications understand the ways in which biases arise in data and model outputs. Through this article, we aim to help the readers recognize biases in AI applications and get familiarized with methods to mitigate biases.,Figure 1 illustrates the types of biases that can arise throughout different stages of AI development. At a high level, there is the algorithmic side and human side of biases, as described below.,AI development begins with collecting patient data, which almost always comes from a selected sample among the underlying target population. Skewness in patient sampling can lead to disparate model performance in over- or under-represented subgroups. Differences in outcome ascertainment, such as higher sensitivity or specificity of an event of interest in electronic health data, can be another source of bias. A previous study has shown how the accurate capture of health care cost as the outcome resulted in a model preferentially recommending White patients for additional treatment resources because less money is spent on Black patients compared with White patients with similar levels of morbidity.4 Clinician bias and complex evaluation process have unfavored Black patients, leading to disparity in receiving kidney transplant; data accurately capturing this practice can generate a model that treats Black race as a risk factor for transplant failure, reinforcing the underlying inequity.5,Often unknown to consumers of AI, numerous modeling decisions take place during the course of development. Unlike traditional medicine where publication of study protocols has become a standard practice, convoluted process of model selection, training and testing, and validation is seldom prespecified or communicated, although it has substantial effect on the outcomes. Data quality can be a function of sociodemographic factors if access to care is associated with reliable capture of data. If a group of people have a lot of missing data because of several barriers to health care, AI models will likely underperform for this group and can lead to more harm than benefits if missing data are simply excluded in model training. Similarly, various model updates that take place upon observing subpar performance, missing data treatment methods, and the decision threshold for hemodialysis can affect the performance of models predicting AKI.6,The human side of bias plays a significant role in translation of AI to clinical benefits. Clinician trust and acceptance in AI can be a deciding factor over the actual model performance for the extent of real-world application of AI. Variability in the levels of health literacy and cultural acceptance among patients can lead to missed opportunities of improving patient outcomes through novel technologies. Importantly, the patient-provider relationship that is a product of history, culture, and mutual trust can be modified through deployment of AI in clinical nephrology, which can have unintended consequences such as loss of trust and authority or reduced adherence to medical advice.7,Biases that are algorithmic in nature, that is related to data sampling, model training, and obtaining outputs, can be addressed at least in part through the debiasing methods. In this sense, bias often refers to unwarranted statistical associations between patient attributes of interest and the outcome. Existing algorithmic debiasing methods can be categorized into preprocessing, in-processing, and postprocessing methods.8 Preprocessing methods treat the training data before model fitting to address imbalances in data. An intuitive example is the reweighing method that transforms the training data to achieve balance in groups defined by sensitive attributes of interest such as race or sex. In-processing methods modify how a model learns from data in a way that reduces the influence of a variable in the learning process. As the name implies, postprocessing takes place after a model is fitted and adjusts the outputs in a post hoc manner to address biases. Tools exist publicly to enable people to readily apply these methods in practice.8,Completely unbiased sampling of data is usually unfeasible, so it is the responsibility of both developers and users of AI to evaluate patient representation bias. Comparing the distribution of patient demographics between training data and target population is a good starting point. Implicit bias in patient care, such as partial recommendation of novel treatment options, can be identified through examining electronic health data that reflect the practice patterns. Efforts to increase diversity in data collection and to provide equitable treatment options should accompany the aforementioned activities. In addition, detailed and transparent documentation of the modeling process, including publication of datasets and code, should become a norm in the field. It can also incentivise researchers to perform replication studies and sensitivity analyses that are critical in ascertaining clinical benefits of AI. Finally, patient and provider education is paramount to ensuring unbiased interpretation and utilization of AI.,In conclusion, big data and AI utilization is an inevitable wave in medicine, and nephrology is no exception. Rigorous bias evaluation and mitigation throughout the development and application process can prevent biased AI from adversely affecting patients and health systems, especially those who are underserved. Recent efforts in providing the public with a guideline or playbook for navigating this process is important progress toward achieving fair and equitable utilization of AI.9 The epitome of AI is its ability to stay “live” and continuously learn over time, calling out the need for continuous monitoring and retraining of models to ensure unbiasedness of data and model outputs.10"
31,31,419,37052952,https://www.doi.org/10.1681/ASN.0000000000000131,https://journals.lww.com/,"Recent randomized clinical trials have demonstrated the benefits of modifying the ratio of sodium to potassium in the human diet, not just on blood pressure but also on “hard” outcomes, such as stroke and mortality.1,2 These studies raise the potential for a ceasefire in the decades-long “salt wars” because they show how modest, and palatable, dietary changes can have profound effects on longevity. It is becoming clear that part, if not much, of the effect of a potassium-enriched diet to reduce blood pressure requires the kidney.,During the past 15 years, remarkable progress has also been made in understanding how the distal convoluted tubule (DCT) participates in potassium homeostasis and how aldosterone can be either kaliuretic or sodium-retentive, depending on the underlying stimulus (the “aldosterone paradox”). Work from several groups has identified a renal potassium switch, which involves DCT cells detecting a decrease in plasma [K+] through basolateral channels, secondary changes in intracellular chloride concentration, phosphorylation of specific serine and threonine kinases, and ultimately activation of the thiazide-sensitive NaCl cotransporter (NCC).3 The reverse takes place when potassium intake is high; in this case, NCC is dephosphorylated rapidly, by a complex mechanism involving protein phosphatase 1 and inhibitor 1.4 The relevance of these phenomena to human health and disease is evident because patients with Gitelman and Gordon (familial hyperkalemic hypertension) syndromes, diseases of the DCT, present with hypokalemia and hyperkalemia, respectively. Potassium-induced dephosphorylation also contributes to the natriuresis consequent to high potassium loading.,Yet, the DCT itself does not, for the most part, transport potassium; so, why does it have such a large effect on potassium excretion? It does so through a unique coupling with the connecting tubule and collecting duct, where potassium is secreted in exchange for sodium reabsorption through the epithelial sodium channel (ENaC). Aldosterone is the canonical ENaC-activating hormone, and both stimulates ENaC trafficking to the apical membrane and ENaC proteolytic cleavage to increase conductance. Aldosterone secretion itself is stimulated by a rise in plasma [K+], so hyperkalemia both inhibits NCC and stimulates ENaC. This leads to increased sodium delivery to a connecting tubule primed to secrete potassium. Part of this priming depends on serum and glucocorticoid-induced kinase 1 (SGK1), which particularly promotes ENaC trafficking to the apical membrane.,During the past several years, Pearce and colleagues have introduced a new “twist” on this scenario, one that involves the mammalian (or mechanistic) target of rapamycin (mTOR).5 mTOR can form a complex called mTORC2, which, unlike mTORC1, is NOT sensitive to the immunosuppressive drug rapamycin. In the kidney, mTORC2 seems to be necessary to phosphorylate and, therefore, fully activate SGK1. The Pearce group showed previously that inhibiting mTORC2 with two structurally distinct competitive inhibitors caused a natriuresis and reduced ENaC activity. In addition, they showed that mTORC2 inhibition affected the phosphorylation of SGK1, thereby placing mTORC2 within the aldosterone-SGK1-ENaC signaling pathway.,This view became controversial, however, when Huber and colleagues later generated mice in which mTORC2 activity was reduced, not pharmacologically, but rather by genetically deleting the rapamycin-insensitive companion of mTOR (RICTOR), another component of the mTORC2 complex, along the distal nephron.6 These mice exhibited elevated aldosterone and nearly absent phosphorylated SGK1, but they otherwise appeared normal. When challenged with a high potassium diet, however, profound hyperkalemia developed. Yet, these mice were able to reduce urinary sodium concentration when challenged with a low salt diet, suggesting that ENaC activity was intact, and patch clamp experiments showed low renal outer medullary K+ (ROMK) activity.,Using a “dialectical approach,” where disagreements are solved through rational dialog and experiment, Pearce and colleagues have now created a new mouse model in which RICTOR is deleted along the entire tubule and used this model to resolve the differences.7 Several aspects of the new approach differ from those taken by Huber and colleagues. First, these authors use the Pax8-Cre system, which effectively deletes proteins all along the renal tubule but only when induced by doxycycline, whereas Huber and colleagues used a system that deletes genes along most of the distal nephron constitutively. Second, Pearce and colleagues studied the effects of potassium loading acutely (after 4 hours) and after 48 hours, whereas Huber and colleagues studied the mice at 5 days. Finally, Pearce and colleagues do not recapitulate the dietary salt restriction experiments. Yet, many of the observations are concordant; the mice in both studies were relatively normal at baseline, with only elevated aldosterone indicating a secretory defect. Both models developed profound hyperkalemia after a potassium chloride challenge (in the first article, when combined with low NaCl intake), and both found that the abundance of phosphorylated SGK1 was quite low.,Yet, there were differences in both results and interpretation. Pearce and colleagues showed that ENaC inhibitable sodium excretion was not as large in the knock out (KO) animals as in controls. However, they note correctly that this measure is complicated by the linear structure of kidney tubules, so that upstream and possibly downstream effects complicate interpretation. Hence, they turned to patch clamp electrophysiology to study ENaC directly; although KCl gavage increased ENaC activity in control mice, it did not in the knockout. Interestingly, ROMK activity was not stimulated by KCl gavage in either group.,The results of the acute gavage experiments paint a clear picture of a signaling pathway involving ENaC, but by 48 hours of potassium loading, things are not as straightforward. At this time, although ENaC activity is still reduced, protein kinase C (PKC) and ROMK activity are lower in the KO mice, similar to what Huber reported at 4 days. Synthesizing the current results with previous results from the Pearce laboratory and results from Huber and colleagues, there seems to be an important role for mTORC2 in mediating rapid SGK1 phosphorylation in response to high potassium challenge. This is necessary for ENaC trafficking and the response to potassium and likely explains the clear inability of mice with inactive mTORC2 to tolerate potassium loading. Over the longer term, however, although ENaC activity still requires mTORC2 signaling, defects in PKC and ROMK activation develop, leading to a mixed picture.,The current results reflect rigorous efforts to solve the mysteries of aldosterone and potassium actions on the kidney distal tubule. One is left, however, hoping that this work continues, as several questions remain. First, the baseline phenotype of the RICTOR KO mice does not recapitulate that observed when the mineralocorticoid receptor is deleted, using the same Pax8-Cre system. In that case, basal plasma [K+] is elevated, and there is salt wasting. Furthermore, the abundance of cleaved αENaC is increased in the RICTOR KO mice, whereas both total and cleaved αENaC are reduced substantially in mice lacking mineralocorticoid receptors, a finding recapitulated by SGK1 deletion. Third, although it is clear that SGK1 plays an important role in activating ENaC, especially over the short term, deletion of SGK1 itself leads to a milder phenotype than mineralocorticoid receptor deletion, suggesting that other signaling molecules are important. Finally, and most importantly, it would be very important to know how these mice respond to low salt intake, and whether like the mice generated by Huber and colleagues, they can activate ENaC to prevent sodium loss. Although great progress has been made recently unraveling the aldosterone paradox, there is much more to learn. For example, the effects of mineralocorticoid receptor activation on ENaC might be different when aldosterone secretion is stimulated by potassium or when it is induced by angiotensin II. Nevertheless, it is now clear that mTORC2 plays an important role in signaling to ENaC in the distal tubule."
32,32,422,37394267,https://www.doi.org/10.1016/j.ajt.2023.06.004,https://linkinghub.elsevier.com/,
33,33,423,37733367,https://www.doi.org/10.1681/ASN.0000000000000223,https://journals.lww.com/,"Data from clinical trials or observational studies shared with the purpose of individual patient meta-analyses have transformed kidney disease clinical practice, research, and public health. We, in the CKD Epidemiology Collaboration Clinical Trials (CKD-EPI CTs), have been involved in data sharing through secondary analyses of clinical trials.1 Using these shared resources, we have provided evidence to support validation and use of GFR decline and changes in albuminuria as surrogate end points for CKD progression.2,3 In this perspective, we describe the current state of data sharing and our experience and challenges and propose ways for further improvement.,Over the past 25 years, formalized mechanisms for data sharing have grown exponentially. The National Institutes of Health first developed data and sample repositories, followed by the emergence and subsequent growth of data sharing platforms (DSPs) for industry-sponsored trials.4 In parallel, laws were implemented to protect patient privacy, such as the US Health Insurance Portability and Accountability Act (1996), General Data Protection Regulation for European Union Member States (2018), and Personal Information Protection Law in the People's Republic of China (2021). In follow-up to the International Committee of Medical Journal Editors' statement of our “ethical obligation to share data because trial participants have put themselves at risk,” their 2017 requirement for sharing of individual participant data in order for consideration for publication solidified data sharing as a vital component of all dissemination activities of all trial results.5,Funding for CKD-EPI CTs began as a collaborative agreement with the National Institute of Diabetes and Digestive and Kidney Diseases in 2003 and continued work through periodic support for the purposes of scientific workshops sponsored by the National Kidney Foundation in collaboration with the Food and Drug Administration and European Medicines Agencies. In 2018, through the support of the National Kidney Foundation as our administrative core, we created a standing research group that allowed enhancement of our data coordination systems, data sharing practices, and analytical methods, resulting in further progress in advancing the evidence to support surrogate end points. We now offer our academic investigators who shared data and sponsors with a return in sharing through distribution of preliminary and confidential results, opportunities to be included in Writing Groups for manuscripts, opportunities to contribute in discussion of implications and future of analyses, and opportunities for sponsors to ask data-related questions or for academic collaborators to request permission to conduct ancillary studies using the data. In addition, we post our analytical code on public sites.6,Before 2018, we had 47 randomized controlled trials; all but two were received through agreements with academic collaborators. For most of that era, there were no formal mechanism to request individual patient data; our knowledge of the field, connections to collaborators, and willingness to work beyond available resources were key to our ability to obtain access to these data. Times are different, both regarding the formal avenues to request data and the number of larger trials evaluating CKD progression. Since 2018, we added 299 randomized controlled trials; 16 were received directly from sponsors and seven from DSP that did not require any approval other than application to the DSP. Despite advances in, and enthusiasm for, data sharing, acquiring and analyzing data is often a slower, more complex process. Indeed, for both before and since 2018, we were not able to receive data from approximately one-third of identified studies. Our knowledge, connections, and intense efforts remain critical to our data sharing process.,Table 1 summarizes steps and processes in acquisition and use of data sharing. We highlight selected challenges below.,All solutions must be attentive to the concerns of all stakeholders, such as privacy for patients, use of proprietary information by competitors or unfavorable regulatory decisions for sponsors, loss of control by sponsors and steering committees for results in the public domain, or insufficient time to publish results of investigators. Evidence suggests that even these parties appreciate the benefits of data sharing. In a survey of clinical trial participants, few had strong concerns about the risks of data sharing,7 and sponsors have been strong supporters of our work in CKD-EPI CTs, recognizing that our analyses can help inform the design of their next trials.,Selected proposed solutions: First, data standards, such as the Clinical Data Interchange Standards Consortium, that provide common frameworks and terminology for study data should be required. Second, when data are shared, the sponsors or investigators should provide a comprehensive data package, including protocol, data dictionary, computed treatment effects if the data have been reduced or anonymized, and software code to re-create the main analysis. Third, we support increased transparency of DSP with enhanced computing resources,4 and increased participation from academic investigators and biotech companies. Fourth, we recommend standardized agreements with prespecified choices for the contentious items delineated, limiting the need for negotiation. Fifth, we encourage international collaborations to harmonize data sharing standards and accommodation of more dynamic rule structures for anonymization.8 None of these concerns are trivial. The first two suggestions can be implemented immediately without discussion with other entities. The third and fourth will require cooperation by many stakeholders, including study sponsors, steering committees, representatives from DSPs, and consumers of secondary data. The fifth will require cooperation by such stakeholders together with government representatives led by highly dedicated champions.,Costs are substantial and distributed across several institutions, making it nearly impossible to quantify the total amount spent on data sharing. There are costs for sponsors, funding agencies, or investigators for preparation of the data to be shared along with supporting documents. There are also costs for maintenance of the sponsor-specific platforms or DSPs. The latter are maintained by large organizations, which require expensive human and computing resources. Use of shared data by research groups, such as ours, also requires extensive support for our infrastructure to identify, acquire, clean, and ultimately maintain the data in a secured setting; perform complex analyses across multiple shared studies; and preserve the relationships that are at the heart of our collaboration. Despite the overall high costs, there remains no entity responsible to address questions related to data once shared.,Finally, we suggest that journals appreciate that data sharing requirements cannot be applied to analyses of third-party data. Analyses suggested by reviewers often are not possible given the data structure, availability, or costs. Direct communication between editors and authors may help to prioritize analyses and enhance the resulting publications.,Our close and collaborative nephrology community allowed us to use individual participant data to address critical questions in CKD epidemiology well before the introduction of formalized mechanisms in data sharing. Our success over the past two decades has been because of our dedication to the questions, perseverance, and utmost respect for the participants and investigators who generated the data; our ability to bring together advocacy groups, regulators, sponsors, and academic investigators; and, most importantly, the strong support from our collaborators. These factors might not be sufficient to overcome all of the challenges we face as we enter our third decade. Other research consortia without ample experience, knowledge of the field, and strong connections to collaborators are likely to be even more challenged. Nevertheless, we will continue to advance the use of shared data to provide answers needed to improve the life of patients with kidney disease."
34,34,424,36918385,https://www.doi.org/10.1681/ASN.0000000000000119,https://journals.lww.com/,"The United Nation's 17 Sustainable Development Goals (SDGs) were launched in 2015 aiming for significant progress toward each goal by 2030.1 Many SDGs have direct relevance to kidney health and should concern the kidney community.2 By 2019, some progress had been made.1 However, the coronavirus disease 2019 (COVID-19) pandemic has caused major setbacks risking significant reversal of progress that may also threaten kidney health in the coming years.,The most overarching SDG is SDG 1, to end poverty everywhere.1 By 2018, the proportion of people living in extreme poverty (on <$1.90/d) had declined from 36% in 1990% to 8.6%.3 The pandemic reversed this progress, pushing an additional 93 million people into extreme poverty by 2022,1 through loss of employment and work hours in formal and informal sectors and the lack of social safety nets in many places. Although many governments did extend social protections, over 90% were temporary, leaving many unprotected.1 Poverty affects kidney health by increasing the risk of kidney disease through limiting access to healthy food and lifestyle choices, increasing exposure to poor sanitation and unhealthy environments, greater occupational risks, restricting access to health care, and increasing reliance on potentially harmful traditional remedies.2 Poverty exacerbated the risk of severe acute respiratory syndrome coronavirus 2 infection among people with kidney disease, which is often more prevalent among those with lower incomes. People with lower incomes tend to live in overcrowded conditions, rely on public transportation, and are less able to work from home—making them more susceptible to repeated infections and their consequences. Many families will remain in poverty after the pandemic because of loss of full-time employment (125 million full-time jobs lost), the increase in numbers of young people not engaged in school or training, and the slow economic recovery.1 These realities are interrelated with SDG 8 (decent work for all) and SDG 9 (sustainable and resilient infrastructure and innovation).,Equity was a major casualty of the COVID-19 pandemic. Gender equity (SDG 5) has worsened. Women and girls had increased hours of unpaid work, pay cuts, dropped out of the labor force, experienced more intimate partner violence, and were at higher risk of child marriage because of the pandemic.1 Even women with education and social status, including those in academic medicine, have been disproportionately affected.4 These gender-linked factors negatively affect maternal health, increasing risk of poor pregnancy outcomes with short- and long-term consequences for maternal and child kidney health and reduce health-seeking possibilities for women in some settings.1 SDG 10 aims to reduce all inequalities. During the pandemic, the average Gini index (measure of income inequality within a country) for emerging market and developing countries increased by 6%.5 Disparities are widening between countries. After a drop in 2020, global manufacturing has begun to rebound, but the recovery is skewed toward high-technology industries and high-income countries.1 The rich have gotten richer and the poor have gotten poorer.,Health and well-being (SDG 3) has very obviously been affected by the pandemic. By December 2021, almost six million people were reported to have died of COVID-19; in May 2022, it was estimated that 14.9 million excess deaths occurred directly or indirectly because of the pandemic.1 COVID-19 has amplified inequities in health, with the older people and chronically ill being the most vulnerable and populations in lower resource settings having less access to preventive and therapeutic options.1 People living with end-stage kidney failure have been particularly affected because of their increased risk of infection, severe illness and death, as well as lockdowns and curfews which reduced access to dialysis and/or transplantation. People living with chronic kidney disease likely experienced disruptions in care. Chronic weaknesses in health systems became glaringly evident, with erratic access to testing and personal protective equipment which affected patients and staff in dialysis units, more so in lower compared with higher resource settings.6,Stalled or reversed progress in other SDGs will affect kidney health over the coming years. Global (mal)nutrition (SDG 2) has escalated, with an additional 150 million people facing hunger.1 Food insecurity and obesity both increased among children associated with lockdowns and school closures. Over 370 million children missed school meals during the pandemic, for many their only source of a balanced meal.7 Nutrition is key for healthy pregnancies and healthy lifestyles to reduce kidney disease risk and in the management of advanced kidney disease to delay progression and manage complications. Gains were lost in improving access to quality education (SDG 4), with an additional 10% of children between grades 1 and 8 falling below minimum reading proficiency levels in 2020.1 As of January 2022, over 600 million students remained affected by school closures.7 Loss of education not only affects future (kidney) health and health literacy but will affect future earning potential and family structure.1 Billions of people worldwide still lack access to safe water and sanitation (SDG 6), which meant fewer opportunities to maintain (hand) hygiene in homes, schools, and health facilities and persistent risk of infections and acute kidney injury.1,The health workforce has been significantly affected by the pandemic. By May 2021, between 115,000 and 180,000 health care workers had likely died of COVID-19.8 Many more have been infected, some in the line of duty, especially where personal protective equipment and vaccines access were suboptimal.6 Physical and moral distress have been high among health workers, in part because of work circumstances but also because of pressures imposed by family needs, fear of infecting family members, overwork in compensating for absences of colleagues due to illness, etc.,The pandemic has had a devastating effect on many lives and livelihoods, but it can also become a catalyst for change. We must develop strategies to build back better on many levels. The world must indeed focus on prevention and treatment of infections, but the pandemic has highlighted the great cost of the decades of having overlooked the needs and vulnerabilities of people living with noncommunicable diseases. Glaring social and structural inequities exposed by COVID-19, coinciding with the Black Lives Matters and Decolonizing Global Health movements, have led to calls for reckoning and change in national and international policies.9 These calls question the meaning of intellectual property when lives are urgently at stake, the benefits and harms imposed by international trade agreements and the benefits and harms of nationalism.9 They highlight the need to urgently address social injustice and achieve social transformation, to change education (medical) curricula, and to be conscious of and curb explicit and implicit bias in life and work—especially within health systems—and have initiated debate around the ethics of moral responsibility, reparation, triage, rationing, and the need for global solidarity.9 A new sense of urgency has arisen around climate change (SDG 13), with the recognition that cooperation and innovation are required, globally, to address this threat. Healthier environments could reduce global deaths by 23%.10 Sustainability and resilience must be at the core of strategies to curb climate change, foster recovery, and initiate rehabilitation of the environment. SDG 16 and 17 emphasize the need for “peace, justice, and strong institutions”1 and the need for partnerships to achieve the SDGs. These are the key goals to operationalize the achievement of all the others.,The tragic invasion of Ukraine by Russia so close on the heels of the pandemic risks further destabilizing the global food supply, oil markets, and economic stability which will disproportionately affect vulnerable and marginalized populations worldwide. People with kidney disease and especially those with kidney failure living on dialysis are particularly vulnerable in war, given disruptions in access to dialysis with destruction of infrastructure, forced displacement, food and medication insecurities, and patient frailty. As we again observe COVID numbers surging in China, many questions arise regarding the effect this will have on China itself, the risks of potential new variants, and the ripple effects on the global economy and health indicators. COVID-19 has illustrated that good health cannot be achieved within the silo of the health system and that health can be threatened by factors independent of the health system. The community of nephrologists and our allies must leverage this moment to regain momentum toward progress on SDGs (Table 1). We owe it to our patients and our communities, and the world will be better for it."
35,35,427,36550624,https://www.doi.org/10.1097/TP.0000000000004476,https://journals.lww.com/,"The 2022 Joint Annual Congress of the International Liver Transplantation Society (ILTS), European Liver and Intestine Transplant Association, and Liver Intensive Care Group of Europe was held in Istanbul, Turkey, from May 4 to 7, 2022. The meeting was held in a hybrid format with excellent in-person and online engagement. This was the first time in 2 y that the ILTS community was able to meet in-person, enabling rich networking opportunities and extensive discussion. The ILTS Congress was attended by 1123 delegates from 61 countries (648 participants on-site and 475 online; Figure 1).,Out of a total of 465 abstracts, 9.9% fell into the basic/translational science category, representing a 1.7% increase in basic/translational abstracts compared with the previous year (Table 1). This is reflective of the increasing enthusiasm for basic science research to elucidate a mechanistic understanding of transplant processes among the ILTS membership. Beyond the original research presented orally at 2 Basic and Translational Science abstract sessions, the Basic and Translational Science Research Committee was also responsible for the organization of a well-received workshop on multiomics and big data in liver transplantation organized by the Basic and Translational Science Research Committee. In addition, a joint workshop was held with the precision medicine and biomarkers special interest groups from ILTS. Our committee also organized an especially well-received and fully booked pre-Congress “hands on” workshop on machine perfusion preservation in conjunction with local transplant leaders in Istanbul and members of the Toronto transplant research team.,Here, we summarize key basic and translational science abstracts categorized into 4 main themes, including (1) ischemia–reperfusion injury (IRI) and machine perfusion, (2) transplant oncology, (3) novel biomarkers, and (4) liver immunobiology (Figure 2). These abstracts used animal models and profiling of patient samples using multiomics approaches and laboratory techniques, including single-cell sequencing and imaging mass cytometry. We will attempt to place these results in the context of the current literature, thereby demonstrating how these studies promise to advance the field.,IRI is a ubiquitous and unavoidable consequence in all solid organ transplantation with a special impact on liver transplantation (LT). In fact, IRI represents the sum of damages that accumulates at each stage of the transplantation process, from the onset of the hypoxia/ischemia phase in the donor (eg, aortic cross-clamp) to reperfusion in the recipient. The degree of IRI directly affects posttransplant organ function and the rate/severity of post-LT complications.1 Notably, the more extended criteria characteristics the graft has (eg, steatosis, advanced age, ischemia time), the more susceptible it will be to IRI. In this year’s ILTS Congress, different studies focused their attention on IRI evaluation and its modulation through both machine perfusion and pharmacological treatments.,Cell culture, liver resection, in vivo IRI (liver hilum clamping), and transplantation models are all used for the evaluation of the IRI and to provide deeper insight into IRI machinery. An activated inflammatory cascade and a reduced energetic pool secondary to IRI are commonly identified as key events that lead to lower results after LT using steatotic grafts. McDaniels et al (J. McDaniels, MDD, unpublished data) investigated the particular susceptibility of steatotic liver grafts to IRI. They used single-cell transcriptomic analyses on steatotic and normal human hepatocytes and were able to confirm a decrease in coagulation factors, increase in proinflammatory profiles, and activation of peroxidation during IRI in fatty liver hepatocytes, confirming in vivo experimental data. In addition, single-cell transcriptomic analysis was able to identify pathways that may represent targets for IRI therapeutic intervention and factors that could reduce regenerative capacity post-IRI.2 Bardhi et al3 explored the role of micro-RNAs (miRNAs) in the regulation of donor organ damage on human liver biopsies. They further emphasized the upregulation and downregulation of these miRNAs and found specific miRNAs that could serve as potential therapeutic targets,3 a promising approach to reduce early graft dysfunction.4,Machine perfusion preservation represents one of the most promising and translational technology in the field of IRI prevention and treatment from a clinical and research perspective. Although hypothermic oxygenated machine perfusion (HOPE) achieved significant results in the clinical setting, normothermic machine perfusion (NMP) reserves the opportunity to maintain liver graft physiology and assess liver homeostatic functions.5 At this year’s Congress, a higher number of abstracts were focused on the application of NMP technology (Table 2). Several groups explored the possibility of NMP to prolong ex situ preservation. Building on these results, Lau et al used a modified commercial NMP system (with dialysis filter) to surgically split human livers without interrupting perfusion and preserve them for several days. As a result, the grafts were able to meet the viability criteria for as long as 13 d.6 The same group showed that, after 72 h NMP, biliary reepithelialization could occur, and biliary viability criteria could be met even by grafts that did not meet them at the beginning of NMP.7 As long as NMP can be prolonged, more opportunities for ex situ organ treatment could be achieved. Despite these promising prospects, a more thorough evaluation of the mechanisms of NMP is needed to better understand these findings and fully exploit the potential for NMP preservation, treatment, and reconditioning. In a metabolomics study of an NMP rat model, Lonati et al8 demonstrated that even in the presence of physiological oxygen supply, numerous changes in energy, glucose, mitochondrial and lipid metabolism, and redox occur during NMP. These findings are consistent with previous observations on the regulation of metabolic pathways in early allograft dysfunction (EAD) and phospholipid turnover.9 In an effort to provide a more physiological environment during ex situ perfusion, Wu et al used normothermic venoarteriovenous cross-circulation between a discarded human donor liver and an immunosuppressed, complement-deprived pig to maintain normothermic perfusion. They successfully preserved biliary integrity and gross architecture for 24 h, but clinical application is still far off.10,Although prolongation of ex situ perfusion was one of the most attractive perspectives that emerged from the Congress, a deeper understanding of the exact biological mechanisms elicited during machine perfusion is essential to further progress. To achieve this goal, optimization and standardization of ex situ protocols (preclinical setting) together with more homogeneous graft quality (clinical setting) are key steps. In addition, taking advantage of the methodology of IRI and regeneration studies, the integration of results obtained by different models and advanced laboratory technologies is of paramount importance to achieving this aim.,Normothermic regional perfusion, an in situ perfusion approach, is widely adopted (in combination or not with other end-ischemic machine perfusion modalities) to improve donation after circulatory death outcomes. Panconesi et al directly compared normothermic regional perfusion and dual HOPE in a donation after circulatory death setting. In a rat model, the authors found lower mitochondrial damage and inflammation in the dual HOPE group as well as higher survival. The implementation of static cold storage and machine perfusion preservation solutions is needed to improve these results further. In a rat and pig model, adding polyethylene glycol and glutathione to the standard solutions during cold preservation (both static and dynamic) allows for better preservation of both lean and steatotic grafts.11,12,Besides machine perfusion, several strategies are being investigated to prevent and mitigate IRI in the LT setting. Concomitantly, multiomics approaches have been applied to elucidate the effect of different compounds on IRI.3 As a result, the search for new therapeutic approaches to mitigate IRI led to a deeper understanding of its mechanisms. Liu et al used a small-molecule macrophage migration inhibitory factor inhibitor in a rat LT model to protect liver donation after circulatory death grafts from IRI damage. Through the administration of this compound, they were able to downregulate IRI in the early reperfusion phase, thus contributing to supporting the role of migration inhibitory factor as a proinflammatory mediator in liver reperfusion.2,13 Duarte et al further investigated the role of inflammatory modulation. They used a mouse IRI model (partial hilum clamping) to evaluate the role of tryptophan immunometabolism as a possible anti-inflammatory/immunosuppressive target. By administering PEGylate-indoleamine 2,3-dioxygenase 1, they showed protection against IRI by reprogramming tryptophan immunometabolism.14 Ferroptosis was shown to be a further promising target for IRI modulation by Rokop et al.15 Indeed, in a mouse model of in situ IRI, ferroptosis was upregulated in the fatty liver group together with lipid peroxidation. Mitochondrial damage is closely related to ferroptosis, inflammation, and lipid peroxidation, whereas the degree of mitochondrial damage was related to liver survival after reperfusion in grafts subjected to machine perfusion.16 Histone deacetylase inhibition was shown by Samuvel et al17 as a protective treatment against IRI and mitochondrial damage. Histone deacetylase acts at an epigenetic level by regulating antioxidant and iNOS expression as well as suppressing NFk-B activation.18,With the complexity of LT mobilizing multiple physiological and cellular processes, there remains a sustained focus on ensuring optimal graft usage. This combined with the organ shortage fortifies the need for the inquiry into biomarkers (Table 3) that can predict and elucidate ideal graft-related outcomes.,Galectin-3 (Gal3) exerts various biological functions, including regulation of inflammation, angiogenesis, collagen synthesis, and cell death.21 Yoeli et al determined Gal3 to be predictive of EAD. Using citrated plasma from LT recipients 1 d after LT, they measured Gal3 in relation to outcomes surrounding EAD, intensive care unit length of stay, and posttransplant dialysis, finding a correlation between Gal3 and more prolonged intensive care unit stay. When combined with alcohol-related cirrhosis and being a non-Hispanic White, Gal3 was found to be highly predictive of EAD.22 The same group found Gal3 to be predictive of 30-d rejection post-LT. In this study, they found a significant association between transplant Gal3 and 30-d acute cellular rejection.23 Gal3 shows promise in being a predictive biomarker for outcomes posttransplant outcomes.,Exploring the pathways leading to disease as well as treatment continues to be of great interest in transplant research this year. Various researchers explored paradigms of liver disease from those involving liver scarring to biliary complications.,Despite recent advances in antiviral therapy, progressive liver fibrosis is a ubiquitous issue after transplantation for patients with recurrent hepatitis C.24 Fibrotic livers are unable to effectively regenerate. Nguyen-Lefebvre et al found that the regenerating fibrotic liver used different pathways different from those in normal regeneration, with regenerating fibrotic livers exhibiting reduced fibrosis when compared with control fibrotic livers. These data suggest that through the regenerative process, fibrotic liver tissue can eventually be replaced by healthy liver tissue.4,25 This cutting-edge research suggests a new avenue of research that can limit the progression of graft fibrosis.,Transplant oncology is a key topic of research within the LT landscape, given that 35% to 40% of LT recipients worldwide currently undergo transplants for hepatocellular carcinoma (HCC). Abstracts in this category were mainly in the clinical category, but there were a few basic and translational abstracts also in transplant oncology. Understanding the biological mechanisms underlying response to bridging therapy or recurrence posttransplant is critical to optimizing posttransplant outcomes.,HCC recurrence remains a notable issue within the LT paradigm. Vergeist et al assessed the serum protein N-glycan in patients with HCC recurrence compared with those without. Through this analysis, they developed a composite serum biomarker based on the increased presence of triantennary and fucosylated glycans and the decreased presence of undergalactosylated glycans in patients with HCC recurrence. This panel has the potential to advise allocation in transplant candidates with HCC.26,Ding et al explored fatty acid desaturase 1 (FADS1) and polyunsaturated fatty acids and their role in HCC suppression. Within mouse models, they found that in liver-specific FADS1 knockout mice, there was delayed tumor formation as well as a noted decrease in tumor mass alongside upregulation of CD8+ T cells.27 Given that a fatty liver-related oncogene drives FADS1, this work highlights the role of polyunsaturated fatty acids in posttransplant HCC recurrence.,Zhe et al explored how HCC cells may protect themselves from the immune system. They examined the role of ferritin heavy chain 1 (FTH1) in tumor-derived exosomes as a protective mechanism against CD8+ T cell–associated ferroptosis, in a series of elegant in vitro and in vivo experiments. Uptake of FTH1 by tumor-derived exosomes led to impaired cytotoxic CD8+ T-cell function. This was reversible with a ferroptosis inhibitor. Additionally, they discovered that FTH1 expression was upregulated in HCC tissues and negatively correlated with clinicopathological scores and overall survival.28,With the evolution of diagnostic and laboratory technologies, the comprehension of the biological and immunological functions of the liver can be improved. In this setting, liver regeneration represents unique crosstalk between biological and immunological mechanisms that could take place within a human being. Indeed, to overcome a liver injury, several adaptive mechanisms are elicited at local and systemic levels. The net balance between cell death, repair, and regeneration plays a key role in organ fate. Therefore, the understanding of these events could help to not only improve the knowledge of liver immunobiology but also improve the comprehension of clinical issues, such as IRI and rejection. Abu Rmilah et al,29 building from research by Wuestefeld et al30 that found MKK4 silencing increased the regenerative capacity of hepatocytes in mouse models, explored a novel proregenerative drug in a model of liver failure after resection. This randomized controlled preclinical study was conducted on pigs and found that the MKK4 inhibitor was able to increase the regeneration index compared with the placebo. These data have the potential to pave the way for larger clinical studies within the landscape of postresection liver failure.,Alpha-1 antitrypsin (AAT) is considered a molecule with potential antiapoptotic and proregenerative effects after reoxygenation. For these reasons, Nguyen-Lefebvre et al explored AAT as a potential therapeutic agent to increase cell engraftment in the livers after 70% hepatectomy and concomitant hepatocyte transplantation. They found that AAT and hepatocyte coadministration resulted in improved cell engraftment. Shi et al31 further pushed this concept by exploring AAT as a new potential agent to protect against ischemic cholangiopathy. Indeed, post-LT biliary complications remain the Achilles’ heel of LT and ischemic cholangiopathy is a significant threat to the quality of life posttransplant.32 In their study, the authors used human cholangiocyte organoids exposed to hypoxia and reoxygenation to assess apoptosis and cell proliferation. They determined AAT to be an effective inhibitor of hypoxia and apoptosis triggered by reoxygenation, as well as a stimulator of cholangiocyte proliferation. Interestingly, AAT was previously reported to be a common albeit underdiagnosed cause of liver disease in LT patients. This study offers another piece to the puzzle underlying genetic causes of liver disease and potential therapeutic options.33,In conclusion, the quality and quantity of basic and translational research representation have continued to grow at the ILTS meeting. The uptake of such research that offers mechanistic insights into transplantation is critical to improving patient outcomes. Organ demand continues to exceed supply, which is when advances in organ perfusion and living donor LT are essential to expanding the donor pool. As nonalcoholic steatohepatitis and HCC continue to grow as indications for LT, new considerations emerge. Therefore, the science must evolve along with these changes in patient profiles over time. We foresee growth in biomarker research in these areas with evolving omics technologies. Newer techniques, including single-cell sequencing and organ perfusion models to improve organ function, will drive this innovation. Xenotransplantation represents an interesting opportunity to meet organ demand, and we anticipate that this research will have a growing presence at future ILTS conferences. The Basic and Translational Research Committee fully committed itself to fostering and promoting the dissemination of basic and translational research in LT during the annual congress of the International Society of Liver Transplantation, not only with the organization of sessions and scientific presentations but also through publications of what has been presented in Congress as this current summary. In addition, the Committee is dedicated to the development of new projects, stimulating interaction between researchers throughout the year through webinars and seminars."
36,36,429,36586559,https://www.doi.org/10.1053/j.ajkd.2022.11.008,https://linkinghub.elsevier.com/,"Multiple myeloma frequently impacts the kidneys, with up to half of patients presenting with reduced kidney function at diagnosis.1, 2, 3, 4 The most common cause of kidney damage is light chain cast nephropathy,5,6 and Bence Jones proteinuria is present in 64%-96% of cases at diagnosis.7, 8, 9 Nephrologists must be keenly aware of multiple myeloma as a potential diagnosis in cases of undifferentiated kidney dysfunction. Because of this, some employ a commonly taught “clinical pearl” assessing for discrepancy between the urinary protein-creatinine ratio (UPCR) and urinary albumin-creatinine ratio (UACR) as a surrogate screen for multiple myeloma. The theory behind this is that light chains in the urine will lead to discordance between the UPCR and the UACR, since overall protein levels will increase but albumin levels will not; however, other causes of non-albumin proteinuria are also known to occur (eg, tubular proteinuria). It remains unknown whether the absolute difference between UPCR and UACR, which we will refer to as the urine protein-to-albumin gap, is useful as a screen for multiple myeloma.,We conducted a population-level retrospective cohort study of residents in Ontario, Canada, in 2009-2021 to examine the association between urine protein-to-albumin gap and subsequent diagnosis of multiple myeloma using linked databases held at ICES. ICES is an independent, not-for-profit research institute whose legal status under Ontario’s health information privacy law allows it to collect and analyze health care and demographic data, without consent, for health system evaluation and improvement. ICES captures data on all Ontario residents who undergo a health care encounter, including health care visits, laboratory tests, hospitalizations, and vital statistics. These datasets were linked using unique encoded identifiers and analyzed at ICES (Table S1). The use of data in this project is authorized under section 45 of Ontario’s Personal Health Information Protection Act and does not require review by a research ethics board. All Ontario residents aged 18-105 years with same-day quantifiable measurements of UPCR and UACR and without a prior history of multiple myeloma or monoclonal gammopathy from April 1, 2009, through March 31, 2021 were included (Fig S1). Individuals were categorized by quartile of urine protein-to-albumin gap and stratified at a UPCR threshold of 50 mg/mmol, the KDIGO definition of severely increased proteinuria.10 Multivariable time-to-event models measured the association between urine protein-to-albumin gap and subsequent diagnosis of multiple myeloma. Full methods are in Item S1.,Baseline characteristics are given in Table S2. Mean age was 63 ± 18 (SD) years and 49% of individuals were female. Mean serum creatinine and estimated glomerular filtration rate (eGFR) were 1.28 ± 0.96 mg/dL and 71 ± 32 mL/min/1.73 m2, respectively. Median UPCR and UACR were 27 (IQR, 12-58) mg/mmol and 4 (IQR, 1-19) mg/mmol, respectively. Among 28,231 eligible individuals, 116 were diagnosed with multiple myeloma (0.4%) a median of 31 days from UACR and UPCR measurement. Among the overall population, with each successive quartile of urine protein-to-albumin gap, myeloma incidence rate (Table 1) and risk (Fig 1A) were progressively greater. We found evidence of effect modification of the 50 mg/mmol UPCR threshold on the association between urine protein-to-albumin gap quartiles and multiple myeloma (P = 0.04). Among individuals with UPCR >50 mg/mmol, the association between successive quartile of urine protein-to-albumin gap and subsequent myeloma diagnosis was strengthened (Fig 1B). In contrast, among those with UPCR ≤50 mg/mmol, there was no significant association (Fig 1C), though potentially this relates to the lower number of cases poststratification. As this population-based study was based on administrative health care data, limitations included an inability to determine the reason a provider ordered same-day UACR and UPCR measurements (which may impact the pretest probability for multiple myeloma), and a lack of standardization of albuminuria and proteinuria measurements across the many laboratories involved (which may have impacted performance metrics).,In summary, these results show that among individuals with UPCR >50 mg/mmol, the risk for myeloma was significantly higher for urine protein-to-albumin gap greater than ˜50 mg/mmol (˜4-fold higher risk), particularly when this gap exceeded ˜100 mg/mmol (˜11-fold higher risk). However, when these thresholds are used as screening cutoffs for multiple myeloma, their performance is modest. For UPCR >50 mg/mmol with urine protein-to-albumin gap >50 mg/mmol, sensitivity was 85%; specificity, 43%; positive likelihood ratio, 1.49; and negative likelihood ratio, 0.35. For UPCR >50 mg/mmol and urine protein-to-albumin gap >100 mg/mmol, sensitivity was 61%; specificity, 72%; positive likelihood ratio, 2.16; and negative likelihood ratio, 0.55. These results may help guide clinicians who use UPCR-UACR discrepancy as a screen for potential multiple myeloma in cases of undifferentiated kidney dysfunction. In turn, this may provide information to guide when further testing to evaluate for multiple myeloma is imperative.,Download : Download Acrobat PDF file (332KB)Supplementary File (PDF). ,Figure S1; Item S1; Tables S1-S2."
37,37,430,37870881,https://www.doi.org/10.1097/TP.0000000000004733,https://journals.lww.com/,"The field of oncology has been at the forefront in medicine, being the first to use disease mechanisms to guide clinical trial design, personalizing therapeutics to tumor biology rather than using a “one-size-fits-all” method.1 Creemers et al from the Radboud University in Nijmegen, The Netherlands, added to this tradition with their recent work published in Nature Communications.2 In their work using in silico models, they address and plan for some of the most critical flaws experienced when using a uniform approach in immuno-oncology drug trials. Immunotherapies often demonstrate unique toxicity profiles, response patterns, and survival kinetics that ultimately violate the proportional hazards assumption, which is common for standard statistical analyses of trial results,3 because they result in delayed clinical effects and long-term overall survival. Planning a trial under these misassumptions can result in erroneous power calculations and follow-up paradigms that are too short of demonstrating efficacy. By first exploring clinical trial parameters (survival patterns, sample size, interim analyses, etc) with in silico immunotherapy trialsmodeled on biological mechanisms and assumed interactions between tumor and immune cells, Creemers et al have demonstrated a novel way to scrutinize trial designs and strategize appropriately for subsequent trial implementation.,Ordinary differential equation models have historically been used to model complex biochemical regulatory networks.4 However, Creemers et al used ordinary differential equation modeling to simulate virtual patients that would experience various iterations of immunotherapy trials based on different mechanisms of action.2 Using 3 different mechanistic paradigms, models were fit to existing data sets, and they showed qualitatively similar forecasts—the speed for tumor growth was as expected and therapeutic intervention was required to rid the tumor. Importantly, because the authors modeled different tumor–immune cell dynamics, investigators can now tailor their in silico trials to the model with the most applicable mode of action, thus expanding the potential impact. With mechanism-based in silico trials, it becomes possible to predict response patterns, survival kinetics, and trial outcomes more accurately for novel therapeutics, leading to increasingly robust clinical trials. These initial validation studies are extremely promising and provide hope for refining the clinical trial design to increase the potential for success.,Although these new methods show promise in immunotherapy trial design, using this model for the transplant population may not be straightforward. Several fundamental differences between designing drugs for transplant patients compared with patients with advanced cancer complicate this translation. The foremost difference is that therapies designed to fight cancer objectively work to stop growth and/or kill cancer cells, whereas therapies used in transplantation require actions to keep cells/organs alive. Although it may only take a single mechanism to kill a cell, there may be hundreds or thousands of mechanisms to keep 1 alive. Therefore, accurately modeling such a complex network is likely a larger, more complicated endeavor. Graft loss is a multifactorial process, reliant on the complexities of host immunologic factors, organ-specific regeneration, and infectious susceptibility, each of which has an intricate array of pathways, molecules, and processes that may have nonlinear relationships. Therefore, the impact of a single mechanism may not be dominant in a model. Additionally, the timing of events for proposed mechanisms may be more distal relative to the context of cancer, providing opportunities for alternative pathways for disease progression and rendering the model design more challenging.,Clearly, better and more thoughtful trial design in transplantation is needed. In particular, the search for surrogate end points that can accurately predict long-term outcomes has not been fruitful.5 Given the low event rate and the necessity to address long-term outcomes, the approach communicated by Creemers et al is of particular importance. However, the complex immunology of transplantation and the need to predict longer-term outcomes will require unique mathematic algorithms. Characterizing mechanisms that lead and predict graft loss increase vulnerability to infection, or designing for minimizing immunosuppression would still be extremely beneficial and move us toward this kind of modeling. Furthermore, transplant would greatly benefit from clinical trial design methods that use time-to-event models that leverage more detailed outcome data that are salient to evaluate the potential of interventions, as frequently used in cancer trials. Identifying the survival kinetics unique to therapeutics used in transplant patients may provide critical insights into how current trial design does or does not meet fundamental assumptions such as proportional hazards. In the case of the latter scenario, mechanism-based modeling that can be used to assist in clinical trial design could significantly evolve the evaluation of transplantation therapeutics—a much-needed advancement for our patients.,In summary, the work by Creemers et al2 offers a unique combination of mathematics and molecular biology to solve one of the most pressing problems in clinical trial design with particular relevance for transplantation."
38,38,431,37230254,https://www.doi.org/10.1016/j.ajt.2023.05.019,https://linkinghub.elsevier.com/,
39,39,439,37939208,https://www.doi.org/10.2215/CJN.0000000000000245,https://journals.lww.com/,"In recognizing race as a population-level social construct that is an inconsistently measured and unordered variable with no direct relation to medical conditions or physiology, the National Kidney Foundation and American Society of Nephrology recommended using updated 2021 Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) equations.1 The equations were developed based on serum creatinine or the combination of creatinine and cystatin C without inclusion of coefficients for race.2 As it modifies eGFR in all races or ethnicities, a shift to the race-free 2021 CKD-EPI creatinine equation may have significant ramifications regarding future epidemiology of CKD. The effect of the 2021 CKD-EPI creatinine equation on projected CKD prevalence across race and ethnicity and future CKD diagnosis remains unclear.,Data from the 2013 to 2018 National Health and Nutrition Examination Survey (NHANES) were used to provide an updated, population-based estimate for the prevalence of CKD in the general adult (18 years or older) population.3 eGFR was calculated with both the 2009 and 2021 CKD-EPI creatinine equations. CKD was defined as reduced eGFR of <60 ml/min per 1.73 m2 or urinary albumin-to-creatinine ratio of >30 mg/g.4 Using NHANES data from 2013 to 2018, logistic regression models were developed adjusting for age, sex, race/ethnicity, and survey years as independent variables and CKD as a dependent variable. Using predictions from these logistic regression models, the prevalence of CKD was estimated in each of 40 groups of sex (male and female), age group (18–44, 45–64, 65–79, and 80 years or older), race, and ethnicity (Asian, Black, Hispanic, White, and other). Following, the prevalence estimate of CKD was combined with US census projections of population counts for years 2025–2060 to project the number of persons with prevalent CKD through 2060.5,On the basis of the 2009 CKD-EPI equation, 44.5 million (16.5% of the US adult population) are projected to have CKD by 2025, and the number of individuals with CKD will be 62.3 million (19.2% of the US adult population) by 2060. According to the 2021 CKD-EPI creatinine equation, it is estimated that 41.8 million individuals (15.5% of the US adult population) will have CKD by 2025, which is 2.7 million fewer than the 2009 CKD-EPI estimates. This number is projected to be 59.0 million (18.2% of the US adult population) by 2060, which is 3.3 million fewer than the 2009 CKD-EPI estimates.,Figure 1 presents the prevalence of CKD by race and ethnicity with the 2009 and 2021 CKD-EPI creatinine equations. Implementation of the 2021 CKD-EPI creatinine equation increases the projected prevalence of CKD among the Black adults and reduces the projected prevalence of CKD among White, Asian, other, and Hispanic adults in a decreasing fashion. On the basis of the 2021 CKD-EPI creatinine equation, the Black population will have the highest prevalence of CKD in 2025 and 2060 (2025 prevalence: 19.6% and 2060 prevalence: 23.4%), followed by the White population (2025 prevalence: 15.8% and 2060 prevalence: 18.2%). With the 2021 CKD-EPI creatinine equation, the highest percentage increase in prevalence of CKD from 2025 to 2060 would occur in the Hispanic (+29%), Asian (+23%), and Black (+20%) populations, respectively.,The race-free 2021 CKD-EPI creatinine equation reduces algorithmic bias, leading to a lower eGFR in Black individuals and higher eGFR in non-Black individuals compared with the 2009 CKD-EPI creatinine equation. Accordingly, the 2021 CKD-EPI creatinine equation for estimating GFR will affect future trends in CKD prevalence. In this study, the 2021 CKD-EPI creatinine equation projected an overall higher prevalence in CKD in all groups; however, when compared with the 2009 CKD-EPI equation, the 2021 CKD-EPI creatinine equation results in higher projected prevalence of CKD among Black individuals to the year 2060, with a lower projected relative prevalence of CKD among White, Asian, other, and Hispanic populations.,The race-free 2021 CKD-EPI equation, which addresses the concerns regarding inclusion of coefficients that consider race in GFR estimation,1 also has implications for revising our population prevalence estimates and thus projections of future CKD, including prevalence across all race and ethnic groups. The results of this study have implications on CKD detection and staging and suggest that the use of the 2021 CKD-EPI creatinine equation might affect opportunities for kidney disease and cardiovascular prevention and care, including CKD recognition and qualification for transplantation. Accordingly, aggregate effect (benefit and harm) on the basis of change in using only the 2021 CKD-EPI creatinine equation requires caution. More widespread use of cystatin C should improve the estimates of CKD prevalence and risk.2,6 Finally, it is important to note that our prediction estimates may be affected by the varying population weights present in US census data and the NHANES study.,In conclusion, the use of only the race-free 2021 CKD-EPI creatinine equation provides similar projected increases in CKD prevalence overall compared with prior equations but projects significant differences in racial and ethnic prevalence of CKD in the US population. Routine measurement of cystatin C and use of 2021 CKD-EPI creatinine–cystatin C equation for GFR estimation can mitigate this differential effect."
40,40,442,37220171,https://www.doi.org/10.2215/CJN.0000000000000209,https://journals.lww.com/,"People with CKD face many symptoms and psychosocial stressors that negatively affect health-related quality of life (HRQOL).1 However, HRQOL is not routinely assessed in clinical care, nor are components of HRQOL captured in traditional outcomes studies of patients with CKD. A patient-reported outcome measure (PROM) can comprehensively survey physical and mental health reported directly by the patient, in a manner tailored to conditions being treated. The Kidney Disease Quality of Life 36 (KDQOL-36)2 is a PROM mandated by the Centers for Medicare & Medicaid Services for use in dialysis care; however, there is no requirement for PROM use in non–dialysis-dependent CKD. Longitudinal capture of patient-reported outcomes represents an opportunity to integrate HRQOL into treatment planning and to deliver person-centered care.,To capture HRQOL-related information in adults with kidney disease attending the Johns Hopkins outpatient nephrology clinics, we developed an electronic PROM (ePROM) using elements of the KDQOL-362 (Symptoms and Problems subscale and questions focused on mental health), with additional questions assessing overall quality of life and nocturia given its prevalence in CKD.3 We integrated the ePROM into the electronic health record (EHR) as a CKD Symptom Survey (Figure 1A) distributed to patients through the MyChart patient portal (Epic Systems Corporation). Use of this ePROM was approved by the Johns Hopkins Institutional Review Board.,The ePROM assesses general health, overall quality of life, physical symptoms (12 questions), and mental health (three questions). Questions are scored according to recommended guidelines,4 with higher scores representing better quality of life and less prominent symptoms. Composite physical symptom scores do not include scores for frequent urination at night, given that nocturia is not assessed in the KDQOL-36.2 Nephrology providers can view scores on Epic's Synopsis tab or by adding the .CKDsymptoms dotphrase into clinic notes (Figure 1B). Prior and current ePROM scores are displayed to highlight changes in scores over time.,The ePROM is assigned to patients with CKD G3–5 (as determined by the EHR Problems list) presenting for a follow-up visit with their nephrology provider. The ePROM is not assigned before new patient visits because patients new to clinic may not have received education regarding their CKD diagnosis and associated symptoms. ePROMs are assigned before each follow-up visit to enable longitudinal assessment of symptoms over time, with a greater frequency of symptom capture for patients with advanced or rapidly progressive CKD who are anticipated to see providers more frequently. Appropriate capture of patients with CKD G3–5 on the basis of the EHR Problems list was not formally assessed.,An alert to complete the ePROM is sent through MyChart 7 days before clinic visits, with a reminder sent 1 day before the visit if not completed. Patients can alternatively complete the ePROM in person (using the Epic Welcome program) on an electronic tablet provided during clinic check-in. The results are linked to the visit and are immediately available in patient charts.,After pilot testing the ePROM with patients of one provider (D.M.P.) through MyChart only, followed by 2 months of ePROM distribution to all clinic patients, we captured 287 distinct ePROM responses (30% of 979 total ePROMs distributed) from 264 patients (some with multiple ePROM completions). On comparing the characteristics of ePROM respondents with nonrespondents (Figure 1C), we noted potential disparities in ePROM completion by race (a lower proportion of respondents were Black people) and sex (61% of respondents were male). Physical symptom scores reported by our patients align with those reported in a study of Chronic Renal Insufficiency Cohort participants.5,We noted important considerations regarding the ePROM. While the KDQOL-36 has been validated in non–dialysis-dependent CKD,6 input from patients and clinicians, with additional testing, is needed to refine the ePROM tool (content, method, and frequency of distribution) and maximize its utility. The ePROM has not been externally validated, which limits comparison of scores across different populations of people with CKD. After further assessment of differences in response rates on the basis of patient factors, interventions may be needed to improve ePROM to reach potentially marginalized populations (e.g., those with non-English primary languages, visual impairment, or limited literacy). Nephrology clinician perspectives are needed to identify facilitators and barriers to ePROM use in routine clinical care. Only after addressing these key issues, and after a cultural change prioritizing inclusion of patient-reported outcomes in care, can we achieve optimal implementation of ePROMs in non–dialysis-dependent CKD.,In summary, electronic health systems can enable dissemination of an ePROM to patients with CKD. The ePROM can be administered before each follow-up nephrology appointment, with results uploaded in patient charts such that they can guide clinic visits and treatment plans. Further research is needed to understand the reach of ePROMs, their effect on patient-provider relations, and associations between ePROM scores and clinical end points."
41,41,444,37163584,https://www.doi.org/10.2215/CJN.0000000000000197,https://journals.lww.com/,"After January 25, 2018, multicenter research applications submitted to the National Institutes of Health (NIH) were required to use a single Institutional Review Board (IRB). This affected academic institutions by centralizing IRB oversight to reduce redundancy. Although single IRBs were common in industry-sponsored research and the United Kingdom,1,2 academic IRBs in the United States were reluctant to cede responsibility for NIH studies. This article describes the development, operation, and onboarding timelines of the NIH “APOL1 Long-term Kidney Transplantation Outcomes” (APOLLO) single IRB assessing apolipoprotein L1 gene effects on transplantation.3 Lessons may be useful for streamlining the launch of multi-institutional studies.,In 1983, protection of human participants in research was codified under 45 Code of Federal Regulations 46, and IRBs were developed to evaluate single-center investigator-initiated projects.4 Over time, studies grew to thousands of participants from hundreds of sites.5 The early IRB model where each institution in multisite studies repeatedly reviewed identical protocols delayed start-up and caused difficulty controlling versions of documents.6 Sites operated with different versions of protocols/consent forms and froze enrollment until study-wide approvals were granted.7 Starting-up multiple sites was challenging due to differing IRB processes.6,8,Industry sponsors had long used centralized, often unaffiliated commercial IRBs in multicenter studies.2 Independent central IRBs often claimed rapid approval for site initiation, unencumbered by the time and burden of satisfying local institutional and state requirements. Although industry sponsors used independent IRBs, federal awards avoided them because of funding limitations. This changed in 2018 when the NIH introduced a policy requiring single IRBs in multicenter studies.9 Similar policies were extended to all federally funded studies through revisions to the Common Rule in 2020.4 Institutions and principal investigators now faced novel challenges and expectations with limited resources.,In September 2017, APOLLO was one of the first large-scale federally funded consortia required to apply the single IRB mandate.3 The Coordinating Center and single IRB are at Wake Forest University School of Medicine (WFU). The WFU single IRB oversees APOLLO and its ancillary studies, interacting with the Steering Committee comprised of the NIH, Coordinating Center, Study Chair, 13 Clinical Center principal investigators, Community Advisory Council leadership, United Network for Organ Sharing, and Association of Organ Procure-ment Organizations (OPOs). The single IRB also interacts with 100 engaged-in-research transplant programs, 46 nonengaged-in-research programs with patients recruited by APOLLO Clinical Centers, 57 OPOs, and 64 OPO-aligned HLA laboratories.,Before drafting study documents, Coordinating Center leadership met with the WFU IRB Director to plan for managing 146 recruitment sites. The Coordinating Center identified one lead study manager for single IRB processing, and it was decided that engaged transplant programs would complete the single IRB process in waves leading to “rolling-start-recruitment.” Similarly, the Coordinating Center worked with OPOs and HLA laboratories to permit deceased donor bio sample collection on a rolling basis.,Before the mandate, WFU limited its role as single IRB of record. First, negotiating and executing institutional reliance agreements required extensive revision and discussion regarding delegation of responsibilities, handling/communicating protocol deviations, and providing updates. The SMART IRB master reliance agreement (https://smartirb.org/agreement/) alleviated these issues by streamlining the process using a single master document, rather than institution-specific reliance agreements for each site. Second, our internal IRB system was not designed to support multiple external teams on one protocol. A technical solution was necessary to manage site communications and document reliance decisions and local considerations. Finally, our Human Research Protections Program (HRPP) was not staffed to support uploads of materials from large numbers of reliant sites. To alleviate these barriers, APOLLO used the IRB Reliance Exchange (IREx), a freely available web-based portal supporting single IRB review documentation and coordinating for multicenter clinical trials (https://urldefense.com/v3/__http://www.irbexchange.org__;!!GA8Xfdg!yUA7R-sU2W4jONG1vnBpRvTkKGVmLyGREj2Jw7R4snBTya0BgWw8rIlwwxnFTScGoidYO741RPSdFUsaNCk$).,The APOLLO single IRB required systematically capturing documentation from 100 reliant sites and efficiently disseminating single IRB approvals and documents. Challenges in implementing a single IRB are coordinating communications and capturing and managing bidirectional information exchange between study teams and their HRPPs/administrators.10 To satisfy “local considerations” (including institutional policies and state laws), the respective IRBs at each site were given the opportunity to acknowledge the study and sign off on its conduct, before the single IRB approval. The IREx platform has been used by >40 single IRBs.10 IREx maintains records of reliance decisions, collects local considerations from sites, controls versions of historical documents, and notifies researchers when new versions are approved. The IREx platform permits delegation of single IRB operational responsibilities to the Coordinating Center's “IREx Study Manager” who uses IREx to manage site communications, track progress for completing required agreements and documentation (local considerations and reliance decisions), store site approvals, and notify site HRPPs and study teams of new approvals (initial, continuing review, site amendments, and study modifications).,Figure 1 presents the process beginning when the lead site has been submitted to the single IRB. APOLLO sites were provided draft protocols and consent templates in July 2018; these were approved by the single IRB on January 17, 2019. Owing to revisions during the IRB review process, an updated protocol, manual of procedures, and consent form were redistributed, and some sites had to restart their local considerations process. Therefore, APOLLO strongly suggests that large studies approve their lead sites before onboarding other sites.,Owing to staggered start-up times for onboarding, APOLLO assessed differences in activation times for early sites versus later sites. The median time from protocol/consent form single IRB approval (January 17, 2019) or the date sites were sent materials (whichever was later) to completion and documentation of local considerations (steps 2–4 in the Figure 1) was 201 days (min=0/max=1020). Local investigators and coordinators were responsible for initiating single IRB processes at their institutions, including submission before the local HRPP could initiate and document reviews (steps 4–5 in Figure 1). Among the subset of 35 programs capturing these data, the median time for local review was 25 days (min=0/max=211). In all 100 APOLLO engaged sites, the median time from completion of local considerations documentation in IREx to single IRB submission for review (step 5 in Figure 1) was 7 days (min=0/max=144) and from IRB submission of a site to approval 5 days (min=0/max=17). Single IRB staff and reviewers only controlled the timeline from local consideration completion to IRB approval (step 5 in Figure 1), a median of 14 days (min=1/max=146). Most sites were submitted to the single IRB within 14 days of completing local considerations and received IRB approval within 7 days.,The first APOLLO site was approved on May 13, 2019, and the last was approximately 3 years later. The time required by the WFU Coordinating Center and single IRB staff/reviewers remained consistent. Time from completion of local considerations to single IRB review and approval averaged 12 days for the first 10 sites (in 2019) versus 8 days for the last 10 sites (in 2020).,The APOLLO single IRB made participant information consistent and promoted efficient study start-up. To ease implementation, the standardized consent form template had limited areas for editing. Although sites requested preferred institutional language, format changes outside areas designated for editing were prohibited. The single IRB collected and respected local considerations and differences in state laws, especially differences in Health Insurance Portability and Accountability Act authorizations and laws governing return of genetic results. Critical to APOLLO, approval of materials in one submission by the single IRB allowed 100 sites to simultaneously incorporate the latest version of the protocol and consent form.,Because study teams often did not know how to navigate the review process, a median of 201 days was required for sites to complete local review and be ready for single IRB review. This is consistent with 2019 IREx data showing the median time from Lead Site Approval to Reliant Site single IRB submission approximating 180 days, decreasing to <100 days in 2021. In comparison, APOLLO had longer time between local considerations completion and submission to the IRB, but shorter times to single IRB approval postsubmission. The Coordinating Center IREx Study Manager prereviewed single IRB materials to verify accuracy and completeness. Preliminary review allowed more efficient IRB approval processes postsubmission and contributed to the WFU single IRB having shorter approval times than other IREx studies. Time from completion of local considerations to IRB approval remained similar between early and late participating sites.,APOLLO includes the collection and biobanking of DNA, serum, and urine from deceased donors and participants at engaged transplant programs; however, the single IRB has had the greatest effect on APOL1 genotyping and planning for eventual return of APOL1 genotypes to participants and deceased donor next-of-kin. To that end, the single IRB mandated that genotyping must be performed in a Clinical Laboratory Improvement Amendments laboratory to permit return of results to participants in all 50 states (despite variable local laws). In addition, the NIH will hold a conference devoted to the return of genetic data to study participants in which APOLLO and J. Brian Moore will participate.,In conclusion, WFU successfully implemented a single IRB model for APOLLO, consistent and compliant with its determinations, and maintained reasonable timelines for approval. Communication and collaboration between multiple groups was pivotal. We recommend identifying an IREx Study Manager with sufficient effort dedicated to the study. Without the coordination, support, and resources available from researchers, staff, and collaborators, these processes would have been more burdensome, onerous, and time-consuming. Researchers developing multicenter studies should plan for budgeting, staffing, and coordinating closely with single IRB representatives."
42,42,446,37130621,https://www.doi.org/10.1016/j.ajt.2023.01.013,https://linkinghub.elsevier.com/,
43,43,448,37876239,https://www.doi.org/10.1681/ASN.0000000000000254,https://journals.lww.com/,"Unequal racial and ethnic health, including kidney health, inflicts serious adverse consequences on US society. We at JASN have tried to elevate the importance of this issue in our pages, including our series “Addressing Racial and Ethnic Disparities in Kidney Disease,”1 which emphasized the need to devise, test, and then implement solutions. This racially and ethnically driven unequal health distribution is woven into nearly every major issue facing American medicine today and must be considered an aberration rather than normalized.,Health systems play a critical role in our goal to eliminate this unequal distribution of health driven in large part by the long history of racism in the United States. Our health systems strive to restore health to all with illness and disease, provide evidence-based guidance as to health maintenance to all, and be inclusive of all who seek to contribute to its mission to generate and distribute knowledge that drives health restoration and its maintenance. Optimal success in these missions requires a racially and ethnically diverse workforce in our health systems.2,3 This long-standing recognition of the importance of racial and ethnic diversity of our research, clinical, and educational workforces makes achieving racial and ethnic diversity a practical imperative and not just a moral aspiration. Although moral motives certainly (and thankfully) drive the work that we in the health care workforce do, our colleagues in the business world recognize that racial and ethnic diversity of the business workforce, particularly in its leadership, drives desired hard outcomes and innovation.4 As eloquently articulated by Dr. Herbert Nickens, this makes achieving diversity “not only the right thing to do, but also the smart thing to do.”2 We in the kidney community have the opportunity to apply this tool toward the racially and ethnically driven unequal distribution of kidney health that persists and is not dissipating organically.5 The diversity tool is applicable to achieving excellence in each component of our tripartite mission.,Diversity in the academic medical environment promotes research that is inclusive of the needs and concerns of minoritized groups in the United States.6 One notable example of the practical effectiveness of a racially and ethnically diverse research group is the great success of the African American Study of Kidney Disease trial, one of the most successful large-scale trials conducted by the National Institutes of Health.7 These investigators helped advocate for this groundbreaking trial from which investigators continue to derive important pathophysiologic and clinical insights. This diverse group of investigators also helped drive practical strategies that helped make the trial the great success that it is. Because of the tragic history of abuse by some US researchers, Black participants justifiably have been reluctant to enter into clinical trials.8 In recognition of this history, Black investigators involved with the trial encouraged hiring of Black study coordinators who helped the trial attain its recruitment goals. Having faculty from other racially and ethnically under-represented groups can inform research to help reduce racially and ethnically driven unequal kidney health distribution, and how best to get it done. Unfortunately, little progress has been made in increasing the racial and ethnic diversity of US medical school faculties,9 particularly basic science faculty.10 Recent Supreme Court decisions have made achieving this goal even more challenging.11 In addition, more work is to be done to increase the proportion of faculty from under-represented groups advancing to more senior faculty levels, including leadership positions.9 Early data support that racially and ethnically under-represented faculty are more likely to engage in research that can lead to a reduction in racially and ethnically unequal health distribution disparities.9–11,Increasing the diversity of the physician and other health professional workforce is purported to improve access to health care for underserved populations, many of whom are minoritized populations.12 An Institute of Medicine report suggests that racism, prejudice, and stereotyping by health care providers contribute to differences in care provided to patients seen.3 In this report, minoritized patients were less likely to receive needed services, including clinically necessary procedures, even when they had similar health insurance and ability to pay compared with non-minoritized patients. Moreover, this report suggested that racial and ethnic differences in patients' attitudes, such as patient preferences for treatment, did not adequately explain these racially and ethnically driven unequal health distributions. Subsequent studies support similar findings regarding racially and ethnically unequal kidney health.5 Racial and ethnic concordance between patients and physicians is associated with better patient satisfaction with the care they receive,13 but further studies will help determine whether this translates to better health, including better kidney health, outcomes.,Diversity of teaching faculty and trainees, such as residents and fellows involved in the education mission, as well as diversity among the recipients of medical education, improves medical education.6 Racial and ethnic diversity improves medical education by contributing to cultural competence, that is, an awareness and appreciation of cultural differences among racial and ethnic groups. Cultural competence in minoritized and non-minoritized educators and providers helps promote more effective health care delivery to a diverse patient population.6 Racially and ethnically under-represented faculty importantly sensitizes the faculty as a whole as to important issues of the population being served and how best to address them. In addition, a group of trainees, notably medical students, and house staff, with greater proportions of racially and ethnically under-represented members enhances the learning experience for the individual learner, leading to better educated, more culturally competent physicians and to better quality care for minoritized patients.12 The current low numbers of racially and ethnically under-represented medical students and faculty suggest that residencies are similarly affected. Because house staff play critical roles in the education of medical students and more junior house staff, it is anticipated that greater proportions of racially and ethnically under-represented resident and fellow trainees will enhance cultural competence among trainees. Having a diverse academic medical center faculty helps achieve the goals of providing quality medical education that promotes delivery of high-quality care and helps inform research agendas, including those that addresses the health concerns of all members of society.6 Increased representation of racial and ethnic faculty brings needed cultural sensitivity to medical education and helps non-minoritized faculty become aware of, and appropriately sensitive to, these important issues of patient care.,Despite decades of its documentation, racially and ethnically driven unequal distribution of health, including kidney health, persists. Our health systems play a critical role in eliminating this unequal health and increasing the racial and ethnic diversity of its workforce is an effective tool in its arsenal by which this critical goal is going to be achieved. Health system leaders are encouraged to use their “bully pulpit” to incorporate and implement this effective tool."
44,44,449,36906296,https://www.doi.org/10.1016/j.ajt.2022.10.010,https://linkinghub.elsevier.com/,
45,45,451,36455682,https://www.doi.org/10.1053/j.ajkd.2022.09.020,https://linkinghub.elsevier.com/,"Adherence to healthy dietary patterns is associated with reduced risk of chronic kidney disease (CKD) progression and mortality in adults with CKD.1, 2, 3 However, diet is modifiable, and changes in diet quality may predict disease course and survival.4,5 Using data from the Chronic Renal Insufficiency Cohort (CRIC) Study, we assessed the associations of 4-year changes in diet quality with the subsequent risk of CKD progression and all-cause mortality in adults with CKD.,The CRIC Study enrolled adults with reduced estimated glomerular filtration rate (eGFR; 20-70 mL/min/1.73 m2) (Item S1).6 Diet was assessed at baseline and year 4. We included participants with nonmissing dietary assessments and covariates (Fig S1). We calculated diet quality using 4 index scores: Healthy Eating Index (HEI) 2015, Alternative Healthy Eating Index (AHEI) 2010, Dietary Approaches to Stop Hypertension (DASH) diet score, and alternate Mediterranean diet score (aMed) (Table S1). We classified scores as low (less than or equal to the baseline median) or high (greater than the median) and categorized changes as sustained low (low on both assessments), sustained high (high on both assessments), worsened (high at baseline, low at follow-up), or improved (low at baseline, high at follow-up). We also examined absolute diet score changes (year 4 less baseline), categorized as increased, stable, or decreased.,We calculated covariate-adjusted hazard ratios for associations between changes in diet quality and time to CKD progression (defined as 50% reduction in eGFR from year 4, or initiation of kidney replacement therapy) and all-cause mortality (Item S1). Person-years were calculated from year 4 until the event, study withdrawal, loss to follow-up, or administrative censoring (January 2019). We assessed robustness of findings across subgroups by sex, race, diabetes, year 4 eGFR, and year 4 proteinuria.,Mean diet scores did not change substantially over 4 years (Fig S2), but there was considerable variation in observed changes (Fig S3).,Participants with sustained high diet scores were more educated and reported higher incomes than those with sustained low scores or scores that changed (Table 1; Table S2). Food and nutrient changes associated with categorized score changes are in Table S3.,There were 412 CKD progression events (308 were kidney replacement therapy initiation) observed over a median of 7.0 (IQR, 3.3-9.5) years. The association between categorized diet change and CKD progression differed by diabetes status. Among adults without diabetes, those with sustained low AHEI-2010 and aMed scores had 41% and 39% lower risk of CKD progression, respectively, while those with improved aMed scores had 58% lower risk relative to those with sustained high scores (Fig 1A; Table S4). Among adults with diabetes, those with AHEI-2010 scores that worsened had 93% higher risk of CKD progression, while both improvements and declines in DASH scores were associated with higher risk relative to those with sustained high scores (Fig 1B). Changes in HEI-2015 scores were not associated with CKD progression. Results were similar across sex, race, eGFR, and proteinuria subgroups.,When analyzed as categorized 4-year absolute score changes, among adults without diabetes, increased DASH scores were associated with 39% lower risk of CKD progression compared to stable scores (Table S5). Among adults with diabetes, those with decreased AHEI-2010 scores had 63% higher risk of CKD progression compared to those with stable scores.,There were 393 deaths over a median of 9.4 (IQR, 7.9-10.4) years. Associations between diet change and mortality were consistent across all subgroups. Compared to those with sustained high scores, those with worsened DASH scores had 41% higher risk of death, and those with sustained low aMed scores had 41% higher risk of death (Fig 1C; Table S6).,Analyzed as categorized 4-year absolute score changes, adults with decreased AHEI-2010 scores had 34% higher risk of death compared to those with stable scores (Table S7).,As diet was not assessed after year 4, we could not measure subsequent changes that may have influenced these associations. We cannot discern the extent to which changes in scores were influenced by random measurement error associated with self-reported diet. Inclusion in our study sample required survival to year 4 and complete diet and covariate assessments, which may have introduced selection bias (Table S8). The main analysis does not account for albuminuria, a key predictor of CKD progression. Reverse causality may explain findings, as participants might have changed their diets in response to their health condition. Additional research is warranted to understand motivators of diet change in this population.,Four-year changes in diet quality were not consistently associated with CKD progression in adults with CKD. Associations varied by diet quality index and diabetes status. Different associations by diabetes status may be due to differing motivations for change, as people with diabetes may be followed more closely by physicians and receive instruction to change their diet. Declines in DASH scores were associated with higher mortality risk. Worsening diet quality may predict earlier death in adults with CKD.,Download : Download Acrobat PDF file (808KB)Supplementary File (PDF). Figures S1-S3; Item S1; Tables S1-S8."
46,46,455,37573131,https://www.doi.org/10.1016/j.ajt.2023.01.015,https://linkinghub.elsevier.com/,
47,47,456,38010153,https://www.doi.org/10.1097/TP.0000000000004761,https://journals.lww.com/,"There has been growing interest in heart transplantation from donation after circulatory death (DCD) donors since the first report of distant procurement and successful transplantation of 3 adult DCD hearts in 2014.1 In that initial series, all 3 hearts were explanted after circulatory arrest, then reanimated, and transported using normothermic machine perfusion, a retrieval technique that is now known as direct procurement and perfusion (DPP). Until then, the few reports of heart transplantation from DCD donors had been limited to cases wherein donor and recipient were colocated in the same hospital.2,For the next 8 y, the 2 initial pioneering centers, St Vincent’s Hospital in Sydney, Australia, and Royal Papworth Hospital in Cambridge, England, published progress updates of their results from DCD donors.3-5 Although all DCD heart transplants performed in Australia and most of the DCD heart transplants performed in the United Kingdom used the DPP technique, 25% of DCD heart transplants performed by the Papworth surgeons used a technique known as thoracoabdominal reperfusion (TANRP), which they first described in 2009.6 TANRP involves the use of a cardiopulmonary bypass circuit to restart the circulation and reanimate the heart in the DCD donor after the confirmation of death, cross-clamping the aortic arch vessels to prevent cerebral perfusion. In their initial experience, the Papworth surgeons used normothermic machine perfusion to transport the reanimated heart following TANRP. More recently, however, they and others have reported successful transplantation of DCD hearts retrieved after TANRP and then transported with static cold storage (SCS), potentially eliminating the need for machine perfusion during transport.4,7 In centers that have adopted routine utilization of DCD hearts, survival of heart transplant recipients from DCD donors has been compared favorably with that of contemporaneous heart transplant recipients receiving grafts from donation after brain death (DBD) donors, with DCD donors now accounting for up to 40% of heart transplants performed by these centers.4,5,7 Heart transplantation from DCD donors has now been undertaken in multiple centers across the United Kingdom, Europe, and, more recently, the United States.7,8,Most recently, the results of the first multicenter trial of heart transplantation from DCD donors were published in the New England Journal of Medicine.8 The study involved 15 US centers and was designed as a noninferiority trial with recipients randomized to receive a heart transplant from either a DCD or DBD donor. All DCD transplants were performed using the DPP technique. A total of 180 heart transplants were performed (90 from DCD donors and 90 from DBD donors), although after the removal of protocol violations (mainly due to the use of donors aged <18 y), numbers were reduced to 80 DCD and 86 DBD donor hearts, respectively. The 6-mo posttransplant survival rate, the primary endpoint of the study, was 94% for recipients of DCD donors compared with 90% for recipients of DBD donors. Interestingly, by 2 y post–heart transplant, survival for the DCD cohort was superior to that of the DBD cohort (93% versus 83%, P = 0.0362). As discussed by the authors, the better survival observed in the DCD cohort may have been explained by the finding that the average age of DCD donors was younger than that of DBD donors. In addition, DCD recipients tended to be younger and were less often hospitalized at the time of transplantation. Although severe primary graft dysfunction was higher in the DCD cohort (15% versus 5%), all recipients of DCD hearts regained normal function, whereas 2 recipients in the DBD cohort had primary graft failure and required urgent retransplantation. Another interesting observation was that 5 heart transplant recipients from DCD donors were deemed protocol violations because the functional warm ischemic time exceeded the 30-min limit specified in the protocol. Although not included in the primary analysis, none of these 5 recipients experienced severe early graft dysfunction and all were alive 6 mo post–heart transplant.,Significant limitations of the DPP technique that have restricted its uptake are the cost of the normothermic perfusion device and the additional personnel required for retrieval. In addition, not all hearts retrieved are deemed suitable for transplantation. In the US trial, 11 of 101 hearts retrieved from the Organ Care System were judged to be unsuitable for transplantation. TANRP, followed by traditional SCS, is potentially a much less expensive retrieval technique; however, cross-clamping the aortic arch vessels and restarting the circulation after the declaration of circulatory death has raised ethical concerns and is not permitted in some jurisdictions.9,Results from the recently published US trial8 provide the first multicenter, randomized data to a growing body of evidence4,5,7 that validates the utilization of DCD donors as an additional source of donor hearts. Although short-term and 5-y survival rates compare favorably with contemporaneous recipients of DBD donors, the longest-surviving adult recipient of a DCD donor heart has only just passed 9 y post–heart transplant.1 Questions remain regarding the longer-term outcomes of heart transplant recipients from DCD donors, particularly the incidence and progression of coronary allograft vasculopathy.,As the ethical debate surrounding TANRP continues, consideration needs to be given to determining the optimal preservation strategy for DCD hearts and refining retrieval techniques to maximize organ utilization.10 Furthermore, as long distant procurement of DCD hearts becomes more commonplace, we are likely to see the limits of the duration of NMP being pushed. The tolerable period of SCS following TANRP and the role of any cold-perfusion strategies are yet to be determined in the setting of long distant procurement, as documented SCS times to date have been relatively short.7,Given the very promising results of the US DCD heart transplant trial and the increasing utilization of the DCD pathway for deceased organ donation, the DCD donor will become an increasingly important contributor to the future growth in heart transplantation. Furthermore, as global experience increases, we can expect that the selection criteria for acceptable donor age and functional warm ischemic time will also widen as it has done for other organs, thereby further increasing the contribution of DCD donors to overall heart transplant activity."
48,48,458,36965828,https://www.doi.org/10.1053/j.ajkd.2023.01.445,https://linkinghub.elsevier.com/,"Accurate glomerular filtration rate (GFR) evaluation is important in patients with cancer to determine correct dosing of chemotherapy and eligibility for treatments and clinical trials. Estimated GFR (eGFR) based on serum creatinine (Scr) (eGFRcr) using the 2009 Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) equation is recommended as the “initial test” for GFR evaluation for routine clinical practice in adults, and eGFR based on serum cystatin C (Scys), with or without Scr (eGFRcr-cys or eGFRcys, respectively) using the 2012 CKD-EPI equations or clearance measurements are recommended as confirmatory tests, as appropriate for the clinical setting.1 Non-GFR determinants for Scr (sarcopenia) and Scys (smoking, inflammation, and use of glucocorticoids) might be more frequent in cancer patients. Using the CKD-EPI equations, we recently demonstrated that eGFRcr overestimated measured GFR (mGFR) in a multiracial cohort of patients with solid tumors in Brazil,2 not in active treatment, but was more accurate than using the Cockcroft-Gault (CG) equation2; that eGFRcys underestimated mGFR; and that the eGFRcr-cys equation was the most accurate equation.,The 2009 eGFRcr equation3 and 2012 eGFRcr-cys equation,4 but not the 2012 eGFRcys equation,4 include a term for race (Black vs non-Black). Nephrology societies in the United States now recommend elimination of race from eGFR equations and use of 2021 CKD-EPI equations for eGFRcr and eGFRcr-cys without a race coefficient.5,6 The race-free 2021 CKD-EPI equations lead to lower eGFR values in Black subjects and higher values in non-Black subjects than the 2009 and 2012 equations.6 The 2021 equations have not been assessed in cancer patients. In addition, GFR estimating equations without race might be particularly relevant in countries with multiracial populations, such as Brazil. Here we evaluate the performance of the 2021 CKD-EPI eGFRcr and eGFRcr-cys equations in the same cohort. A complete description of methods is reported elsewhere2 and is summarized in Item S1.,The study population was composed of 1,200 patients. Mean (SD) age was 58.8 ± 13.2 years and mGFR was 78.5 ± 21.7 mL/min/1.73 m2; 50.9% of patients were male. Race distribution was Black participants 12.8% and non-Black participants 81.2% (including White 69%, Asian 2.2%, and Mixed 16%) (Table S1). For eGFRcr, the overestimation of mGFR was larger for the 2021 versus 2009 equation (median of −10.0 vs −8.1 mL/min/1.73 m2), resulting in lesser accuracy (1-P30 of 22.3% vs 19.1%) (Table 1). Nonetheless, the 2021 equation was more accurate than the CG equation (1-P30 of 22.3% vs 24.9%, P = 0.05). For eGFRcr-cys, the overestimation of mGFR was larger for the 2021 versus 2012 equation (median of −4.1 vs −2.0 mL/min/1.73 m2). The 2021 eGFRcr-cys equation was more accurate than the 2021 eGFRcr equation and the 2012 eGFRcys equation (1-P30 of 9.8% vs 22.3% and 12.3%, P values <0.001 and 0.01, respectively). Findings were similar in subgroups defined by age, sex, and clinical stage; as expected, overestimation of mGFR by 2021 eGFRcr and eGFRcr-cys equations was less for Black than non-Black participants (Fig 1 and Tables S2-S5). These results support implementation of the 2021 CKD-EPI eGFRcr equation for patients with solid tumors in Brazil rather than the CG equation as the initial test in GFR evaluation, and for greater implementation of Scys testing.7,Removing race from eGFR equations represents an important advance in GFR evaluation. Although race correlates with ancestry, a biological variable, race varies with numerous social determinants of health, which may affect the relationship between Scr and GFR and lead to heterogeneity in the non-GFR determinants of Scr.8 Self-reported race correlates imperfectly with genomically determined ancestry9; and in mixed-race populations such as in Brazil, race ascertainment has been difficult to establish and is probably influenced by social, environmental, and cultural background, which play a higher role when race barriers are not so clearly established. For these reasons, removing race from eGFR equations may represent an advance in cancer care in Brazil by eliminating a source of bias.,Strengths of our study include a mixed and multiracial study population, in which race ascertainment has been difficult to establish and which does not differ substantially from the general population in Sao Paulo; a prospective design; using a reference method for mGFR; and standardized assays for Scr and Scys. Limitations include conduct in a single-center study, lack of repeat evaluations after beginning treatment, and lack of comparability to the general population of Brazil.,We conclude that the 2021 CKD-EPI equations without race can be incorporated in cancer care in Brazil. Although 2021 eGFRcr was slightly less accurate compared to 2009 eGFRcr and 2012 eGFRcr-cys, the difference was not substantial and could be acceptable in the framework of current understanding of use of race and ethnicity in medical decision making. It is important for oncologists and nephrologists in cancer centers to engage with clinical laboratory pathologists to discuss implementation of these new equations in clinical practice and to provide greater availability of Scys.,Download : Download Acrobat PDF file (368KB)Supplementary File (PDF). Item S1, Tables S1-S5."
49,49,459,37642955,https://www.doi.org/10.2215/CJN.0000000000000303,https://journals.lww.com/,"On January 31, 2020, Secretary of Health and Human Services Alex Azar II declared a public health emergency (PHE) for coronavirus disease 2019 (COVID-19) under Section 319 of the Public Health Service Act. On February 9, 2023, Health and Human Service Secretary Javier Bacerra declared that conditions warranted an end to the PHE on May 9 because rates of infection, hospitalization, and death related to COVID-19 had decreased dramatically.,The picture is significantly different for patients living with kidney diseases in whom the effect of COVID-19 has been dramatic. Rates of infection and mortality were far greater than those in the general Medicare population; for the first time in the 50-year history of the Medicare End-Stage Renal Disease (ESRD) program, the population of patients treated by maintenance dialysis decreased in 2020.1 This is most likely a reflection of the disproportionate effect of the pandemic on this vulnerable population. Transplant recipients affected by comorbidities and chronic immunosuppression also demonstrated higher morbidity and mortality than other patients with COVID-19.2 The high risk of COVID-19 for patients with kidney diseases continues despite the end of the PHE and poses unique challenges to this vulnerable population. Table 1 outlines these challenges and possible solutions.,The Federal Government made extraordinary efforts to protect Americans from COVID-19. Operation Warp Speed led to the approval and production of antiviral vaccines. The Advisory Council on Immunization Practices led efforts to assure that every American had access to primary vaccines and boosters to provide protective immunity to COVID-19, saving millions of lives. The direct distribution of vaccines to patients in dialysis facilities had a substantial effect on ensuring protection of patients with kidney failure.3 During the PHE, the Federal Government purchased a supply of vaccines for all Americans, eliminating cost as a barrier to immunization. If the end of the PHE signals that patients must pay for all or part of vaccine's cost, patients with kidney diseases and other vulnerable populations will have new barriers to best care.,Standard vaccinations, such as influenza, are provided to patients during their routine health care encounters to improve vaccine uptake. Dialysis providers and transplant centers obtain vaccine doses through regular distribution channels but must have single-dose vials available to prevent waste and minimize cost.,During the PHE, the Federal Government made laboratory and over-the-counter COVID-19 tests available free of charge. In-center hemodialysis patients must travel to and from dialysis facilities and receive treatment in a congregate setting three times weekly. Patients with CKD and those with kidney transplants require physician evaluations and laboratory studies; most cannot isolate at home. Laboratory testing for COVID-19 identifies infected patients and allows them to be separated from those vulnerable patients who are uninfected. Thus, testing and isolation minimized the spread of COVID-19 in this vulnerable population.,If testing is less readily available with the end of the PHE, patients may be exposed and infected where they might have been protected. Because antiviral treatments must be started within a narrow therapeutic window (e.g., within 5–7 days of symptom onset), delayed access to testing may make some ineligible for these effective therapeutics.,Antiviral therapies saved millions of lives during the COVID-19 pandemic. The Federal Government provided and paid for these medications to patients with documented severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection. If the end of the PHE initiates out-of-pocket expenses and reduces access to treatments depending on an individual's health care coverage,4 fewer patients may receive recommended treatment. Once commercial payers bear part of the expense, they may require preauthorization, documentation of a SARS-CoV-2 infection, and duration of symptoms before treatment. Preauthorization will likely delay access to therapeutic agents for patients at higher risk of poor outcomes.,There is no evidence that antiviral agents have been used inappropriately during the PHE. Looking forward, there is no reason to anticipate changes in prescribing patterns, and it would greatly benefit patients if preauthorization was prohibited. Inappropriate use of these therapies may be minimized with a monitoring system to verify stability of prescribing.,During the PHE, eligibility for Medicaid benefits was expanded. Many patients with kidney disease, especially pediatric patients, gained coverage through the expansion. With the end of the PHE, eligibility will return to the standards that existed before February 2020, placing these patients at risk of loss of coverage and access to care. For those with CKD, less care may increase progression to kidney failure; for those with kidney transplants, less care may increase the risk of allograft failure. Because the cost of preventing these outcomes is substantially less than the cost of dialysis, the United States would benefit greatly from continuing the expanded coverage provided during the PHE.,During the PHE, waivers were granted to expand telehealth, including permission for the originating site to be a patient's home and use of unencrypted systems for telehealth visits without risk of penalty under the Health Insurance Portability and Accountability Act. In some areas of the United States, access to telehealth was limited by lack of reliable internet services, particularly in underserved urban and rural areas. Patients with kidney disease were greatly served by increased access to telehealth; not only did it afford them the opportunity to remain in the relative safety of their home but it also facilitated access to care.,Secretary Becerra indicated that these waivers will continue until the end of 2024 as reflected in the Consolidated Appropriations Act of 2023. Continuing these waivers beyond 2024 will improve the health of patients with kidney diseases.,During the PHE, providers were required to report COVID-19–related data to the Centers for Disease Control and Prevention, including positive tests for SARS-CoV-2 and antiviral vaccinations. Such data can be used to determine transmission patterns and define pandemic-related morbidity and mortality. Imprecise definitions of severe COVID-19 and of mortality due to COVID-19 made it difficult to interpret these data.,While the United States Renal Data System provides a wealth of data about kidney diseases in the United States and how patients with kidney diseases are affected by diseases like COVID-19, publication 2 years after data are collected limits its utility. Furthermore, this systemic deficiency will be exacerbated after the end of the PHE when mandatory data reporting ends. A lesson learned from the pandemic: We need a data repository with clear reporting standards and tools for appropriate users to use these data for timely analysis of the effect of a next pandemic or other health emergency in this vulnerable population.5,Improvements in the burden of COVID-19 portend the end of the PHE, but the risks of infection, severe morbidity, and mortality continue for patients with kidney disease. Modest but important steps to mitigate this risk include clarification of Advisory Council on Immunization Practice vaccine recommendations, single-dose vaccine vials, ready access to testing for SARS-CoV-2 and antiviral therapeutics, expansion of Medicaid, action to improve access to telehealth, and a readily available easily accessible data repository."
50,50,461,37004914,https://www.doi.org/10.1016/j.ajt.2023.03.021,https://linkinghub.elsevier.com/,
51,51,462,36976655,https://www.doi.org/10.2215/CJN.0000000000000137,https://journals.lww.com/,"Coronavirus disease 2019 illness among patients receiving hemodialysis confers risks beyond those experienced by otherwise healthy adults. These include a higher risk for hospitalization and death,1 due to underlying comorbidities and impaired immune response to vaccination, and higher risk for transmission to other medically vulnerable patients receiving care thrice weekly at the same location. Early detection—e.g., via routine (asymptomatic) screening during dialysis sessions—can potentially attenuate both risks, as has been demonstrated in other high-risk populations.2 Dialysis facilities, however, are typically managing a high volume of medically complex patients, many of whom require resource-intensive oversight during treatment. Furthermore, patients with end-stage kidney disease are disproportionately from disadvantaged populations and are more likely to face systematic barriers such as poverty, racism, and limited English proficiency. Thus, the feasibility and acceptability of embedding any form of routine severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) testing in dialysis facilities is uncertain.,As part of a pilot to inform the conduct of a larger pragmatic study (NCT05225298), we aimed to determine dialysis facility–level feasibility and estimate preliminary patient-level acceptability of integrating routine SARS-CoV-2 testing into dialysis center care. We selected four hemodialysis facilities that varied in size, geography, and neighborhood demographic characteristics.,We first asked facility social workers to exclude patients who could not comprehend a research information sheet because of cognitive or language barriers. We then provided eligible patients with an English or Spanish language research information sheet detailing the rationale for offering routine SARS-CoV-2 testing and the opportunity to opt out of sharing of limited personal health information through an electronic medical record. After a period of at least 1 week, we offered eligible patients the option for a staff-administered SARS-CoV-2 test during dialysis treatment on two occasions, 2 weeks apart, over 4 weeks (spanning September to October 2022). Dialysis staff were able to cancel the test offer to patients known to have tested positive for SARS-CoV-2 in the past 90 days. Staff performed anterior nasal swabs for Abbott Alinity rt-PCR tests, which were processed within 24 hours at a central laboratory. We report data on the number of tests offered, test acceptability (i.e., proportion of offered tests that were completed), and SARS-CoV-2 detection by facility state and test offer. Our study is part of the Rapid Acceleration of Diagnostics in Underserved Populations consortium, and a partnership between US Renal Care (a large dialysis network), Ascend Clinical (a central laboratory that processes the routine laboratories of patients undergoing dialysis at US Renal Care), and Stanford University, with rtPCR testing supplies donated by Abbott. The full study protocol was approved by the Advarra Institutional Review Board.,Among 214 eligible patients, one opted out of further data sharing, and one was absent in both testing periods. The age and sex distribution of the overall patient cohort was representative of the US dialysis population (Table 1). Relative to the US dialysis population, there was overrepresentation of patients noted to be Hispanic and underrepresentation of patients noted to be Black in the electronic health record. We offered at least one test to 212 patients over the two testing periods. The proportion of eligible patients offered a SARS-CoV-2 test was 93% in the first testing period and increased to 99% by the second testing period. One hundred and eighty-eight patients (89%) completed at least one test. Test acceptability varied geographically and was lowest in Colorado and Georgia facilities. Test acceptability was nonetheless >50% for all facilities at the first test offer and highest in the largest dialysis facility (located in California). With the exception of the California facility where test acceptability increased, all other facilities saw a decline in test acceptability by the second test offer. In a majority of refusals, patients declined to state a specific reason (92%), 5% stated they did not want to test while asymptomatic, and 3% stated they found the test uncomfortable.,In this run-in phase intended to inform the conduct of a larger pragmatic study, dialysis facilities were able to successfully integrate offering a SARS-CoV-2 test to a majority of their eligible patients. Overall, most of our diverse patient population accepted the opportunity to perform screening SARS-CoV-2 tests with one high-volume facility achieving very high (>75%) test acceptance. Further work is required to confirm feasibility at an organizational level and for longer durations and to better understand test acceptance over time and variation by geography. Even if proven feasible and acceptable in the planned pragmatic study, incorporating such screening into routine care—whether for SARS-CoV-2 alone or for a panel of respiratory viruses—would require a cost-benefit analysis before widespread implementation.,Our study was performed during a period of relatively low community-level transmission of SARS-CoV-2 in the United States—weekly cases were in the 300,000–400,000 range in mid-October, compared with 9–10 million in mid-January 2022.3 Yet, the detection rate even on the second round of testing was 4%—a high yield indicating the potential utility of offering screening to high-risk, asymptomatic individuals.4 Identifying patients who frequent the dialysis facility before they develop symptoms may allow for more effective infection control policy implementation and reduce exposure of other, high-risk patients and staff. Successful early identification with routine screening may also lead to reduced serious health consequences including need for hospitalization. However, efforts to protect patients receiving dialysis may need to match early detection with an expanded roster of therapies, which, although currently limited for patients with end-stage kidney disease, should be prioritized.5"
52,52,463,36328100,https://www.doi.org/10.1053/j.ajkd.2022.09.011,https://linkinghub.elsevier.com/,"Nephrotic syndrome (NS) is associated with an elevated risk of venous thromboembolism (VTE), occurring in up to 25% of NS patients.1, 2, 3 Guidelines advocate for primary thromboprophylaxis in select NS patients.4 Warfarin, the most studied oral anticoagulant in NS, has been supplanted by direct oral anticoagulants in many other indications. Apixaban, a direct FXa inhibitor, has particular appeal owing to limited renal clearance and observational data suggesting it may be safe and effective with reduced glomerular filtration.5,6 Because apixaban is highly protein bound (87%-93%, with 66% bound to albumin), the hypoalbuminemia and proteinuria common in NS could greatly influence apixaban pharmacokinetics and pharmacodynamics (PK/PD).7,8,This single-institution, parallel-arm phase 1a study evaluated safety and PK/PD of a single apixaban dose in NS (NCT02599532). On day 1, participants were given oral apixaban while fasting, at 10 mg to mirror initial dosing guidance for VTE. Blood was collected prior to and 0.5, 1, 3, 4, 6, 8, and 24 hours after dose for PK analyses and anti-FXa levels. Blood samples were also collected at baseline and at 3, 6, and 24 hours to evaluate thrombin generation, and at baseline and 24 hours for quantitative D-dimer. Urinary protein-creatinine ratio (UPCR), baseline prothrombin time/international normalized ratio, activated partial thromboplastin time, and platelet count were evaluated prior to apixaban administration. Safety assessments were performed at baseline, prior to the 24-hour time point on day 2, and for the entire adverse event reporting period (0-48 hours) (Fig S1). We measured total and free fraction apixaban concentrations by liquid chromatography–tandem mass spectrometry. Functional apixaban concentrations were determined by a chromogenic anti-FXa assay. Detailed methods are included in Item S1.,Eleven participants with NS (6 membranous nephropathy [MN], of which 3 were ineligible at the study visit (Fig S2); 4 focal segmental glomerulosclerosis; 1 minimal change) and 11 healthy controls were enrolled, and 19 completed the study. The groups had similar baseline characteristics (Table S1). Apixaban PK parameters are summarized in Table 1. We examined both total (ie, the protein-bound apixaban fraction plus the protein-unbound “free” apixaban fraction) and free (protein-unbound fraction only) apixaban. The Tmax of both total and free apixaban was 3 hours, indicating similar gastrointestinal absorption rates among all participants. There was no statistically significant difference in Cmax or AUC0-24 of total or free apixaban between NS and control participants. However, the Cmax value for free apixaban as a percentage of that of total circulating apixaban was lower in controls (6.5% [range: 3.6%-13.0%]) than in NS participants (9.1% [6.3%-13.7%]) and highest among those with severe NS (10.3% [6.3%-13.7%]). Similarly, for AUC0-24, corresponding values were 6.4% (5.0%-9.5%), 8.8% (7.0%-12.2%), 9.7% (7.6%-12.2%). While the observed ranges of free apixaban are consistent with published apixaban plasma protein binding estimates (87%-93%),7,8 they suggest greater free fraction exposure in NS, possibly owing to decreased circulating albumin levels.,Compared to controls, those with severe NS had 23% lower AUC0-24 and 28% higher CL/F of total apixaban (Table 1). In severe NS, the higher clearance of total apixaban was observed concurrently with 14% lower free apixaban clearance. Together these differences suggest more rapid clearance of total apixaban in severe NS is driven by a disproportionately greater clearance of the protein-bound apixaban fraction, possibly owing to increased proteinuria.,Among NS participants, maximum free apixaban concentration and AUC0-24 were higher (25% and 28%, respectively), while CL/F was 18% slower than in controls. Collectively, these data could suggest that greater exposure to pharmacologically active free apixaban fraction could possibly translate to higher bleeding risk. At baseline, D-dimer and thrombin generation were nominally higher in NS than in control participants, but did not reach statistical significance. However, D-dimer at 24 hours post dose was significantly higher in NS participants. These and additional PK/PD results are included in Item S2 and Figs S3-S9.,We found a single 10 mg dose of apixaban was safe, and non-steady-state PK/PD was similar between NS and control participants, with some variance dependent upon disease severity. Severity of hypoalbuminemia in NS is the most well-established risk factor for VTE,9 particularly for MN; NS participants with the highest VTE risk and greatest need for pharmacologic thromboprophylaxis may be most likely to have altered PK/PD for highly protein-bound drugs, such as apixaban.7,10 While we observed relatively similar PK/PD in NS and control participants (Fig 1; Figs S3-S8), notable differences could have potential clinical importance if validated, particularly among those with the most severe NS (eg, differences in total apixaban AUC0-24 and CL/F). However, the clinical implications of reduced exposure and increased clearance of total apixaban are currently unknown in a disease whose cardinal features could potentially affect the free fraction PK because all PK/PD measures in this study were assessed only after a single 10 mg dose.,Clinically, we have observed apixaban anticoagulation failure in an MN patient who had recurrent VTE with lower than expected peak apixaban Xa activity, suggesting potentially reduced therapeutic efficacy.11 Among those with severe NS, we noted increased exposure of free fraction apixaban with slower CL/F compared to controls, which was in contrast to the findings of total apixaban. These latter findings could suggest increased bleeding risk from free drug owing to prolonged exposure. However, the exact apixaban concentration leading to increased bleeding risk remains unknown, and the exposure differences we observed may not meet that threshold. Moreover, our toxicity data and present PD data do not confirm such an increased bleeding risk, but only capture changes from a single dose and not steady-state PD.,While PK parameter differences for the pharmacologically free apixaban are descriptively different between NS and control participants, these data need both confirmation and assessment of their clinical relevance in a larger cohort of NS patients, and with steady-state apixaban PK measurements. Steady-state PD assessments may also reveal more differences between NS and control participants. Our observations remain hypothesis-generating given the single-dose study and small sample size. Future studies incorporating steady-state apixaban PK/PD will be needed to reveal the clinical significance of any alterations to free and total apixaban PK in NS, including those patients with more severe hypoalbuminemia or proteinuria.,Our study used traditional PK/PD approaches (eg, quantification of total circulating apixaban and anti-FXa), in combination with innovative approaches to quantify circulating free apixaban and apixaban PD (eg, D-dimer and thrombin generation quantitation), to explore pharmacological differences for both bound and unbound apixaban vis-a-vis proteinuria and hypoalbuminemia in NS. We did note PK/PD differences, whose clinical consequences warrant further evaluation. Future multidose studies with steady-state measures of apixaban exposure, elimination, and PD (NCT04278729) can advise potential clinical studies of NS and VTE and inform clinically appropriate use of this agent for this population.,Download : Download Acrobat PDF file (1MB)Supplementary File (PDF). Figures S1-S9; Items S1-S2; Tables S1-S2."
53,53,466,36283810,https://www.doi.org/10.1681/ASN.2022060685,https://journals.lww.com/,"Underuse of home dialysis, despite its potential for improved patient-centered and health-service outcomes in ESKD, has prompted multiple regulatory policies since the Medicare authorization of dialysis in 1972. In 2019, the Advancing American Kidney Health (AAKH) Executive Order announced a bold step forward, setting the target of >80% for the combined use of home dialysis and transplantation by 2025.1 As the stakeholders debate the feasibility of such targets, it appears clear that a meaningful increase in pre-emptive transplantation, the most desirable form of kidney replacement therapy, is unlikely over the next decade, and any significant success toward this target will require a substantial increase in home dialysis utilization. Considering a top-down “peritoneal/home dialysis-first” philosophy has limited stakeholder “buy-in” in the United States,2 a bottom-up change in clinical culture, which emphasizes patient empowerment, is required. Several reports show that such cultures can be successfully implemented at individual-institute levels, resulting in high home dialysis utilization3,4; however, replicating these at a systems level has been difficult.,Using a parsimonious conceptual model highlighting the state of advanced CKD care with reference to home dialysis in the United States (Figure 1), we argue that two essential components of such cultural change are the universal provision of (1) pre-ESKD nephrology care, and (2) targeted comprehensive pre-ESKD education to all patients with advanced nondialysis CKD. This Perspective highlights the current evidence supporting the overriding importance of these components and explores measures that facilitate them through close collaborations between the stakeholders.,Ample evidence supports the importance of pre-ESKD nephrology care in increasing home dialysis use. Pre-ESKD nephrology care improves the quality of advanced CKD care and positively influences pre- and post-ESKD survivals.5 The odds of home dialysis are more than three times higher among those with pre-ESKD nephrology care compared with those without; odds further increase as the duration of pre-ESKD nephrology care increases.6 Recent descriptions of the transition care units are encouraging, especially for those without the opportunities for pre-ESKD dialysis decision making.7 However, concerns for increased morbidity and the need for system-level changes, with attendant increase in healthcare cost, allow few institutes to provide these services, and <5% of all incident in-center hemodialysis patients ever transfer to home dialysis.8 Despite these data and enhanced focus on early diagnosis and referral, about 40% of patients with incident ESKD, even today, continue to have “none” to “limited” (i.e., <6 months) pre-ESKD nephrology care,9 and the sociodemographic disparities of this population closely mirror those evident in home dialysis utilization.6,10,11 Hence, alternate approaches are needed to better identify these patients during their advanced CKD stage and create healthcare models that facilitate a state of near-universal pre-ESKD nephrology care.,Epidemiologic investigations show that diagnosis codes are unreliable in identifying patients with CKD in clinical databases, with their accuracies varying widely between 8% and 83% on the basis of provider awareness.12 Electronic health record (EHR)–based models using eGFR have much greater, >99%, accuracy for identifying stage ≥3 CKD,13 but we lack validated phenotypes to stage CKD in pragmatic, clinical databases. Establishing models capable of real-time identification of patients with a high likelihood of progression to ESKD14 can help conduct targeted needs-assessments evaluations and outcome research to facilitate the provision of care required to those in greatest need.,An important challenge beyond the identification is nephrology referrals. Clinical decision support tools with EHR prompts have been shown to improve the quality of care and reduce medical errors in several disease models.15 Implementing these widely can improve specialist referrals for patients with advanced CKD within the existing, primary care–led, volume-based Medicare reimbursement system. A large healthcare maintenance organization (HMO) recently showed that implementing quality initiatives that included EHR prompts increased incident home dialysis utilization from 15.2% to 33.8% over ten years.16 Unfortunately, implementation of such measures is uncommon in conventional non-HMO systems, and there have been modest improvements in the pre-ESKD nephrology care prevalence over the last decade. Automatizing these resistant-to-improve referral functions within an EHR may allow more rapid advances in the quality of care to those most in need (Table 1). An ongoing study, TEACH-VET, aims to develop and test an EHR-based model to identify advanced CKD within the Veterans Administration database and evaluate their need/status of nephrology care by proactively initiating communications with the patients and their primary providers.17 If validated, these strategies can be applied across any mid-large EHR system, although thoughtful safeguards will be needed to ensure that the final decision regarding nephrology care is achieved through the trusted relationships between these patients and their primary providers. Additionally, we need statutory clarifications in the Stark Law to allow and incentivize individual healthcare organizations to develop and implement proactive mass-referral systems in the conventional non-HMO systems.18,Finally, universalizing pre-ESKD specialty care will require professional sociceties to engage with a broad swath of stakeholders to address multiple reimbursements and specialty-specific structural causes driving the ongoing nephrology workforce shortage (Table 1).19 Protocol-based care models that leverage the expertise of nephrology-led advanced practice providers can maintain a high quality of care at a substantially lower healthcare system burden.20,The bottom line: having nearly half of the incident ESKD population without pre-ESKD nephrology care, the most foundational factor for home dialysis, is untenable to achieving the rates of home dialysis envisioned in AAKH.,Although pre-ESKD nephrology care is essential for incident home dialysis use, it is not enough. Patients with advanced CKD, and their caregivers, need to comprehend the medical and lifestyle implications of different dialysis modalities to select home dialysis in a genuinely informed manner. Unfortunately, awareness of CKD and its management options is poor among this cohort.9 It is unsurprising that pre-ESKD kidney disease education (KDE) is among the strongest predictors of home dialysis use after pre-ESKD nephrology care.6 A meta-analysis of the available studies using KDE showed that pre-ESKD KDE increases the odds of home dialysis selection and use by over three times, with an average reported rate of home dialysis of approximately 45%–50% among those educated.21 More importantly, these effects are in addition to, and independent of, those achieved by ongoing nephrology care.6,An interesting question: Can providing comprehensive pre-ESKD KDE universally to all patients with advanced CKD significantly improve home dialysis use? The data are conflicting, with studies showing both disparities in the delivery of KDE and its effect on home dialysis across sociodemographic variables, although some recent studies show a near uniform increase across studied variables.3,4,6 Two randomized trials, PREPARE NOW and TEACH-VET, are examining the efficacy of a universal approach at a broader systems level. PREPARE NOW is a pragmatic cluster randomized study testing the effects of a multicomponent, system-based intervention on several outcomes, including the utilization of self-care dialysis therapies. The intervention combines EHR with integrated risk prediction tools, transition care specialists, and psychosocial support in patients with advanced CKD under routine nephrology care in a rural network.22 TEACH-VET is a randomized controlled study aimed to integrate an easy-to-implement, stand-alone KDE delivered to the patient and their caregivers in an intent-to-teach manner into the routine care of veterans with advanced CKD and assess its effects on informed dialysis decision making, home dialysis selection and use, and several clinical, patient-centered, and health-service outcomes.17 These studies will provide critical evidence regarding the needs and efficacy of a system-based approach to universal pre-ESKD KDE for all patients with advanced CKD.,Several structural barriers limit the widespread adoption of KDE in routine clinical care.23 To begin with, at present, even ascertaining the occurrence and evaluating the pragmatic efficacy of KDE in routine clinical care is difficult because few providers use the Medicare KDE reimbursement codes. A recent study showed that <1% of the patients with incident ESKD receive pre-ESKD KDE services, whereas approximately 7% use home dialysis therapy, indicating the occurrence of some form of KDE in a much larger proportion.6 It is likely that this undercoding, at least in part, is driven by the providers’ preference for more lucrative clinical codes to document the KDE provided during clinical care. Qualitative studies are needed to examine the barriers and facilitators of practitioners’ developing, providing, and coding these services more widely. Studies also show that pre-ESKD KDE benefits multiple clinical outcomes, including disease awareness, quality of care, and even survival.24 Broad changes in the Medicare KDE reimbursement policies are needed to eliminate the patient copay requirements and better align the overall KDE reimbursements to their downstream cost and outcome benefits (Table 1).,Additionally, outside a few opinion-based guidelines, it is currently unclear what constitutes adequate KDE. Prospective studies are needed to define and refine the individual parameters of KDE, such as the optimal language, culture-appropriate delivery methods, and the role of the caregiver, pivotal in driving greater patient-centered home dialysis use. Pending these, professional societies may need to establish opinion-based guidelines promoting the universal provision of targeted KDE for all patients with advanced CKD.,Finally, the need for KDE in CKD occurs long before modality determination. Foundations for such longitudinal engagement are unlikely to be stable when implemented through unilateral changes in the nephrology practice.19 Engaging stakeholders, such as primary care providers, renal social workers, dieticians, pharmacists, and surgical and interventional radiology support teams, can create a durable healthcare infrastructure capable of supporting all eligible and willing patients with and on home dialysis.25,To conclude, the development and progression of CKD, delivery of KDE, home dialysis use, and mortality in ESKD are syndemic issues, i.e., the predisposing factors, interventions, and the outcomes are affected by the health inequities. Thus, increasing home dialysis use in ESKD requires addressal of the longitudinal care deficiencies of the pre-ESKD period. AAKH provides a unique opportunity to leverage the political and regulatory momentum to establish the most basic requirements of equitable advanced CKD care, namely pre-ESKD specialty care and targeted comprehensive pre-ESKD education. However, establishing such a system that supports and promotes greater patient-centered home dialysis use across sociodemographic boundaries will require coordinated efforts between different ESKD stakeholders, including the providers, patient advocacy groups, and regulatory agencies."
54,54,469,36932020,https://www.doi.org/10.1016/j.ajt.2023.03.002,https://linkinghub.elsevier.com/,
55,55,470,37516300,https://www.doi.org/10.1053/j.ajkd.2023.04.011,https://linkinghub.elsevier.com/,"Perioperative acute kidney injury requiring dialysis (AKI-D) after left ventricular assist device (LVAD) implantation is associated with poor outcomes, including a mortality ranging from 40% to 75%.1, 2, 3, 4, 5, 6, 7 There is substantial heterogeneity across studies in the reported proportion of patients able to come off dialysis, ranging from 21% to 67%.1,5 For patients who require only short-term dialysis, longer-term kidney outcomes are uncertain. Studies to address this knowledge gap are vital, as glomerular filtration rate is a critical factor in clinical decision making regarding eligibility for subsequent advanced life-saving heart failure therapies. Accordingly, we examined survival and changes in estimated glomerular filtration rate (eGFR) after LVAD implantation in LVAD recipients with and without postoperative AKI-D at a tertiary care hospital. We hypothesized that post-LVAD AKI-D can be followed by improvements in eGFR during long-term follow-up.,We conducted a longitudinal cohort study of adults who received a durable continuous-flow LVAD from January 2010 to December 2017 at Tufts Medical Center (Boston, MA). Detailed methods are presented in Item S1. The exposure of interest was AKI-D within the first month of LVAD implantation. AKI-D recovery was defined as sufficient recovery of kidney function to discontinue dialysis. The prespecified primary outcome was percent change in eGFR at 6 months, 1 year, and 2 years from preimplantation baseline eGFR. Baseline eGFR was determined for each patient using the median of all eGFR measurements within the 30 days before LVAD implantation.8 Multivariable linear regression was performed adjusting for baseline eGFR, age, sex, bridge to transplant versus destination therapy status, history of diabetes mellitus, and ischemic cardiomyopathy.,During the study period, 288 patients received LVADs, of whom 30 developed AKI-D (Fig S1). Among those with AKI-D, 15 experienced AKI-D recovery while 15 patients died before experiencing recovery. Baseline characteristics are summarized in Table 1 and Tables S1 and S2. Patients with AKI-D had lower baseline eGFR (46 mL/min/1.73 m2 in those who recovered vs 53 mL/min/1.73 m2 in those without recovery) as compared with patients without AKI-D (62 mL/min/1.73 m2) (P = 0.005).,Four (13%) patients with AKI-D started dialysis before LVAD implantation, and 22 (73%) patients with AKI-D initiated dialysis within the first week of LVAD implantation (Fig 1A). Initial dialysis modality was continuous venovenous hemodialysis (CVVHD) in all but 1 patient, who started with intermittent hemodialysis (HD) but subsequently transitioned to CVVHD. Among those with AKI-D recovery, 9 were transitioned to intermittent HD before experiencing recovery (Table S3). Median total time on any dialysis modality was 12 days (range, 2-256 days) for those with AKI-D recovery and 17 days (range, 2-146 days) for those without recovery.,Six-month mortality was 100% for those with AKI-D without recovery (n = 15), compared with 2 (13%) deaths in the AKI-D group who recovered, and 31 (12%) deaths among those without AKI-D (n = 258). Mortality remained similar between patients with AKI-D who recovered and patients without AKI-D at 1 year (Fig 1B). Among the 15 patients without AKI-D recovery, 12 died while receiving CVVHD and 3 transitioned to intermittent HD before death. Time receiving intermittent HD before death ranged from 1 to 4.5 months.,Percent change in eGFR (95% CI) at 6 months, 1 year, and 2 years was 18% (−41%, 4%), 22% (−48%, 4%), and 39% (−75%, -3%), respectively, lower among the 15 patients with AKI-D and subsequent recovery as compared with those without AKI-D in adjusted analysis (Table S4, Fig S2). Individual eGFR trajectories among those with AKI-D are depicted in Fig S3.,Our study does have a few limitations. The total number of patients with AKI-D is small. This limits the number of covariates adjusted for. It is also worth noting that interpretation of eGFR can be challenging both in AKI and in LVAD recipients owing to factors such as reduced muscle mass and edema.,This study uniquely compares kidney outcomes between those without AKI-D and those with recovery from AKI-D; we found that among those with kidney recovery, percent change in eGFR was similar to LVAD recipients without AKI-D. AKI-D is common and, in the absence of kidney recovery, is associated with extremely high mortality among LVAD recipients. These findings may be useful in decision making about advanced heart failure therapies. Like other populations,9 LVAD recipients in this study with AKI-D have poor survival, reflecting illness severity leading to AKI-D. Palliative care should be involved early for these patients. Patients and their family members should be educated about the poor prognosis and decreased quality of life that having both an LVAD and kidney failure entails so that they can participate in shared decision making about whether they want to continue dialysis if kidney recovery seems unlikely. Larger longitudinal studies are needed to identify risk factors for AKI-D and subsequent recovery of kidney function.,Download : Download Acrobat PDF file (288KB)Supplementary File (PDF). Figures S1-S3, Item S1, Tables S1-S4."
56,56,471,38108673,https://www.doi.org/10.1053/j.ajkd.2023.10.004,https://linkinghub.elsevier.com/,"Related Article, p. 162,Related Article, p. 162,Nephrology providers have expertise in caring for patients with complex health problems related to chronic kidney disease (CKD), including concurrent CKD complications and commonly co-occurring chronic conditions such as diabetes and heart disease. Along with these problems, older adults with CKD often have other non-CKD-related chronic conditions and their associated medications, as well as functional limitations, cognitive impairment, and physical and mental health symptoms.1 The constellation of age-related problems and symptoms have been summarized in geriatric medicine as the Geriatric 5Ms. These include mind (cognition, mental health), mobility (physical function, frailty, falls), medications (polypharmacy, drug interactions, inappropriate medications), multicomplexity (multiple chronic conditions, complex psychosocial needs), and matters most (health goals such as maintaining independence and maximizing quality of life).2 There is emerging evidence of the importance of the 5Ms domains among older adults with kidney disease.1,3,4 Although depression has been extensively studied in persons with kidney disease and is a key domain in the Geriatric 5Ms,5, 6, 7 specific components of depression have not been well characterized in older adults with CKD. One example that has not been well studied is apathy.,To better understand the association between apathy, defined generally as diminished motivation and social engagement, and physical and cognitive health outcomes among older adults with CKD, in this issue of AJKD Voorend et al8 conducted an analysis using data from a prospective cohort study. The study population included 180 adults ≥65 years old with an estimated glomerular filtration rate (eGFR) ≤20 mL/min/1.73 m2 from outpatient nephrology clinics in the Netherlands. Apathy was defined as scoring 2 or more from a 3-item subscale of the 15-item Geriatric Depression Scale. These included questions about dropping activities and interests, preferring to stay at home, and feeling full of energy. Outcomes included multiple validated measures of physical and cognitive function, obtained at baseline and over 4 years of follow-up, as well as mortality.,More than one-third of study participants met the definition of apathy. Authors also found associations between apathy and lower physical and cognitive function and lower health-related quality of life across several measures at baseline. During follow-up only 1 cognitive measure was found to be associated with apathy. Apathy was, however, associated with an increased risk of mortality during follow-up. Findings were overall similar in secondary analyses restricting the study population to those with isolated apathy compared to those without apathy or depression. Based on these findings, the authors concluded that apathy may be an important clinical phenotype in CKD that deserves more attention. There are several additional clinical implications for providing clinical care for older adults with CKD that could be considered.,First, the authors’ discussion on apathy in decision making brings forward the importance of how to provide person-centered care when one’s decision-making capacity may be impacted. In non–kidney disease populations, patients with apathy have changes in the frontal lobe, which may impact decision making.9 Apathy has also been associated with aversion of effort-based decision making, decision inertia, and decision fatigue.10, 11, 12 Because decision making is affected in multiple domains, patients with apathy and CKD likely require additional support to engage in complex shared decision making, particularly around dialysis treatment decision making. Several approaches should be considered beyond clinical awareness of apathy symptoms. The Geriatric 5Ms offers a framework to increase patient engagement and relevant important decisions by focusing on what matters most. For patients experiencing apathy, clinicians may need to adjust how they communicate. For example, in the setting of decision inertia and decision fatigue, language around what matters most may need to be simple and brief and include input from family caregivers. As there is a lack of evidence for best communication practices to provide person-centered care for individuals with apathy and CKD, more research is needed on effective communication strategies.,Second, findings that apathy is associated with multiple domains of physical and cognitive functioning and health-related quality of life suggest that CKD patients with apathy could benefit from an interdisciplinary team approach. Interdisciplinary care is common in nephrology, especially among those receiving dialysis. However, expanding the clinical team to include psychologists, mental health social workers, exercise professionals, and geriatricians may be necessary to effectively integrate the Geriatric 5Ms to provide person-centered care to help mitigate symptoms of apathy.13 There remains an opportunity to assess the impact of interdisciplinary teams with the goal of integrating the Geriatric 5Ms into kidney care to alleviate apathy symptoms and other related disorders.,A third clinical consideration is related to the measurement of apathy in the current analysis. The questions related to dropping activities and interests and preferring to stay at home are closely related to the geriatric concepts of social participation and community mobility. Prior research has shown that life-space mobility, a measure of community mobility and social participation, declines more rapidly among older adults with eGFR <45 mL/min/1.73 m2 compared to those with eGFR ≥60 mL/min/1.73 m2.14 Lack of energy is also a well-described component of the frailty phenotype.15 Whether or not apathy as measured by the Geriatric Depression Scale subscale is identifying some older adults with limited community mobility owing to physical limitations and frailty, rather than motivational deficits, should be further studied. Regardless, interdisciplinary care that supports both mobility (physical function and community mobility) and the mind (ie, physiological symptoms of apathy) may help improve quality of life for older adults with CKD.,As with all observational research, this paper has limitations that should be considered and used to guide future research in this area. For example, only 12% of the participants completed all 4 years of follow-up. Thus the longitudinal data on function as well as mortality results should be viewed with caution. Furthermore, although authors found an association between apathy and mortality, whether or not treating symptoms of apathy in older CKD patients could reduce mortality is not yet known. From a practical standpoint, singling out apathy for treatment by nephrology clinicians may be difficult in the current state of CKD management in which psychological disorders are frequently under-recognized and undertreated.6 Because symptoms of apathy and depression are closely related, it may be more clinically impactful to emphasize the assessment and treatment of psychological disorders as a whole. As the authors state, 36% of their cohort had symptoms of apathy and depression, compared to 18% with only isolated apathy symptoms. Thus, it may better serve the patient population to develop strategies to lessen psychological distress overall, rather than focus solely on symptoms of apathy.,Findings from Voorend et al add to the existing literature on the importance of psychological symptoms among older adults with CKD. Specifically, their findings suggest that perhaps apathy should be considered a new risk factor for adverse health outcomes in this population. Clinical care and future research should consider interdisciplinary approaches guided by the Geriatric 5Ms to address apathy along with the complex health care, physical, and psychological needs of this patient population."
57,57,472,38150244,https://www.doi.org/10.2215/CJN.0000000000000419,https://journals.lww.com/,"The prevalence of patients with infections caused by multidrug-resistant organisms (MDROs) has dramatically increased across health care systems. MDROs represent a significant public health issue,1 and outpatient dialysis centers are increasingly being called upon to accept and care for these patients.,Hemodialysis facilities serve a critical role in communities, caring for some of the most complex and vulnerable patients in the health care system. The efficient operation of these facilities is essential for the functioning of acute care, post–acute care, and long-term care networks. As medical leaders of the major dialysis provider organizations, we want to share our perspective on the effect and unique challenges of caring for these patients who present differential burden for dialysis facilities.,MDROs are characterized by resistance to one or more classes of antimicrobials.1 They are especially problematic because of their hardiness and endurance in the environment, high transmissibility, difficulty with detection, frequent asymptomatic colonization, and significant associated morbidity and mortality.1 Once present, they are challenging and costly to eradicate,2 increasing the risk of spread to other patients through environmental and staff contamination.,More than 550,000 patients in the United States receive maintenance dialysis for kidney failure; the majority are treated in communal settings. Most are chronically ill and functionally immunocompromised with multiple comorbidities. They are frequent utilizers of health care facilities and often have in-dwelling medical devices. Recent data indicate that over 50% of patients with kidney failure receive antibiotics annually.3 These factors render patients on dialysis uniquely susceptible to colonization and infection with MDROs, and bacterial colonization and infection rates in patients receiving dialysis greatly exceed those of other at-risk populations.4–7,Data on prevalence and incidence of MDROs in outpatient dialysis facilities are limited. A meta-analysis from 2014 including 23 reports revealed a 6.3% pooled prevalence for colonization of patients receiving dialysis with vancomycin-resistant enterococcus.5 However, rates varied across studies from <1% to 30%.4–7 A prospective cohort study in patients on dialysis in which serial surveillance cultures were obtained demonstrated that 28% of patients were colonized with one or more MDRO.4,General recommendations for preventing the emergence and spread of these organisms are presented in Table 1.9 Specific measures for management and treatment of infected patients vary by organism but may include isolation or cohorting, contact investigation and reporting, contact precautions, staff training and education, routine/active surveillance, dedicated staff, unique personal protective equipment requirements, augmented cleaning (Environmental Protection Agency list P or K), and extra disinfection of equipment (Table 1).1,2,9,10 The Centers for Disease Control and Prevention has not published guidelines for the management of these patients in outpatient dialysis facilities. While it is common practice to extrapolate the same standards designed for acute care settings, these current recommendations are not uniformly transferable.,These recommendations are difficult to implement in an outpatient setting and often require substantial additional human and material resources. The literature suggests that many dialysis facilities are challenged with ensuring infection control and, consequently,11 that transmission of MDROs does occur in the outpatient dialysis setting. A prospective cohort study in an outpatient dialysis facility where colonization with MDROs was present demonstrated acquisition of one or more MDRO in 40% of patients who were negative at enrollment.4 Current characteristics of the dialysis facilities contribute to these challenges including staffing challenges, capacity issues, as well as shared treatment space, staff, equipment, bathrooms, and waiting areas. In addition, proximity to many other patients and frequent and prolonged contact between health care workers and patients create substantial opportunity for spread of MDROs. Standard precautions used in dialysis facilities do not reliably halt MDRO transmission in all circumstances.10,Requiring a level of care that approaches that of acute care settings without the necessary infrastructure negates the efficiencies that allow hemodialysis units to operate at scale. The recommended infection control interventions, physical environment changes, and pharmaceutical costs to care for patients with MDROs can be prohibitive and disruptive for dialysis facilities.1,2 Management of patients with MDRO may necessitate altering patient and staff scheduling, limiting or eliminating the use of shared equipment and space, and at times even closure or suspension of new admissions.2,12 These additional precautions are costly and impose additional staff demands, further exacerbating unprecedented labor shortages, supply cost inflation, and caregiver burnout. Unlike acute care settings, it is not possible to provide one-on-one nursing for these patients in an outpatient setting.,For many of these organisms, given the difficulties with eradication and challenges of identifying the organisms by standard laboratory techniques, current guidance suggests that contact precautions are continued indefinitely.9 Isolation of colonized or infected individuals can create unwarranted fear, stigma, and psychological burden for patients and their families. There are often additional laboratory costs created by the specific culture methods required for MDRO surveillance. If the patients require treatment of an MDRO, the options are often limited, costly, and frequently associated with significant side effects and/or require extra monitoring. Dialysis facilities do not have on-site infectious disease consultants, infection preventionists, or laboratory and pharmacy resources to assist with these added demands.,As a result of these complex logistic issues, many dialysis facilities, facing untenable operational trade-offs, are unable to accept patients with MDRO infection or colonization. The current system was structured to deliver efficient treatment of large numbers of generally stable patients with kidney failure; however, it is not equipped or staffed to care for patients requiring prolonged isolation treatments.,Alternative solutions to accommodate the patient on dialysis with an MDRO include independent or staff-assisted home dialysis or dialysis in skilled nursing facilities; however, costs and logistic barriers may not permit these options. The extensive comorbidities common to patients with MDRO limit the number of patients suitable for home dialysis. Staffing shortages limit the ability of individual skilled nursing facilities to provide dialysis care for small numbers of such patients.,Additional funding to compensate for the increased costs of care and exemption from hospital-based guidelines, coupled with the development of guidelines specific to outpatient dialysis facilities, would likely increase the willingness and ability of facilities to care for these patients. Research must be dedicated to understanding the most cost-effective means of mitigating and managing MDROs in outpatient dialysis facilities and exploring novel options for the care of these patients. A better understanding of the transmission dynamics, the benefits of barrier measures, separation of patients with MDRO, and sanitization of personnel and equipment is needed to facilitate the care of these patients. Developing and testing innovative strategies to limit risk of MDROs and their spread requires collaboration between dialysis providers, physicians, acute and post-acute settings, long-term residential facilities, transport providers, and payers. The increasing adoption of at-risk financial government and commercial payor programs opens opportunities for offsetting the increased expenditures to fund innovation and pilot infection control interventions. Initiatives, such as KidneyX, can spur innovation in these areas. Partnership with the local ESKD networks and the American Society of Nephrology, Nephrologists Transforming Dialysis Safety, may yield additional expertise and solutions. In addition, collaboration with state and local health departments and the Centers for Disease Control and Prevention may bring additional resources, and often, these are reportable infections.,Dialysis providers look forward to participating in policy discussions and research efforts that consider the need to align MDRO vigilance and interventions across a broad spectrum of care."
58,58,473,38109091,https://www.doi.org/10.2215/CJN.0000000000000400,https://journals.lww.com/,"AKI secondary to light chain cast nephropathy (formerly myeloma cast nephropathy) is common in patients with multiple myeloma at presentation and a time-critical emergency. We describe our approach to this problem, including the requirement for prompt diagnosis, immediate anti-myeloma treatment including dexamethasone and bortezomib, and high-quality supportive care.,Mrs. L, a 55-year-old woman with a baseline eGFR of 48 ml/min per 1.73 m2, presented to her primary care physician with fatigue and bone pain. She was taking a nonsteroidal anti-inflammatory drug (NSAID). Her eGFR was 14 ml/min per 1.73 m2, and her calcium was 3.01 mmol/L. She was admitted to hospital for further management.,We consider light chain cast nephropathy in the differential diagnosis of AKI, including in patients with known CKD, particularly when there is one or more of hypercalcemia (present in around 50% at presentation), use of NSAIDs, “bland urinalysis,” and normal-size kidneys by imaging.,We do a serum-free light chain measurement urgently and look for an involved immunoglobulin light chain level of ≥500 mg/L as evidence of light chain cast nephropathy.1 Most patients will have AKI stage 3 at presentation and several grams of involved light chain in the serum. Mrs. L had a lambda serum-free light chain of 10,194 mg/L. A bone marrow biopsy was performed to confirm diagnosis, and a positron emission tomography–computed tomography scan was performed for myeloma staging (bone and extramedullary disease).,If we strongly suspect the diagnosis but the serum-free light chain result is negative, we redo the assay with greater dilutions as very high concentrations of immunoglobulin light chain (e.g., more than 20,000 mg/L) can lead to antigen excess and false-negative results. If the serum assay is unavailable, urinary protein electrophoresis for urinary-free light chains (Bence Jones proteinuria) should be performed.,We do serum protein electrophoresis at the time of serum-free light chain measurement to screen for an intact immunoglobulin; 50% of patients with light chain cast nephropathy have light chain–only myeloma and 50% intact immunoglobulin myeloma. If the serum-free light chain or protein electrophoresis is abnormal, we do serum immunofixation; this is more sensitive than serum protein electrophoresis and provides isotype characterization. Mrs. L had an IgG lambda paraprotein of 5.3 g/L.,If the involved serum-free light chain is <500 mg/L, we consider other causes of AKI. Urine albuminuria and proteinuria should be quantified. Significant albuminuria may suggest a glomerular lesion (e.g., light chain [AL] amyloidosis or another monoclonal gammopathy of renal significance), whereas urine albumin will typically be ≤10% of the urinary protein in light chain cast nephropathy.2 We only do a kidney biopsy at presentation for substantial diagnostic uncertainty, for example, a serum-free light chain <500 mg/L and/or significant albuminuria.,Clinicians, nurse specialists, and pharmacists with specialist knowledge in the disease are required to help patients understand their diagnosis and treatment plan. Close working between hemato-oncology and nephrology clinical teams is essential.,Bortezomib and dexamethasone are the cornerstones of treatment. When the serum-free light chain result was received (the day after admission), Mrs. L was commenced on dexamethasone 20 mg daily. We started antimyeloma treatment immediately to rapidly reduce serum-free light chain levels to stop ongoing cast formation and renal tubular injury. Even if bortezomib is delayed by 1–3 days, dexamethasone should not be deferred as it has a cytotoxic effect independent of bortezomib. We do not wait for bone marrow biopsy results before starting dexamethasone, which is given as oral pulse therapy 20–40 mg a day for 4 days; we use a lower dose (e.g., 10–20 mg a day) in frail patients. Mrs. L commenced bortezomib and cyclophosphamide 48 hours after admission. The bone marrow aspirate had shown 75% plasma cells. Positron emission tomography–computed tomography showed innumerable small bony lucencies, in keeping with diffuse involvement by myeloma.,In patients with severe AKI who receive an antimyeloma regimen, including bortezomib and dexamethasone, over 90% sustain a median fall in serum-free light chain levels of >50% by day 4–6.3 A 60% reduction by day 21 is associated with recovery of kidney function in 80% of those affected. There is a lack of data around switching or escalating treatment in patients with early suboptimal responses, but it may be a consideration. Kidney function recovery can occur up to 3 months or more after presentation. Mrs. L had her lowest eGFR of 11 ml/min per 1.73 m2 and did not require dialysis treatment.,The intensity of the antimyeloma regimen is based on functional status4 and includes twice weekly bortezomib and pulse oral dexamethasone with or without additional agents, such as cyclophosphamide and daratumumab or thalidomide and daratumumab. The addition of daratumumab is based on numerous trial data showing additive benefit and limited toxicity.5,6 However, clinicians should be aware that patients with severe kidney failure are usually excluded from trials of antimyeloma chemotherapy. Furthermore, some antimyeloma drugs, such as lenalidomide, are difficult to use in AKI because of dose adjustment requirements for kidney function. Mrs. L received six cycles of bortezomib (twice weekly), cyclophosphamide, and dexamethasone and then had an autologous stem cell transplant. Post-transplant, she has a complete clonal response and is on maintenance lenalidomide. She has stable kidney function with an eGFR of 30 ml/min per 1.73 m2.,In patients who have a good disease response and a good functional status, we offer consolidation treatment with stem cell transplantation. While most patients who require dialysis at presentation will recover independent kidney function, many are left with a long-term eGFR <45 ml/min per 1.73 m2; stem cell transplantation is not contraindicated in this group or patients who continue long-term dialysis.3,Mrs. L was treated on admission with intravenous (IV) fluids and 60-mg pamidronate as a single IV infusion. In vivo studies have demonstrated the effect of modifiable factors on the development of the lesion.7 The Medical Research Council myeloma IV trial showed better kidney recovery in patients hydrated to maintain a urine output of 3 L or more a day.8 Our approach to diagnosis and management is presented in Figure 1.,We do not offer this in our practice. A prospective randomized controlled trial (RCT) performed before the introduction of bortezomib showed no benefit for plasma exchange.9 High cut-off hemodialysis is far more effective for free light chain removal; however, there was no difference in serum-free light chain levels in the first 3 weeks when high cut-off hemodialysis and high-flux hemodialysis were compared in an RCT.3 The two RCTs that have used a bortezomib-based regimen reported conflicting results for high cut-off hemodialysis, with higher kidney recovery but no difference in overall survival in one study10 and lower survival in the other study.3,Mortality has dramatically improved since the introduction of bortezomib and other novel chemotherapy agents. Overall median survival for myeloma is now approaching 6 years. For light chain cast nephropathy and AKI requiring dialysis, over 50% of patients recover independent kidney function if treated promptly, with overall survival more than 80% at 2 years. While survival for patients who remain dialysis dependent is less good, increasing numbers are living for many years due to advances in myeloma treatment. We offer kidney transplantation to those with an excellent disease response who otherwise fulfill criteria."
59,59,476,37801686,https://www.doi.org/10.2215/CJN.0000000000000341,https://journals.lww.com/,"Mr. A, an 85-year-old widower, had kidney failure because of diabetic nephropathy, coronary artery disease, heart failure, and osteoarthritis. His eyesight was poor and hand dexterity limited. He and his daughter received counseling on different dialysis options and supportive care. Shared decision making was made for Mr. A to have automated peritoneal dialysis (APD), performed by his daughter who lives nearby. Both Mr. A and his daughter preferred home peritoneal dialysis (PD), which saved Mr. A from having to frequently travel for dialysis. PD enabled him to receive dialysis in a home environment with less hemodynamic disturbance, no postdialysis fatigue, and no vascular access needed.,Mr. A was initiated with a 2-L bag of 1.5% dextrose dialysate solution for three cycles over 10 hours at nighttime with dry day as he had over 1 L urine/day. However, he complained of low ultrafiltration volume and a drop in urine volume after an episode of peritonitis 18 months after PD initiation, resulting in high BP and volume overload. Urea nitrogen was 328 mg/dl (18.2 mmol/L). Potassium and phosphate were 3 mEq/L and 4.37 mg/dl, respectively. His diuretic dose was maximized to increase urine volume. After discussion with Mr. A and his daughter, his incremental regimen was stepped up to a 24-hour regimen by adding a 2-L bag of icodextrin 7.5% daytime exchange to optimize uremic clearance, volume, and BP control. He received dietary counseling to improve adherence to a salt-restricted kidney diet, and a healthy dietary pattern with high fiber was promoted to relieve constipation. He was taught home exercises to improve and maintain physical function.,According to the International Society for Peritoneal Dialysis (ISPD) Practice Recommendations 2020,1 elderly patients (defined as individuals 65 years or older) with kidney failure should undergo frailty screening using tools, such as the Clinical Frailty Scale, to assess their suitability and need for assistance before initiating PD.1 Conducting a comprehensive geriatric assessment involving multidisciplinary input to determine individual medical, social, and functional needs is the gold-standard approach if initial screening identified frailty (i.e., Clinical Frailty Scale ≥5).1 Elderly patients, especially those with frailty, face many unique challenges. PD prescription and care plans need to be individualized with shared decision making, taking into account baseline physical and cognitive function, comorbidities, symptom burden, lifestyle habits, social support and environment, preferences, and realistic life goals (Figure 1).,Assisted PD particularly suits elderly patients who fail to perform PD themselves because of cognitive or functional decline. It is usually performed by family, aged home staff, or, in some countries, community-based nurses and can be manual or automated. Observational studies reported comparable hospitalization rates, but more dialysis-related complications with assisted PD versus in-center hemodialysis. There was a suggestion that peritonitis rates were lower in assisted PD versus self-care PD.2 Age 75 years or older was not associated with higher peritonitis rates in assisted PD. APD allows both patients and caregivers flexibility and more daytime freedom for life participation, but at a cost.,Elderly patients with significant residual urine volume should receive incremental PD to start, minimizing their treatment burden because the gains from high-dose PD may be less.3 This allows more time for life participation, yet without compromising uremic clearance and volume control.3 Incremental regimens are usually individualized. One may vary in either the number of hours or exchanges performed within 24 hours. The prescription needs adjustment as residual urine volume declines. Monitoring of clinical status and residual urine volume is essential in incremental PD prescription.,Preserving residual kidney function is of prime importance because having more residual kidney function predicts better clinical outcomes, enables incremental PD, and reduces treatment burden.4 ISPD outlined strategies to better preserve residual kidney function, including the use of biocompatible low-glucose degradation product solutions4; administration of drugs that block the renin-angiotensin-aldosterone system, although with weak evidence5; and avoidance of nephrotoxic agents, such as nonsteroidal anti-inflammatory drugs and aminoglycosides.,Preventing peritonitis is a priority in elderly patients because peritonitis is a leading cause of mortality and morbidity in PD and a leading patient-centered outcome.6 ISPD recommended daily topical mupirocin at exit sites or nasal mupirocin to prevent exit-site infections and Staphylococcus aureus carriage. Other preventive measures include retraining of PD techniques for those who perform PD themselves but have physical or cognitive decline over time and those complicated with gram-positive organism peritonitis or after prolonged hospitalizations during which they stop performing PD themselves.7 Modification of PD techniques, for example, shortening of the connection tube to facilitate exit-site dressing, may help to lower peritonitis rates.1,Volume overload is highly prevalent and is associated with higher mortality and heart failure–associated hospitalization risks. Common precipitating factors include nonadherence to dietary salt and fluid intake, low peritoneal ultrafiltration, and declining urine volume. Other than increasing ultrafiltration, treating the underlying cause is essential to achieve and maintain euvolemia. Reinforcing a low-salt fluid-restricted diet, avoiding ultraprocessed foods with hidden salts, and promoting home cooking are essential tips in volume management.8 Many elderly patients have reduced taste acuity and habitually add more salt/seasonings to their food. They may mostly buy take-away food if home cooking is not feasible. Currently, there is no conclusive evidence that bioimpedance-guided fluid management is associated with clinical outcomes.8 ISPD recommended icodextrin to optimize volume control.4,BP is a key objective parameter in assessing the quality of PD prescription. However, targets have not been confirmed.8 Regular medication review is essential to minimize polypharmacy and medication-induced hypotension.1,Symptoms such as fatigue, poor sleep, anxiety, depression, poor appetite, pain, restless legs, constipation, sexual symptoms, and pruritus are common in patients undergoing PD.1,4 Among them, fatigue was ranked one of the highest priority outcomes for patients and caregivers.6 Elderly patients have more difficulties in coping with symptom burden.9 Furthermore, their symptom burden is usually complex, multifactorial, and difficult to quantify. Symptom attributes may change over time.9 Currently, there are no validated symptom-based questionnaires guiding treatment in patients undergoing PD.1 Anxiety and depression are common in elderly patients and require psychological support. Severe cases may need pharmacological treatment.,Protein energy wasting is an important complication associated with higher infection and mortality risks. The International Society of Renal Nutrition and Metabolism (ISRNM) recommended appetite scores, body weight, dietary intake, physical examination of muscle mass, and body fat loss supplemented by biochemical tests (serum potassium, albumin, and cholesterol) for nutritional assessment.10 Nutritional intervention requires a multidisciplinary approach. Supporting elderly patients to purchase kidney-friendly food products in the community and assistance with meals facilitate nutritional management.1 Oral nutritional supplements should be considered for those with protein energy wasting and those who fail to achieve the recommended dietary intake to meet their nutrient and energy demands, particularly patients with intercurrent illness, inflammation, and hypercatabolism.10,Maintaining physical function and balance and minimizing fall risks are key elements in optimizing quality of life for elderly patients.11 Liaisons with geriatric teams for fall assessments and instituted community-based support is needed.1 Promoting group physical activities, home-based exercises, and social functions helps to maintain both physical and cognitive functions and positively engage elderly patients in an active lifestyle that benefits their physical and psychological well-being.11 However, elderly patients may inevitably face cognitive or functional decline. Although some may still manage to perform PD with mild cognitive decline, it is important to recognize when they can no longer perform PD themselves and arrange for assisted PD or revisit their care plan.,Mr. A continued APD at home, assisted by his daughter, for over 2 years with very few hospitalizations, but his cognitive and physical function gradually declined over time. He became mostly home-bound. Both Mr. A and his daughter prioritized alleviating his symptoms such as insomnia, depression, and constipation over survival and agreed upon taking a conservative, palliative care approach in the case he further deteriorated.,Similar to Mr. A, what matters most for elderly patients receiving PD may not be survival, but quality of their living, to live well on PD with minimal symptoms and treatment burden, enabling them to enjoy the remaining time of their lives with minimal hospitalizations.6 Setting up an individualized, patient-centered, goal-directed PD care plan with shared decision making is essential in optimizing care in this population. Patient life goals and care plans will need to be reviewed and realigned as their clinical condition progresses over time and be backed up with palliative support care where appropriate.1"
60,60,477,36148606,https://www.doi.org/10.1111/ajt.17201,https://linkinghub.elsevier.com/,
61,61,478,36344210,https://www.doi.org/10.2215/CJN.09430822,https://journals.lww.com/,"The Florida Society of Nephrology (FSN) was organized in 1966 by a small group of prescient nephrologists at a time when nephrology was in its infancy. Fast forward 56 years, and the FSN has evolved into a vibrant alliance of hundreds of academic and private practice nephrologists focused on education, advocacy, confronting the challenges of building a future nephrology workforce, and practice management improvement. Unlike other medical and surgical specialties, “organized” nephrology has operated mostly at the national level with strong organizations such as the American Society of Nephrology (ASN), Renal Physicians Association (RPA), and the National Kidney Foundation. This article summarizes the opportunities that were created in Florida by developing an independent and strong state-based grassroots nephrology organization working to improve the current and future practice of nephrology in our state and the care delivered to patients with kidney disease.,The successful execution of the myriad of FSN initiatives requires contracting with an effective and committed management organization that works closely with leadership. The FSN maintains a traditional leadership structure of President, Vice-President, Secretary, and Treasurer, each serving 2-year term appointments. They serve on the Executive Committee (EC) along with the Program Committee Chair, Chair of the Past Presidents Council, and eight directors-at-large who serve a maximum of three 2-year terms. Committees focused on Advocacy, Women in Nephrology, Educational Programming, Communications, Finance and Audit, and Past Presidents Council meet regularly and report to the EC. Membership in the organization is dues requiring with financial accommodation made for large groups and academic centers. The Society enjoys robust support from industry partners at the annual meeting, which empowers the organization to invest in its other important initiatives.,An important mission of the FSN has always been the delivery of cutting-edge knowledge to our members. The centerpiece of the organization's education pillar is the annual meeting. Spread over a weekend at a family-friendly resort, four thought leaders from around the country plus two from academic centers in the state are invited to present state-of-the-art lectures in an intimate setting that encourages vibrant discussion and exchange of ideas. The FSN works closely with the Florida Renal Nurses Association to provide a parallel educational track for its members. Both continuing medical education (CME) and maintenance of certification credits are available for the participants. Another highlight is the Geronemus Fellows Competition that encourages trainees to submit their research for consideration. Two finalists are selected to deliver an oral presentation during the main session. This is an excellent opportunity for fellows to gain valuable experience presenting in front of a large group of physicians. The meeting also serves as a networking event where academic and community-based members can discuss issues of mutual importance and fellows can make contacts and interview for potential job opportunities.,Recognizing that many cannot attend the annual meeting and that we must bring quality educational opportunities into the communities where our members live and work, the FSN has developed a series of evening regional meetings featuring an invited out-of-state speaker. Additionally, another educational initiative involves a recent collaboration with our academic teaching centers whereby selected Nephrology Grand Rounds are available to FSN membership in real time over the Zoom platform.,One of the most effective ventures of the FSN has been to harness the purchasing power of a larger organization by acting as a buyer for multiple small groups. As an example, in 2013, the FSN collaborated with a risk-purchasing group for medical malpractice protection, which provides coverage at a 17%–55% discount from what a practitioner could independently obtain in the marketplace. Similarly, the FSN created a health insurance program that provides self-funded and level-funded health insurance benefits for its members and their employees. In addition, during the early months of the coronavirus disease pandemic, the FSN was able to purchase and make available to its membership personal protective equipment at a time when supplies were scarce and difficult to obtain for small practices.,The FSN recognizes that it also has a responsibility in advancing patient care. The organization has facilitated participant recruitment into clinical trials across Florida by listing centers that are actively enrolling patients on the FSN website. In addition, experience has taught us that during natural disasters, nephrologists are in the unique position of being responsible for providing life-sustaining care to displaced dialysis patients. Data suggest that living in a county affected by a hurricane increases the mortality of dialysis patients during the next 30 days by 13%.1 As is the case with many coastal states, every county in Florida has the potential to be affected by a hurricane. The FSN works closely with the Florida Renal Administrators Association in advance of hurricane season to assist with the implementation of the emergency protocols that each local jurisdiction has in place and the emergency preparedness requirements that all dialysis facilities must follow. In addition, the FSN's network of nephrologists can quickly respond to emergency needs of their colleagues and provide a clearinghouse of information, availability, and best practices.,There has been a sharp decline in the interest of medical students and internal medicine residents in nephrology as a career choice over the last decade.2 Forty-seven percent of nephrology fellowship programs reported having at least one unfilled fellowship slot during the 2022 appointment year match.3 Studies have shown that up to two thirds of medical students develop an interest in their chosen specialty before or during medical school and cite mentorship as one of the important factors in their specialty choice.4 Recognizing this nephrology workforce challenge, the FSN launched an initiative in 2016 for the early engagement of medical students through an outreach effort to all Florida medical schools. As part of this initiative, the FSN provided educational grants to medical students to attend the annual meeting. During this weekend, the students interacted with FSN members and leadership in special sessions and developed mentor/mentee relationships in an effort to spark their interest in nephrology. The pre- and postmeeting survey of the medical students showed significant improvement in the negative perceptions of nephrology regarding work–life balance, procedural opportunities, and complexity of renal pathophysiology. We are aware of one student who has chosen nephrology as a career pathway. This FSN initiative has evolved into a long-term partnership with the National Institutes of Health–funded University of Miami Kidney Innovative and Interdisciplinary Medical Education in Research Activities (UM-KIIMERA) program. Ten first-year medical students are selected from the Florida medical schools to attend the UM-KIIMERA immersive summer mentored nephrology research program. The FSN follows this up with travel scholarships to these UM-KIIMERA medical students to attend the subsequent annual meeting and present their research projects. The FSN couples this with travel grants to chief nephrology fellows from Florida fellowship programs to attend the same meeting, thereby fostering the interaction of medical students and fellows. The KIIMERA students will continue to be tracked on their career decisions. This grassroots workforce effort can be a model for other state nephrology societies to supplement initiatives being taken at the national level by ASN.,It is imperative for the nephrology community to play an active role in political advocacy and public policy at the state level to safeguard the interests of both patients with kidney disease and our nephrology practices.5 FSN operates a bipartisan political action committee that meets with and financially supports pronephrology candidates. The Society also retains a contract lobbyist in the state capital to advocate for nephrology issues and keep membership aware of legislative initiatives as they move through the committee process. In addition, FSN leadership meets with the Florida Agency for Health Care Administration each year to discuss access to care issues for patients with kidney disease. Furthermore, FSN participates in Florida Kidney Day each year and meets personally with state legislators to encourage support of pronephrology legislation. In the past legislative session, the FSN successfully lobbied along with other organizations in opposition to a bill that proposed to change the structure of the Florida Organ Transplantation Advisory Committee that could have negatively affected kidney transplant programs in the state. Furthermore, FSN interfaces with both regional and national organizations. As an example, FSN leadership is actively involved in the Florida Medical Associations' annual advocacy meetings, and the FSN invites RPA representatives to the FSN annual meeting to update members about important RPA initiatives.,These advocacy efforts and collaborations with other medical organizations are critical to breaking down the barriers to providing high-quality care to patients with kidney diseases and protecting the interests of nephrologists.,With the vast availability of resources on the web and the conglomeration of physician practices, one may question the need for a nephrology organization at the state level. The success of the FSN demonstrates the inherent value of a strong state-based nephrology organization and the important space that it fills in the day-to-day life of nephrologists in Florida. By aligning the interests and efforts of academic and private sector nephrologists, the Society delivers quality educational content, advocacy in the state capital, and medical student programs that highlight the excitement of a career in nephrology. Important practice support initiatives for malpractice relief and affordable health care are also in place. The Society and its membership also represent a cohesive statewide network that is available for rapid mobilization to assist colleagues with patient care during public health emergencies, such as what has occurred after destructive hurricanes or early in the pandemic. Nephrologists in other states may find this model of grassroots nephrology to have value and could benefit from the more than 50 years of experience of the FSN."
62,62,479,37659834,https://www.doi.org/10.1016/j.ajt.2023.01.017,https://linkinghub.elsevier.com/,
63,63,480,38096088,https://www.doi.org/10.1681/ASN.0000000000000289,https://journals.lww.com/,"Sodium is an obligate component of dialysate fluid, essential for preventing cell lysis. In most settings, dialysate sodium (DNa) concentration is set as a facility-level default and not determined by individual clinical characteristics. Reports consistently find that most dialysis facilities practice a “default” approach to DNa.,Initially, DNa levels may have been determined by technical factors. The DNa levels of the 1960s were around 130 mmol/L, creating a sodium diffusion gradient in the context of long slow dialysis sessions. As technologies advanced and population pressures mounted, dialysis sessions became shorter and DNa rose to support hemodynamic stability.,Over the past decade, there has been increasing interest in the possibility that lower DNa concentrations may be a mechanism to reduce the excess cardiovascular disease seen in people with kidney failure. Disordered sodium and fluid homeostasis is a defining feature of kidney failure. Persistent sodium and fluid overload may mediate cardiovascular pathology through increased systolic blood pressure, greater thirst-driven interdialytic weight gain, and direct vascular toxicity, leading to left ventricular hypertrophy, myocardial fibrosis, and ultimately cardiac morbidity and mortality.,There is a global momentum for a reduction in DNa concentrations. In fact, the average DNa concentration has fallen in every country included in the Dialysis Outcomes and Practice Patterns Study (DOPPS) reports.1 Interest in reducing DNa concentrations was most clearly stated in a proposal published in 2014 by a coalition of the major US dialysis providers.2 The statement acknowledged the need for randomized evidence but, in the meantime, called for a reduction of DNa concentrations to 134–138 mmol/L, along with lengthened dialysis session times and avoidance of sodium loading.,These changes, however, are not supported by robust evidence. The observational evidence did not show improved survival with lower DNa, rather showed an association with harm for some subgroups. The DOPPS serial cross-sectional analyses of registry-level data from multiple countries conducted a series of analyses of DNa concentrations and mortality. DNa levels >140 mmol/L were associated with lower hospitalizations and, in units practising a default approach, with lower all-cause mortality.3 The association with better mortality was particularly apparent for people with lower serum sodium concentrations (<137 mmol/L).4 The DOPPS findings were consistent regardless of whether they restricted the analyses to units practising a default DNa approach or included units with individual DNa approaches. These reports were interpreted cautiously, given the inability to completely adjust for confounders, including the multiple differences in dialysis delivery. By contrast, a study of 2272 people in an American dialysis chain found that DNa concentrations >140 mmol/L were associated with an increased risk of mortality in individuals with higher predialysis serum sodium, compared with DNa concentrations of 140 or <140 mmol/L, although there was no significant association in those with lower predialysis sodium values.5 This study included relatively few patients (257 patients in total) who received a DNa concentration of <140 mmol/L underscoring one of the limitations of comparing different observational reports.,What other evidence supports the enthusiasm for lower DNa concentrations? A relatively small randomized evidence base suggests improvements in some short-term cardiovascular surrogates. The randomized evidence base of <700 patients in 15 trials6–9 shows beneficial effects overall for some surrogates, including lower SBP and less interdialytic weight gain. An improvement in BP is persuasive, given the benefits for pharmaceutically induced blood pressure lowering in the dialysis10 as well as the general population. However the randomized DNa trials show more frequent intradialytic hypotension with lower DNa concentrations,6 a phenomenon associated with worse cardiovascular outcomes, as well as giving conflicting results for left ventricular hypertrophy.7,8,In this context, a new observational study sheds light on contemporary practice. Pinter et al.11 analyzed mortality in more than 68,000 incident hemodialysis patients under the care of a single dialysis chain in 25 countries comparing lower DNa concentrations (138 mmol/L or less, mostly 138 mmol/L) to higher DNa (>138 mmol/L, mostly 140 mmol/L). Lower DNa was associated with a 57% increase in all-cause mortality (hazard ratio, 1.57; 95% confidence interval, 1.25 to 1.98) compared with higher DNa after adjusting for multiple confounders. Unlike the earlier reports, the increased mortality associated with lower DNa was seen regardless of whether patients had low, normal, or high serum sodium levels but was particularly marked in those with low serum sodium (hazard ratio, 2.56; 95% confidence interval, 2.00 to 3.28). Strengths of this study include its inclusion of patients from a single chain, data extracted from a common electronic medical record, and the ability to adjust for multiple confounders.,The Pinter report does not resolve the dilemma facing the dialysis community. The evidence for DNa concentrations remains confusing, whereas the observational evidence for mortality does not support the direction in which the world's dialysis units are moving.,There is an urgency to generate high-quality evidence given DNa is an obligate part of hemodialysis delivery. The Randomized Evaluation of Sodium Dialysate Levels on Vascular Events (RESOLVE) study (NCT02823821) is an international cluster randomized quality assurance study led by a global coalition of clinicians, clinical researchers, and patient partners designed to provide definitive guidance for practice. Participating dialysis units already practising a default DNa approach are randomized to one of two default DNa concentrations in current practice, 137 and 140 mmol/L. Recruited units currently practice a default approach, which means they need not change their clinical approach and can continue to use nondefault DNa levels in individual circumstances. The study is embedded in clinical care in multiple countries, reflecting the diversity of clinical practice. It uses existing clinical information systems and clinical definitions to determine the components of its primary end point of all-cause death, myocardial infarction, and stroke. The study reflects clinical practice other than the replacement of arbitrary with randomized determination of default DNa concentration and the sharing of anonymized data to permit analysis. It operates with simplified consent (waiver or opt-out consent) with the approval of local Institutional Review Boards and in accordance with global and national statements criteria for simplified consent including that patients are not exposed to risks greater than those encountered in clinical practice.12,The choice of a cluster randomized design over an individually randomized one is important for a few reasons. First, most units currently practice a default approach, a finding confirmed by Pinter's analyses. Second, a cluster randomized design allows great generalizability of the study findings which is particularly important when the intervention cannot be avoided as with DNa for recipients of hemodialysis therapy. Third, inclusion of a broad population is particularly important when there are plausible suspicions of an interaction between frailty and the effect of treatments. Participants in conventional randomized trials often represent relatively healthier patients. This is also true of trials recruiting dialysis patients where participants often have better prognostic markers at study entry and experience roughly half the mortality rate of people in the general dialysis population.13 DNa may have different effects in healthier compared with frailer patients as suggested by the DOPPS analyses. People with low serum sodium may respond differently to DNa than those with higher sodium, and low serum sodium has a well-established association with frailty, including among dialysis patients.,RESOLVE is well underway with six actively participating countries and others in preparation. Pinter's analysis has added materially to the urgency to complete the study by raising again uncertainties rather than providing definitive answers. The limitations of all the observational evidence are readily apparent. It is impossible to adjust for all known and unknown confounders, particularly for questions, such as sodium management, which may be affected by diet, culture, and ethnicity as well as multiple aspects of dialysis and general medical care. It even remains difficult to directly compare the large observational analyses, given the differences in populations, DNa distributions, and other dialysate practices. Meanwhile, variation in practice persists, perpetuating a situation in which some patients may have worse outcomes, if DNa concentration has an effect on cardiovascular outcomes and mortality.,The issue of DNa concentration selection is one of many examples in health care where practice arises from arbitrary decisions. A RESOLVE-like approach could be considered for many aspects of health care delivery. Quality assurance approaches where randomization is embedded in continually evolving care delivery under appropriate ethical oversight could accelerate the advent of better outcomes for patients."
64,64,481,36754002,https://www.doi.org/10.2215/CJN.0000000000000055,https://journals.lww.com/,"Autosomal dominant polycystic kidney disease (ADPKD) spectrum disorders are characterized by the development of kidney cysts and progressive decline in kidney function.1 Management of these diseases has dramatically improved due to extensive experimental studies revealing mechanisms of cystogenesis in the kidney and defining therapeutic targets, positive results from clinical trials of therapies, innovations in image analysis, and the application of genetic testing that provides additional prognostic information at the point of care.2 Mutations in the main ADPKD-causative genes (i.e., PKD1 and PKD2) reduce levels of their encoded proteins polycystin-1 or polycystin-2 initiating formation of cysts, which continue to grow involving various cellular mechanisms. Despite there being multiple pathways that are dysregulated in ADPKD, intracellular cyclic adenosine monophosphate (cAMP) signaling is one of the major mechanisms that underlay cystogenesis in the kidney. Experimental studies confirmed that in renal epithelial cells lining kidney cysts, increased level of cAMP contributes to progressive cyst growth (Figure 1). On the basis of many observations, cAMP-targeted therapies aimed to lower cAMP in kidney cysts are considered beneficial in PKD treatment.3 Recent strategies are focused on targeting two cAMP-linked G-protein–coupled receptors expressed in kidney tubules, i.e., vasopressin receptor 2 (V2R) and somatostatin receptors (SSTRs).,In the human kidney, the V2R is expressed in the collecting ducts and medullary thick ascending limb of the loop of Henle. Ligands of V2R increase cAMP levels resulting in activation of cAMP downstream effectors (such as protein kinase A, exchange protein directly activated by cAMP, phosphorylated-extracellular signal-regulated protein kinase), phosphorylation of several intracellular targets (such as water channel AQP2 and the transcription factor cAMP response element-binding protein). Altogether, these events accelerate cell proliferation and fluid secretion—i.e., two major mechanisms accounting for progressive cystogenesis in the kidney. Therefore, blockade of V2R is a worthwhile strategy in PKD treatment. In several clinical trials, an antagonist of V2R, tolvaptan, slowed the progression of cyst development and kidney insufficiency and was approved in 2018 in the United States for patients with ADPKD at risk of rapid cyst progression.4 Tolvaptan can delay eGFR decline, retard kidney volume increase, and reduce kidney pain, urinary tract infection, and hypertension. However, some adverse effects such as thirst, polyuria, and hepatic injury were reported.5,Studies related to the presence of SSTRs in healthy and PKD kidneys are limited. Recent reports suggest that all five subtypes of SSTRs are expressed in renal epithelial cells with predominant expression of SSTR1, 2, and 5. Experimental data in PKD1-deficient rodents also showed the decreased levels of SSTR2 in cystic kidneys.6 Activation of SSTRs inhibits cAMP signaling, decreases cell proliferation, and reduces the release of aldosterone and renin. Thus, agonists of SSTRs have the potential to inhibit cAMP production, consequently slowing down cystogenesis in the kidney. As of today, the most common clinically used SST analogs are octreotide, lanreotide, and pasireotide. Effects of SST analogs on ADPKD progression were investigated in several clinical trials with the overall conclusion that SST analogs have a beneficial effect on total kidney volume but do not retard the rate of kidney function decline. SST analogs have been shown to have clear benefits in polycystic liver disease in reducing liver growth.7 The adverse effects of SSTR analogs include diarrhea (most common side effect), abdominal pain, biliary sludge (and potentially cholelithiasis), hyperglycemia, and prolonged QT interval.8 Most effects are mild to moderate in severity, occurred mostly within the initial 2–3 months of treatment, and resolved on treatment or with dose reduction.,Given that multiple mechanisms underlay cystogenesis in the kidney in ADPKD, concurrent targeting of different pathways or synergistic targeting of different entities in a single pathway is an attractive concept. Indeed, simultaneous treatment of PKD1-deficient mice with tolvaptan and pasireotide reduced cystogenesis in the kidney and cAMP level.9,The article by Trillini et al.,10 published in this issue of CJASN, investigated how concurrent use of two therapeutic agents targeting different entities (i.e., V2R and SSTRs) of dysregulated cAMP pathway affects cystogenesis in the kidney in ADPKD. Octreotide–long-acting release (LAR) added on to tolvaptan reduced GFR to a greater degree than tolvaptan alone at 1 week but not 4 weeks, and there was complete recovery of GFR to baseline during medication washout. The authors interpreted these findings as indicating a hemodynamically mediated reduction in hyperfiltration with combination therapy. Both GFR changes (those at 1 week and 4 weeks) were followed by two rebound GFR increases during the 1-month washout from both treatment periods, which they believe could be a signal of nephroprotection in the long term. Total kidney volume and cystic kidney volume were not affected by tolvaptan and placebo therapy, whereas they significantly decreased with dual therapy.,Combined tolvaptan–octreotide therapy also dampened albuminuria and reduced pollakiuria. There are some limitations to this study, including small number of participants and a very short study duration performed in a single center. Therefore, the durability of the positive effects seen in this study is uncertain at this time.,While the article by Trillini et al. is the first clinical study demonstrating possible benefits of combinational drug approach for ADPKD treatment, some important questions remain to be addressed. Will longer treatment with tolvaptan and octreotide-LAR be more effective? What is the effect of drug withdrawal on cystogenesis in the kidney? Would this combinational therapy be beneficial if implemented at early stages of ADPKD? Would it be best to first study fast progressor groups, i.e., Mayo Irazabal class IC, ID, and 1E (who have the fastest progression in PKD)? What would the adverse effects of prolonged treatment with these two drugs be? Would combined treatment promote hepatic injury? Whether the combined effects of both tolvaptan and octreotide may prove to be renoprotective in the long term deserves to be tested. To answer these questions, multicenter studies, carefully designed, with adequate statistical power and follow-up duration are now needed to determine whether the effects of tolvaptan and somatostatin analogs on kidney (and liver) are cumulative and sustained, and that are capable of examining the effect on kidney function, even if modest."
65,65,482,38039089,https://www.doi.org/10.1681/ASN.0000000000000230,https://journals.lww.com/,"We followed with interest the report by Perico et al. on the next-generation human bone marrow-derived, antibody-purified (CD362+) population of next-generation bone marrow–derived, anti-CD362-selected, allogeneic mesenchymal stromal cells (ORBCEL-M) that preserves renal function (evaluated by eGFR estimated by the CKD Epidemiology Collaboration and Modification of Diet in Renal Disease equations not by measured GFR) in people with progressive diabetic kidney disease.1 Mesenchymal stromal cells have emerged as potential candidates for cell-based therapies to modulate the immune response and repair tissues after injury.2 Immunologic profiling provided that ORBCEL-M preserves circulating regulatory T cells, lowers natural killer T cells, and stabilizes inflammatory monocyte subsets.1 We appreciated that their findings shed light on the research of cell-based therapies for patients with diabetic kidney disease. However, after reading this article, we would like to highlight some important issues in this study.,First, we found that the article lacked descriptions of the kidney biopsy. According to previous research results, although renal lesions in patients with type II diabetes manifest in a quite heterogeneous fashion,3 histopathologic lesions on kidney biopsy provide prognostic information, even after adjustment for proteinuria and eGFR. Therefore, we thought the authors should have stated this in the article. The authors could further determine and compare the degree of renal lesions in different patients through kidney biopsy. These efforts will contribute to the credibility of their findings.,Second, we observed that critical information, such as the hypoglycemic drugs, especially novel hypoglycemic drugs, such as dipeptidyl peptidase-4 inhibitors and glucagon-like peptide-1 analogs, were ignored. Dipeptidyl peptidase-4 inhibitors and glucagon-like peptide-1 analogs had potential kidney protective effects, and the lack of this information might threaten the reproducibility of this study.,Moreover, a previous multicenter, randomized, double-blind, dose-escalating, sequential, placebo-controlled trial assessing a single intravenous infusion of allogeneic mesenchymal precursor cells or placebo in adults with diabetic nephropathy with an eGFR of 20–50 ml/min per 1.73 m2 indicated that mesenchymal precursor cell infusion stabilized or improved eGFR and measured GFR at week 12 without adverse events. However, we could find no measured GFR benefit in Novel Stromal Cell Therapy for Diabetic Kidney Disease (NEPHSTROM). Not come singly but in pairs. A previous retrospective cohort study found that autologous adipose-derived mesenchymal stem cells significantly improved high-density lipoprotein, low-density lipoprotein, and remnant-like particle cholesterol levels.4 We could find no changes in total cholesterol and triglycerides in NEPHSTROM.,Fourth, the wide range of baseline urine albumin-to-creatinine ratio as well as immune and inflammatory profiles in a small number of patients complicated the assessment of changes within and between groups in these parameters. In addition, multiple exploratory statistical analyses cannot rule out the possibility of type 1 errors.,Finally, a small sample size is suitable for a 1b/2a trial, it cannot rule out rarer safety events. Although there were no immune-related adverse events, the authors have not reinfused patients with cells from the same donor to confirm the lack of sensitization.,In conclusion, the authors documented the safety and tolerability of a single infusion of lowest dose ORBCEL-M in patients with diabetic kidney disease. However, further investigation of ORBCEL-M in diabetic kidney disease in appropriately sized and powered studies of longer duration, including periodic dosing and optimal dose and frequency, is urgently needed."
66,66,483,37921833,https://www.doi.org/10.1681/ASN.0000000000000208,https://journals.lww.com/,"We thank Groothof et al.1 for their letter and the opportunity to clarify certain aspects of our analysis.2 In our calculation of eGFR based on creatinine (eGFRcr), eGFR based on cystatin C (eGFRcys), and eGFRcr-cys, we followed current recommendations for reporting cystatin C (using two decimals for values in mg/L) and for serum creatinine (using two decimals for values in mg/dl and integers for values in micromole/L).3,4 Thus, we did not round cystatin C values to the nearest integer before calculations as asserted by Groothof et al. In our baseline table, we rounded cystatin C to the nearest integer; here, we present them using 2 decimals for the interested reader: 1.28 (0.97 to 1.80) mg/L in the overall population, 1.57 (1.23 to 2.12) mg/L for eGFRcys<eGFRcr, 1.01 (0.85 to 1.37) mg/L for eGFRcys≈eGFRcr, and 0.96 (0.80–1.96) mg/L for eGFRcys>eGFRcr.,We did not implement a stratified bootstrap but performed bootstrapping followed by stratification by discordance group. Reanalysis using a stratified bootstrap leads to virtually identical results: For the eGFRcys>eGFRcr discordance group, median biases (95% confidence interval [CI]) using a stratified bootstrap are −4.5 (−5.3 to −3.8) ml/min per 1.73 m2 for eGFRcr, 8.4 (7.3 to 10.0) for eGFRcys, and 1.8 (1.1 to 2.5) for eGFRcr-cys. For the eGFRcys<eGFRcr discordance group, median biases (95% CI) using a stratified bootstrap are 15.0 (14.5 to 15.5) ml/min per 1.73 m2 for eGFRcr, −8.6 (−9.0 to −8.3) for eGFRcys, and 0.7 (0.5 to 1.0) for eGFRcr-cys.,Similar to Groothof et al., we now also reanalyzed our data by rounding cystatin C values to one decimal or the nearest integer before calculating eGFR. Rounding cystatin C values to one decimal or the nearest integer would result in sample sizes of 736 and 1150, respectively, in the eGFRcys>eGFRcr discordance group (compared with 713 in our main analysis that used two decimals). When rounding cystatin C to one decimal before eGFR calculation, median biases (95% CI) in the eGFRcys>eGFRcr discordance group would be −4.4 (−5.3 to −3.8) ml/min per 1.73 m2 for eGFRcr, 9.5 (7.9 to 11.2) for eGFRcys, and 2.0 (1.5 to 2.8) for eGFRcr-cys. For the eGFRcys<eGFRcr group, median biases would be 15.0 (14.5 to 15.5), −8.7 (−9.0 to −8.3), and 0.7 (0.4 to 1.0), respectively. When rounding cystatin C to the nearest integer, median biases (95% CI) would be −1.0 (−1.3 to −0.5) for eGFRcr, 18.4 (17.4 to 19.3) for eGFRcys, and 10.1 (9.2 to 11.0) for eGFRcr-cys. For the eGFRcys<eGFRcr group, median biases would be 13.1 (12.6 to 13.6), −11.2 (−11.8 to −10.8), and −1.6 (−1.9 to −1.2), respectively.,In conclusion, we agree with Groothof et al. that researchers should use two decimals for cystatin C values in mg/L before calculating eGFR."
67,67,484,38039090,https://www.doi.org/10.1681/ASN.0000000000000231,https://journals.lww.com/,"We thank Drs. Chen and Liang1 for their very relevant comments on our manuscript.2 In regard to kidney biopsies, we agree that analysis of pathologic abnormalities could have allowed us to definitively exclude patients with nondiabetic CKD3 and might further strengthen the between-group comparisons. However, as described, histologic diagnosis was not a trial inclusion criterion, and only 1 of 16 enrolled participants had a prior kidney biopsy (which confirmed diabetic nephropathy). In our discussions during the design of the trial, we concluded that (1) the risk of performing a biopsy as part of the study protocol could not be justified and (2) the frequency of prior biopsy confirmation of diabetic nephropathy among patients attending our three clinical sites was too low to support adequate enrollment. We believe that other trial inclusion criteria, such as the presence of abnormal albuminuria and lack of clinical features suspicious for non–diabetes-associated CKD, minimized the likelihood that some enrolled patients had kidney diseases distinct from diabetic kidney disease and are consistent with many other therapeutic trials in diabetic kidney disease.,Regarding the lack of information on newer hypoglycaemic drugs, we have documented that only 1 of 16 participants was prescribed a sodium-glucose cotransporter 2 inhibitor at the time of randomization. We now also clarify that four participants in the placebo group and three in the ORBCEL-M group were prescribed a DPP4 inhibitor or glucagon-like peptide-1 receptor agonist at the time of enrollment. We recognize that protocolized inclusion of such agents and/or focusing on participants unable to tolerate them will be critical issues for future cell therapy trial designs.,In relation to the effects of mesenchymal stromal cell (MSC) therapy on the rate of decline of GFR, we believe that our results are generally consistent with those previously reported for allogeneic mesenchymal precursor cells during a shorter follow-up period.4 In Novel Stromal Cell Therapy for Diabetic Kidney Disease (NEPHSTROM) cohort 1, the rate of decline of measured GFR was numerically but not significantly lower for recipients of ORBCEL-M compared with placebo over 18 months of follow-up, while similar trends for eGFR were statistically significance. For lipid parameters, the lack of significant changes after MSC therapy was unsurprising to us in this prospectively followed, placebo-controlled clinical trial cohort and likely reflects that the trial participants had good, stable lipid control at the time of enrollment.,Finally, although our findings document the safety and tolerability and confirm low potential for this MSC-based cell therapy to sensitize recipients, we acknowledge that the small sample size of NEPHSTROM provides only preliminary evidence of its potential renoprotective and immune modulatory/anti-inflammatory effects. Thus, we fully agree with Chen and Liang that the larger numbers of participants and incorporation of other design features will be necessary to validate these findings in future trials."
68,68,485,37678741,https://www.doi.org/10.1053/j.ajkd.2023.07.010,https://linkinghub.elsevier.com/,"Lyu et al1 evaluated discrepancies between different glomerular filtration rate (GFR) estimating equations among 30,261 patients with atrial fibrillation. In patients with low body mass index (BMI), they observed that GFR was lowest according to estimated creatinine clearance (eCrCl), highest according to estimated GFR (eGFR) indexed to body surface area, and intermediate according to nonindexed eGFR. In patients with high BMI, they observed the opposite trend. However, this conclusion is a mathematical consequence of the equations themselves and does not require any analysis of patient data.2,Lyu et al acknowledge that their study is limited by the lack of data for drug clearance, which remains an issue in this field of research.3 Alternatively, drug clearance can be modeled by measuring plasma clearance of an exogenous tracer (mGFR). We recently performed this kind of study among 120 older adults presenting to the Emergency Department of Hvidovre Hospital in Copenhagen, Denmark.4 Patients were excluded from analysis for prior amputation, use of an immunosuppressant, or acute kidney injury at the time of GFR measurement (defined by changes in serum creatinine from baseline). From the resulting population, we identified 25 patients (48% female, median age 78 years, median BMI 28 kg/m2) with a diagnosis of atrial fibrillation and non–vitamin K antagonist oral anticoagulant prescription (Table 1).,We found that eCrCl had a lower proportion of estimates within 30% of mGFR (P30) compared to eGFR based on creatinine alone or in combination with cystatin C (apart from the 2021 creatinine-based CKDEPI equation, which had even lower P30). Importantly, this corresponded to a larger discrepancy in dosing recommendations for rivaroxaban. Our results corroborate the findings by Lyu et al and earlier studies demonstrating poor performance of eCrCl.5 We hope the abundance of studies in this area prompts the start of large studies comparing GFR estimating equations to drug clearance and their impact on clinical outcomes."
69,69,486,36795033,https://www.doi.org/10.2215/CJN.0000000000000092,https://journals.lww.com/,"The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) coronavirus disease 2019 (COVID-19) pandemic affected the entire kidney transplant system, from pretransplant evaluation, waitlist access, donation, organ utilization and transplantation through post-transplant outcomes, often in ways that exacerbated existing inequities.1 Kidney transplant recipients with COVID-19, affected by underlying comorbidity and chronic immunosuppression, have higher morbidity and mortality than nontransplant patients with COVID-19.2 As vaccines and treatment options for COVID-19 became available, outcomes improved overall, but complications remained high for kidney transplant recipients.1 Thus, while an end to the pandemic may be in sight for the general population, transplant recipients will require better prevention and treatment regimens. Kidney transplant candidates and recipients want to know “When will we get back to normal? Who can we trust to navigate the rapidly, changing landscape?” What have we learned in this pandemic to improve future outcomes?,During the first phase of the pandemic, as scarce hospital resources were directed to the care of patients with COVID-19, transplant practice was profoundly disrupted. Deceased donor kidney transplantation slowed, often limited to candidates with particular urgency. Living donor kidney transplantation, considered “elective” surgery in many centers, was generally halted to avoid risk of infection in living donors and recipients.3,Transplant programs made many adaptations to remain open. When critical transplant unit staff was redeployed to COVID-19 care units, remaining staff assumed wider responsibilities. Operating room time and availability, anesthesia services, and intensive care unit (ICU) beds all were severely limited. As the hospital emergency eased center by center, these resources were reconstituted. Determining the infectious status of donor organs and recipients was needed. A rebound in deceased donor kidney transplantation was facilitated when the Organ Procurement and Transplantation Network (OPTN) captured and reported organ donor SARS-CoV-2 testing results. Expanded online patient education and use of telehealth streamlined patient evaluation and follow-up in many centers.,In contrast with the recovery in deceased donor kidney transplantation, living donor kidney transplantation in 2020 and 2021 lagged behind 2019 levels, with particular disparities for Black patients.4,Decisions to continue or curtail transplantation during a pandemic or disaster should be evaluated continuously. Deceased donor kidney transplant should be considered an essential procedure to continue within the limits of local resources and patient safety. While living donor transplantation may be delayed in emergency times, vigilance is necessary to avoid exacerbating disparities in access to living donor transplantation.4 As knowledge and treatments evolve during an emergency such as a pandemic, transplant centers should continuously communicate with patients and general nephrologists. Critical transplant staff must be retained, even if temporarily reassigned to emergency duties, to ensure transplant program success during and after a pandemic or disaster.,Early in the pandemic, deceased donation and Organ Procurement Organizations processes fell dramatically. A decline in traumatic deaths nationally, and a dramatic rise in hospital and ICU admissions for COVID-19, limited opportunities for deceased organ donation. For the fewer available potential donor organs, obtaining consent from next-of-kin was challenged by limits on in-person contact. Inconsistent availability of COVID-19 testing in potential donors5 limited transplant surgery.,Before the advent of vaccines and effective antiviral therapy, COVID-19–positive donors were generally excluded from organ donation. Time-sensitive testing later informed organ procurement and placement, and cases of kidney allograft use from SARS-CoV-2–positive donors emerged.6 Short-term outcomes suggest that transplantation from well-selected SARS-CoV-2–infected donors may be as safe and effective as from noninfected donors.7 Emerging experience also demonstrates safety of kidney transplantation in select COVID-19 recovered patients.,Rapid development and availability of reliable and time-sensitive deceased donor infection testing, along with infection reporting by the OPTN, are critical to make organ acceptance decisions during a pandemic. In the future, prompt monitoring, data analysis, and reporting will help determine when organs from donors affected by novel infections can be safely used for kidney transplantation and when affected candidates can proceed to transplant.,Immunocompromised patients were excluded from every major COVID-19 clinical trial, including vaccines. Without safety and efficacy data for transplant patients, clinicians extrapolated from the general population with COVID-19.1 Access to vaccination, medications, and general pandemic procedures varied by geography, which led to regional practice variations in immunosuppression modification, prophylactic interventions, and other treatments. Information sharing and collaboration among transplant institutions worldwide increased during the COVID-19 pandemic. This unparalleled cooperation helped guide individual practice when evidence was scarce.,With only limited short-term observational data on remdesivir for patients with kidney disease, this antiviral was initially considered contraindicated. More recent data suggest that remdesivir reduces ICU mortality without significant nephrotoxicity in kidney transplant recipients. Other empiric therapies were instituted that were often unhelpful and, sometimes, harmful.1 Although the OPTN added COVID-19 as a cause of death, it did not capture patient-level information related to vaccinations, treatments, or changes in immunosuppression. This limited database made it difficult to correlate clinical care with outcomes. As clinicians did their best to draw conclusions, kidney transplant patients felt confused and asked for trusted sources of information.,When evidence is scarce during an emergency, collaboration among transplant institutions is extremely valuable. In the absence of randomized controlled trials and robust observational data, patient care must continue and patient questions must be answered. The use of “lesser quality” data from surveys of national practice patterns and experiential medicine, with continuous tracking and reporting, can generate best available information to inform practice. More robust and rigorous national registries and tools to rapidly analyze data may inform clinical practice sooner. Health care agencies and providers must provide ongoing authoritative information to patients in a rapidly changing landscape. The impact of building trust with the patient community is invaluable to get “buy-in” from the patients we serve, especially during times of uncertainty. In addition, we must recognize and manage the lasting mental health consequences of a pandemic for patients and providers.,While the emergence of COVID-19 vaccines played a major role in reducing the risk and severity of COVID-19 in the population at large, transplant recipients had many questions regarding vaccine efficacy, safety, and reactogenicity. Because few immunocompromised patients participated in clinical trials, most data gathered in this population came from local registries and patient-initiated sharing. These data influenced our understanding of “fully vaccinated,” the need and timing of booster immunization, and need for pre-exposure prophylaxis in immunosuppressed transplant patients who may not have adequate immune responses to vaccination.,Data analysis now shows that vaccination is associated with reduced mortality in transplant patients.2 Professional societies strongly recommend COVID-19 vaccination for transplant candidates before transplant and immunosuppression when possible8 and, given the ongoing emergence of new variants, continue to advise that vaccinated transplant recipients maintain personal measures to minimize SARS-CoV-2 exposure. While all US transplant programs encourage vaccination, programs exhibited significant heterogeneity in COVID-19 vaccination mandate policies, citing administrative opposition, legal prohibitions, and concern about equity in access to transplants.9,Preventative and treatment regimens for solid organ transplant recipients require careful application and re-evaluation during a pandemic. Transplant clinicians should communicate that understanding with our patients. Excluding immunocompromised patients and delays in funding drug and vaccine studies were major “misses” during this pandemic that need to change in the future. Immunocompromised individuals should be included in studies of new therapeutic agents.,The OPTN made proactive changes during the pandemic in response to community requests. Programs were allowed to apply for retroactive waiting time modification for candidates unable to obtain timely testing required for registration. The OPTN also allowed temporary inactivation of candidates for reasons related to COVID-19 precautions without loss of waiting time and temporarily paused some data form submission requirements. The Centers for Medicare & Medicaid Services facilitated ongoing transplant surgery through Tier 3b designation and temporary relaxation of telemedicine restrictions to facilitate access to care and reduce possible exposure to SARS-CoV-2 during in-person visits. The Scientific Registry of Transplant Recipients conducted a data “carve out” from performance reports from March 13, 2020, to June 12, 2020, when “all hands on deck” were devoted to clinical care rather than data acquisition.10,Regulators, payers, and policy makers must recognize that at times of emergencies, priority must focus on care and accessibility and adjust reporting requirements accordingly. Reporting waivers and telemedicine played major roles in making transplant care more accessible during the pandemic and should be repeated in future emergencies.,Kidney transplant recipients have a high risk of adverse outcomes during the COVID-19 pandemic and continue to be at risk. Immunocompromised patients were not included in studies of vaccines and therapeutics, so safety and efficacy were unknown. Survey and experiential data helped direct best care. For future pandemics, timely development of screening tools and wide availability of early testing will be invaluable in decisions for organ acceptance and transplantation. Drug and vaccine development should include this population. Legislators and regulators should work with clinicians to adapt health care policies and procedures to changing needs and remain trusted sources of information for patients and caregivers in future health care crises."
70,70,488,37717845,https://www.doi.org/10.1053/j.ajkd.2023.06.008,https://linkinghub.elsevier.com/,"In the United States, glomerular filtration rate (GFR) is commonly estimated using serum creatinine and the 2021 Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) equation for individuals older than 18 years or the 2021 Chronic Kidney Disease in Children Under 25 Study (CKiD-U25) equation for those between 1 and 25 years of age with CKD (Item S1).1,2 These equations may result in different estimated GFR (eGFR) values at 18 years and older, leading to uncertainty in assessment of severity of disease, progression rate, and clinical decisions based on level of GFR. The CKiD-U25 has not been externally validated in a diverse population of young adults.,We compared the CKD-EPI and CKiD-U25 equations in young adults prior to the generally accepted age-related GFR decline (aged 18-40 years) in the 2023 CKD-EPI creatinine external validation dataset (1,491 participants from 21 studies) with measured GFR (mGFR) using urinary or plasma clearance of exogenous filtration markers (Item S2, Tables S1 and S2, Fig S1).1,2 We hypothesized that the CKiD-U25 equation would perform better in young adults with lower GFR, similar to the population in whom the CKiD-U25 equation was developed (mean GFR of 49 [SD 23.0] mL/min/1.73 m2), compared to those of older age and higher GFR, similar to populations in whom the CKD-EPI equation was developed (mean GFR of 67.6 [SD 39.6 mL/min/1.73 m2]). We evaluated bias and precision (median and interquartile range of the difference between mGFR and eGFR, respectively), and accuracy (percentage of eGFR within 15% or 30% of mGFR, agreement of eGFR to mGFR categories).1,3,4 In sensitivity analyses, we calibrated mGFR to account for potential differences between measurement methods in validation versus the development datasets (Table S3).5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21 We also evaluated performance of the European Kidney Function Consortium (EKFC) equation, which can estimate GFR across the full age spectrum, but was developed in a predominantly white population (Table S2).22,Mean (SD) age was 31.7 (6.0) years and mean (SD) mGFR was 92.7 (32.7) mL/min/1.73 m2 (Table S4). Younger age was associated with higher mGFR (Fig S2). The equations provided similar estimates for participants with eGFR less than 60 mL/min/1.73 m2. At higher values, CKD-EPI yielded generally higher GFR estimates (Fig 1, top panel). Magnitude of the difference in eGFR between equations was larger at younger age and shorter height (Fig S3).,For the CKD-EPI equation, there was minimal bias between mGFR and eGFR overall (−0.5 [95%CI −1.5 to 0.7] mL/min/1.73 m2), with small variation by GFR (Fig 1, middle panel, Fig S4, Table S5). In contrast, the CKiD-U25 equation moderately underestimated mGFR overall (7.2 [6.1, 8.3] mL/min/1.73 m2), with large underestimation at higher levels of eGFR (Fig 1, bottom panel, Fig S4, Table S5). There was greater variation by age groups with CKiD-U25 than CKD-EPI, with greater underestimation at younger adult ages (Table 1). The CKiD-U25 equation also had greater underestimation, compared to CKD-EPI, across sex and race groups as well as body mass index (BMI) >20 kg/m2, but smaller bias for the BMI <20 kg/m2 group (Table S6). P30 was similar for both equations in all subgroups, except for BMI <20 kg/m2, in which P30 was higher for the CKiD-U25 equation. Adjustment for possible differences in measurement methods for GFR attenuated the bias in CKiD-U25 (Table S7). The EKFC equation underestimated mGFR compared to the CKD-EPI equation (Tables S6-S8 and Fig S5) and was similar to CKiD-U25.,For young adults with CKD, the transition from pediatric to adult care can occur over a wide age range. In addition, young adults without previously diagnosed CKD may have need for evaluation of GFR. Providers have choices for GFR estimation in these settings. In this study, we found that the CKiD-U25 equation, developed in children and young adults with CKD, had minimal bias in young adults with lower GFR, similar to the CKD-EPI equation, but underestimated mGFR at higher values. The CKD-EPI equation had consistent performance across GFR and age subgroups. In contrast, the EKFC equation performed similarly to the CKiD-U25 equation, as was noted in a European cohort of young adults with higher GFR.23 Differences between study populations in which the equations were developed, especially level of GFR, should be considered when using these equations in clinical practice.2,Strengths of this study are the diverse population across range of GFR, disease, and race group, separate from the population in which the equations were developed. A limitation is that the healthy individuals in CKD-EPI development and validation populations included people with type 1 diabetes or kidney donor candidates, who may differ from young adults in the general population.,The results support use of the 2021 CKD-EPI equation for reporting of eGFR by clinical laboratories in individuals older than 18 years of age. For young adults with childhood CKD, our results support continuing use of the CKiD-U25 equation to maintain consistency of eGFR. This study reinforces the need for additional research in young US adults to resolve differences observed at high levels of GFR and refine recommendations for use of eGFR equations.,Download : Download Acrobat PDF file (2MB)Supplementary File (PDF). Figures S1-S5, Items S1-S3, Tables S1-S8."
71,71,489,37972815,https://www.doi.org/10.1053/j.ajkd.2023.10.006,https://linkinghub.elsevier.com/,"We thank Delanaye and Pottel for their response letter,1 which raises some important points. Our main goal was to assess the relative accuracies of estimated glomerular filtration rate (eGFR) equations based on creatinine, cystatin C, beta-trace protein, and beta-2 microglobulin in hospitalized older adults. We found that the addition of cystatin C to creatinine-based equations improved accuracy, whereas the addition of beta-trace protein and/or beta-2 microglobulin did not. This corroborates earlier literature, but it is one of only a few such studies among elderly multimorbid patients. These patients are at a disproportionately high risk of inaccurate GFR estimates and adverse outcomes related to such errors,2 but they are under-represented in the development of most modern eGFR equations.,We agree that the 2023 European Kidney Function Consortium (EKFC) creatinine–cystatin C combination equation should be considered. Our article was already through peer review when this equation was published, but subsequent analysis shows that it performs similarly to the 2012 Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) combination equation in our cohort (Table 1). We think it is clear from these data that both CKD-EPI and EKFC outperform their respective FAS equations. Whether EKFC is more appropriate in Europeans is a topic of ongoing debate.3,Regarding our use of 99mTc-DTPA plasma clearance as the reference method, this is the current clinical standard within Denmark and the only option available at most clinical laboratories. Delanaye and Pottel reference a review by Soveri et al4 stating that DTPA plasma clearance is inaccurate, but several studies contradict this conclusion.5 As it happens, we are currently investigating other methods, including iohexol plasma clearance.6"
72,72,490,38087445,https://www.doi.org/10.1681/ASN.0000000000000288,https://journals.lww.com/,"As doctors and health care providers, we focus on both individuals and populations and gain insights from clinical experience and published literature and use that combination of experience and knowledge to predict events in the future to improve the outcomes of our patients.,Ideally, our thoughtful application of all information available to us will help to optimize the initiation of medications, enable informed discussion around decision making, and risk-stratify individuals or groups so that resources used to care for them are appropriately allocated.,Predicting outcomes in CKD is important from several perspectives: in the clinical realm for individuals, in health policy to inform strategies and resource allocation, and in research to improve enrollment and inform sample size.,The outcomes we choose to predict include short term within 2–5 years, medium and longer term >5–10 years, and are usually those outcomes for which data to predict are easily measured or ascertained from medical records. In the nephrology world, we have been preoccupied with predicting progression to kidney failure or death and to some extent cardiovascular events because these are the outcomes most relevant to our patients. We recognize the very long horizon that this chronic condition has and that events occur throughout the life cycle and may have different implications at different points in time for individuals and health care systems.,In medicine, the most used prediction equations include Framingham Risk Score (10-year CV risk) and the Congestive Heart Failure, Hypertension, Age and Diabetes score (risk for stroke in nonrheumatic A fib), and others include Respiratory Failure Risk Index (postoperative respiratory failure risk) and Fracture Risk Assessment Score (10-year osteoporotic fracture risk).1–4 The addition of kidney variables to Systematic Coronary Risk Evaluation and Probability of Cardiovascular Event scores, which predict CV events in the general population, has been used to further optimize CV risk assessment in those with CKD.5 Note that some CV prediction models were developed in very remote cohorts and were not always internationally representative, but nonetheless are used in clinical practice ubiquitously. The 2023 Scientific Statement from the American Heart Association has suggested new sex-specific race-free risk equation for 10-year and 30-year estimates for total cardiovascular disease including eGFR, with other models that add in other factors such as urine albumin creatinine ratio hemoglobin A1c, and social determinants of health.6,In nephrology, we have the KFRE (kidney failure risk equation with 2-year and 5-year risk for kidney failure), Hemodialysis Mortality Risk (6-month mortality risk in those on maintenance dialysis), and the Kidney Donor Risk Index (risk of kidney failure in donor after donation).7–9 All of these tools have been developed using the best methodology at the time and externally validated.,However, despite the development of several tools to predict events that are important to us and to our patients, the uptake of them into clinical practice is quite poor. Note that although the Framingham risk equation predicts risk of only >10% of a CV event in 10 years, it has been used to initiate statin use for decades. Contrast that with KFRE which predicts kidney failure requiring dialysis or transplantation within 2 or 5 years: A very dire event within a short time frame and yet still not heavily used in clinical practice even a decade after extensive validation in numerous populations around the world.,Multiple performance metrics for prediction equations exist to help us evaluate them: discrimination, calibration, Net Reclassification Index (NRI), Integrated Discrimination Index, and net benefit. Discrimination refers to the predicative ability of the model for an event of interest, calibration is a measure which determines agreement between observed and predicted outcomes, NRI is a sum of the differences between appropriate and inappropriate reclassification, Integrated Discrimination Index is a sum of NRI over all the possible cutoffs for the possible outcomes, and net benefit assesses the usefulness of a prediction model in clinical decision making. Each of these performance metrics has benefits and shortcomings, and perhaps given the mathematical nature of them, they are not well understood in the context of clinical practice. As pointed out in the article by Milders et al., in this edition of the journal, there is large variation in the quality of reporting and model development and not all these metrics are reported.10 The scoping review describes the publication of novel models, external validation of existing models, and updating of models used in CKD, dialysis, and transplant populations. The authors note an underrepresentation of patients from Africa, South America, and Australia, which may limit their applicability in those regions. They note that models for predicting patient-reported outcomes (like quality of life or life participation) are scarce or nonexistent and that often sample sizes are small, reporting guidelines not adhered to, and is some, no, or inappropriate performance metrics reported. Perhaps one of the most important findings was that very few of the models published were presented in a useable format (regression formula or risk score) which hampers both validation and subsequent implementation.,In the past 7 years, in the kidney space, a variety of authors have described the potential value of using prediction models in clinical practice. Potok et al. described the improved accuracy of KFRE in predicting dialysis needs in a cohort of patients which exceeded the estimates of the physicians11; others have examined the utility of KFRE as adjuncts to optimizing the timing of vascular access.12 The use of components of kidney function (eGFR and albuminuria) in different risk equations to facilitate decision making for different outcomes (kidney failure, CV events, AKD, and death) has been described.5,A key question is why there is a poor uptake of prediction tools in nephrology: Do we not trust them, have we not spent time understanding when and how to use them, or do we doubt their utility in individual circumstances? Do we need to develop implementation programs to demonstrate their utility in clinical practice and that their use leads to better decision making or outcomes for patients or health care systems? Until recently there were few interventions to delay progression of CKD or effectively prevent CV events: So have we been reluctant to use prediction tools because of our inherent nihilistic attitude that there is little we can do, so why bother to predict?,How do we bridge the gap between promising prognostication models that may help one to identify people at risk for specific events, clarify time points in disease trajectory for decision making or intensified therapy, and optimize outcomes for individuals and optimize use of limited resources within health care systems?,With the advent and increasing availability and sophistication of electronic medical records (EMRs) worldwide, embedding the risk equations into EMRs with decision thresholds and proposed action plans seems an obvious and reasonable approach. Where EMRs do not exist, or cannot embed the tools, easy-to-access downloadable applications onto smart phones should be accessible to all. However, perhaps the step before that is to convince nephrologists and non-nephrologists that using these prediction tools actually does facilitate care and can improve efficiency and appropriateness of interventions. In clinical practice, we need to demonstrate that these tools improve clarity of conversations with patients and colleagues, lead to improved timing of access referral, or referral for dialysis education or transplantation. We would need to examine the best way to use the tools, and how best to communicate numeric risks to patients and colleagues. To date this aspect of implementation has been understudied. In research, perhaps including risk prediction equations as part of eligibility criteria for specific studies would help with recruitment and enrichment of study cohorts with those with a higher likelihood of the outcomes of interest. The socialization of the potential utility of prediction models remains a challenge, although their use is encouraged in the upcoming guidelines for the management and evaluation of CKD.,We should invest in research to test the utility and performance of existing clinical prediction models in diverse patient populations and in different health care systems to inform care pathways and decisions. How and if patients and clinicians accept the use of these tools and if they would accept their actions being guided by them remains unknown and requires study. Without understanding both components (true impact on decision making or outcomes and acceptance by patients and clinicians), the call for widespread implementation of validated prediction models will not be heard. Refocusing research efforts on evaluating the effect of using both existing and patient-centered prediction tools may be the value proposition required to truly improve clinical outcomes. In parallel, evaluating the impact of using them in clinical trials, to streamline study enrollment and execution, may also encourage their use in clinical practice."
73,73,491,37030585,https://www.doi.org/10.1053/j.ajkd.2023.01.453,https://linkinghub.elsevier.com/,"Different glomerular filtration rate (GFR) estimating equations are used in drug development and clinical practice. Traditionally, United States Food and Drug Administration (FDA) dosing recommendations for drugs cleared by the kidneys were based on the Cockcroft-Gault equation,1 which is less accurate than the Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) equation.2,3 The CKD-EPI equation is indexed to body surface area to facilitate comparison across individuals of different body size. However, because kidney clearance of drugs within individuals correlates better with nonindexed estimated GFR (eGFR) than indexed eGFR, nonindexed eGFR is recommended to guide optimal drug dosing.3,4 We evaluated when these GFR estimates differ, and how they may compel differences in drug dosing, using a population with atrial fibrillation and rivaroxaban as an example drug.,We included 30,261 patients from Geisinger Health System with a diagnosis of atrial fibrillation (Table S1) and available serum creatinine, weight, and height measurements (Fig S1, Item S1). GFR was estimated using estimated creatinine clearance (eClCr) by the Cockcroft-Gault equation (mL/min, actual body weight),1 indexed eGFR by the CKD-EPI (2021) equation (mL/min/1.73 m2),5 and nonindexed eGFR (mL/min) by accounting for individual’s body surface area (Table S2) and compared by body mass index (BMI) and age groups. The recommended dose of rivaroxaban based on the 3 estimates was evaluated in patients with indexed eGFR 15-60 mL/min/1.73 m2. In secondary analysis, we also evaluated eClCr using adjusted body weight.6,Mean age of the cohort was 76.4 (SD, 12.4) years, 45.5% were female, and mean BMI was 29.0 (SD, 7.5) kg/m2 (Table S3). Patients with higher BMI category had higher eClCr and nonindexed eGFR, with more substantial increase for eClCr (P for trend <0.001 for both).,The distribution of eClCr, indexed eGFR, and nonindexed eGFR differed by BMI (Fig 1) and age (Fig S2). Different GFR estimates had good agreement with BMI 25-39.9 kg/m2 and age 65-84 years. In contrast, among patients with low BMI, eClCr was lower than nonindexed eGFR, which was lower than indexed eGFR. Among patients with high BMI, mean eClCr was higher than indexed and nonindexed eGFR (110.0 mL/min vs 65.3 mL/min/1.73 m2 vs 86.9 mL/min, respectively). In patients aged ≥85 years, eClCr was lower than indexed and nonindexed eGFR (38.5 mL/min vs 51.6 mL/min/1.73 m2 vs 51.9 mL/min, respectively).,There were large differences in recommended rivaroxaban dosing. Among 13,015 patients with eGFR 15-60 mL/min/1.73 m2, people with low BMI often required a standard dose of rivaroxaban by indexed and nonindexed eGFR and a reduced dose by eClCr (33%-46% by indexed eGFR and 5%-17% by nonindexed eGFR for BMI <18.5 kg/m2, depending on age; Fig 2). In contrast, people with high BMI often required a reduced dose of rivaroxaban by indexed and nonindexed eGFR and a standard dose by eClCr (46%-62% by indexed eGFR and 12%-21% by nonindexed eGFR for BMI ≥40 kg/m2). Patients with low BMI often required a standard dose of rivaroxaban by indexed eGFR and a reduced dose by nonindexed eGFR; conversely, high BMI required reduced dose by indexed and standard dose by nonindexed eGFR.,The distribution of eClCr using adjusted body weight was more similar than eClCr using actual body weight when compared with indexed and nonindexed eGFR (Fig S3), but substantial differences in recommended rivaroxaban dosing remained (Fig S4).,As demonstrated by our data and prior studies,7,8 the decision to use eClCr or eGFR, and whether to use indexed or nonindexed eGFR, affects dosing recommendations, particularly among people with higher BMI and older age—a subset of the population that is growing. Determining the optimal dose is critical, as inappropriate dosing of rivaroxaban may lead to increased mortality.9 Large-scale outcome studies could inform these decisions as to which method better differentiates benefit and risk of adverse drug events, but are not likely to occur for all medications and clinical settings. Consistent with past recommendations,3,4 use of the CKD-EPI equation, the more accurate and widely used GFR estimating equation, may facilitate harmonization of GFR estimates between drug labeling and clinical practice. Draft industry guidance from the FDA now recommends use of the CKD-EPI equation for pharmacokinetic studies.4 Indeed, new drugs, such as dapagliflozin, have incorporated indexed eGFR in pharmacokinetic studies and include eGFR-based dosing recommendations in the FDA-approved label.10,The strengths of our study include a systematic assessment of GFR distributions and dosing discordance by 3 estimating equations in a large real-world patient population. However, we did not examine actual drug clearance or drug safety and efficacy, and our results may not be fully generalizable to other populations.,In conclusion, among patients with atrial fibrillation, eClCr, indexed eGFR, and nonindexed eGFR differ by body size and age, especially among people with very high BMI and older age, resulting in substantial drug dosing discordance. Using the most accurate method to assess GFR in individual patients may be a preferred approach.,Download : Download Acrobat PDF file (1MB)Supplementary File (PDF). Figures S1-S4, Item S1, Tables S1-S3."
74,74,492,37678742,https://www.doi.org/10.1053/j.ajkd.2023.08.002,https://linkinghub.elsevier.com/,"We thank Iversen et al1 for their commentary on our article. Kidney function is a critical consideration for drug development, approval, and use, as it alters pharmacokinetics and pharmacodynamics. Glomerular filtration rate (GFR), the best overall index of kidney function, is complicated to measure directly. Thus, GFR is estimated in most settings. Different methods to estimate GFR are used in drug development and clinical practice. Such inconsistencies may result in confusion and discordance in drug dosing. In our study,2 we used a large patient population and demonstrated the real-world implications of inconsistencies among estimated creatinine clearance (eClCr by the Cockcroft-Gault equation), indexed eGFR (CKE-EPI 2021 equation), and nonindexed eGFR, especially among people with very high body mass index (BMI) and old age.,The Cockcroft-Gault equation has been traditionally used for pharmacokinetic studies and drug labeling. However, as supported by Iversen et al’s letter, one limitation of the Cockcroft-Gault equation is its lower accuracy compared to the CKD-EPI equation. Choosing the appropriate GFR estimating equation for dosing decisions is challenging for drugs that use eClCr for labeling, especially among people with older age. This may have a large impact with an aging population.3 Observational outcome studies may help inform the proper choice of GFR estimating equations. For example, outcome studies prompted the US Food and Drug Administration to change metformin renal dosing recommendations from a serum creatinine–based to eGFR-based threshold.4 Further large-scale studies to assess the performance of CKD-EPI equations are needed to guide renal drug dosing, especially among people with higher BMI and older age."
75,75,493,37459117,https://www.doi.org/10.2215/CJN.0000000000000256,https://journals.lww.com/,"Advanced CKDs are more common in Black people than in other populations. Current and historical social inequities, driven by policy, economic structure, and environmental factors, are critical drivers of this disparity.1 However, pathobiology also contributes. Kidney failure incidence in Black compared with White men is significantly higher, regardless of systolic BP level and after adjustment for some social factors.2 Given the familial clustering of kidney diseases, genetic studies were one of the designs to discover the candidate, biological causes of this excess kidney disease risk in Black people. A specific locus on chromosome 22 was associated with common, nondiabetic kidney disease in Black but not non-Black people. Two genetic variants in the APOL1 gene, which changes its protein coding sequence, accounted for the association signal.3,4 This association has been replicated in studies of participants with both categorical and continuous CKD phenotypes. Importantly, APOL1 high-risk genotypes are found only in some individuals of African ancestry and are associated with hypertension-associated kidney disease, the second most common cause of kidney failure. Two copies of APOL1 genes with kidney risk variants greatly increase kidney disease risk. However, most carriers of the APOL1 high-risk genotypes do not develop kidney diseases, indicating additional stresses, most yet not all definitively established and some likely social factors, are required for disease development. Finally, APOL1 both circulates and is synthesized by kidney cells, but studies of kidney transplant recipients demonstrate that kidney injury is a result of locally produced APOL1.,APOL1 is an innate immune effector molecule that confers protection against African trypanosomes in humans and some non-human primates. We now know that kidney disease–associated APOL1s cause trypanolysis of species, which cause African sleeping sickness4 by forming pores in membranes that permit movement of cations. In vitro work established that variant APOL1s also form pores in mammalian cells and caused cytotoxicity because of cation flux. Mice that expressed variant, but not reference, APOL1 transgenes under control of their human promoters develop kidney disease, providing evidence for their causality.5,6,These and other studies have identified APOL1 function as a credible target for novel therapies to reduce some of the public health burden of kidney disease borne by Black people. In a recent publication, Egbuna and colleagues report preclinical data and the results of a small, phase 2 trial that tested the effects of a small molecule blocker of APOL1 pore function, inaxaplin.7 Inaxaplin blocked ion flux in cells treated with tetracycline to stimulate APOL1 expression from a transgene and reduced albuminuria in a transgenic APOL1 mice model of proteinuric kidney disease. Sixteen participants with biopsy-proven FSGS were treated with inaxaplin in a single-group, open-label trial to test its safety and efficacy to reduce proteinuria. Thirteen participants, three with nephrotic range proteinuria, met predefined criteria for inclusion in the efficacy analysis and had a geometric mean reduction in proteinuria of −47.6% (95% confidence interval, −60.0% to −31.3%) after 13 weeks of inaxaplin treatment. Nine of these participants continued to be followed for 12 weeks off the drug and had a persistent reduction in proteinuria of approximately 30%. Adverse events were mild to moderate. No participant was discontinued from the study. On the basis of this trial, inaxaplin has been granted Breakthrough Therapy designation by the US Food and Drug Administration for FSGS in people with APOL1 kidney risk genotypes and Priority Medicines designation for APOL1-medicated CKD by the European Medicines Agency.,These results are remarkably impactful. In 10 years, our community has progressed from a seminal discovery, the association of APOL1 genetic variants in Black people with some nondiabetic kidney diseases, to successful completion of an early-phase clinical trial. The speed of this process is breathtaking and should be celebrated. While optimism is justified, it must be tempered. The published study is quite small, lacks a control group, and may be subject to unrecognized confounders. The persistent decrease in proteinuria after the end of active treatment suggests some benefit may have been derived from care delivery in a clinical trial. Changes in eGFR are not reported. Only participants with FSGS were enrolled, and inaxaplin efficacy remains an open question in individuals with other APOL1 kidney diseases, especially hypertension-associated CKD. Thirteen of the 16 people enrolled in the trial had mild-to-moderate interstitial fibrosis. Early biopsy and diagnosis may be critical to optimize inaxaplin efficacy, although encouragingly, the three individuals with severe fibrosis still had significant proteinuria reduction. On the basis of these outcomes, a blinded, randomized trial, with an adaptive design, has been initiated to definitively test the efficacy of inaxaplin in APOL1 kidney diseases (NCT05312879). Nondiabetic Black people with proteinuric kidney diseases and APOL1 risk genotypes are eligible to enroll. Primary outcomes are change in proteinuria and eGFR slopes at specified times. A composite secondary outcome of sustained loss of eGFR >30%, kidney failure, or death is being ascertained. The trial results just reported have significant implications but represent an early step in the evaluation of inaxaplin. Further testing of its efficacy and safety will occur in the ongoing definitive trial. As a community, we must help ensure its successful execution by educating our patients about APOL1 kidney disease, an effort supported by the patient community, and encouraging those who meet inclusion to consider enrolling.,Other therapies for APOL1 kidney diseases are in the pipeline. The weight of the data demonstrates that variant APOL1s have a toxic gain of function, which causes tissue injury. Given this, reducing APOL1 synthesis should have benefit. The coronavirus disease 2019 vaccine highlighted the efficacy of RNA-based therapeutics. A RNA medicine, which blocks APOL1 synthesis by degrading it messenger RNA, reduced albuminuria in a mouse model of APOL1 kidney disease.6 A trial of this type of RNA medicine has been concluded in 30 healthy men, but no results have been released (NCT04269031). Cytokines stimulate APOL1 synthesis, and another trial will test the safety and efficacy of blocking cytokine-activated signaling pathways with the JAK/STAT inhibitor baricitinib8 (NCT05237388). As basic and translational research progresses, other targets for APOL1 kidney diseases may emerge. A small molecule suppressor screen is ongoing to identify small molecules that block APOL1 protein synthesis.9 Interesting preclinical data demonstrate that variant APOL1 activates the cytosolic stimulator of interferon genes and NOD-like receptor family pyrin domain-containing 3 pathway.10 Agents that block specific components of the NLR family pyrin domain-containing 3 pathway are in clinical trials as treatments of other diseases and could be repurposed for the treatment of APOL1 kidney diseases if additional evidence justifies targeting this pathway. In anticipation of successful trials, we need to prepare ourselves and our trainees to make culturally sensitive, shared decisions with our patients about proceeding with APOL1 genetic testing to guide their CKD treatments.,This is time for our community to celebrate. After years mired by limited therapeutic innovation, a pipeline of clinical trials is testing drugs targeting molecular mechanisms, revealed by years of bench and translational research. As a community, we have to step up. First, we must get the word out to our internal medicine trainees that kidney medicine is alive and well and is using cutting-edge approaches to bring new diagnostic and therapeutic approaches to the clinic. We need the best and brightest to join these efforts. In addition to the new inaxaplin trial, we must also support other clinical and translational research initiatives using patient cohorts to identify targeted therapies and test their efficacy for kidney disease treatments. For example, the NEPTUNE Match Trial (NCT04571658) will identify the molecular mechanisms in patients with nephrotic syndrome by measuring urinary biomarkers of pathway activity and determine whether these participants can be matched to ongoing trials of drugs that target those pathways.,The inaxaplin trial is a remarkable milestone that tests a targeted therapeutic, on the basis of molecular mechanisms, for common kidney diseases, specifically affecting people of African ancestry. However, successful clinical trials of novel treatments of kidney diseases will only address part of the root cause of the kidney disease burden. The edifice of structural racism must be challenged and dismantled to ultimately reduce the excess burden of kidney and other common chronic diseases in communities of color. The intensive efforts invested in translational sciences, which have bought a targeted therapy for APOL kidney disease in 10 years, must also be directed to reducing the social factors that perpetuate health disparities. With people with kidney disease as our partners, we have the scientific momentum and shared purpose to meaningfully affect kidney health. Now is the time; delay is not an option."
76,76,494,37490295,https://www.doi.org/10.2215/CJN.0000000000000266,https://journals.lww.com/,"After metformin and sodium-glucose cotransporter-2 inhibitors, glucagon-like peptide-1 receptor agonists (GLP-1 RA) are a preferred agent to manage type 2 diabetes in patients with CKD.1 Despite this, their use is lower among those with than without CKD.2 We evaluated factors associated with GLP-1 RA prescription and facility-level variation in prescription patterns among patients with CKD, type 2 diabetes, and atherosclerotic cardiovascular disease who would likely have multiple indications for these medications.3,We identified adult patients with comorbid CKD, type 2 diabetes, and atherosclerotic cardiovascular disease using data from the Veterans Affairs (VA) Corporate Data Warehouse, reflecting real-world health care at 130 VA facilities and their affiliated outpatient clinics across the United States.4 We included individuals with an in-person or telehealth visit (index visit) with a primary care provider (PCP) between January 1, 2020, and December 31, 2020. Patients with an eGFR <60 ml/min per 1.73 m2 were considered to have CKD. Type 2 diabetes was defined by diagnosis codes, laboratory values, and medications as has been previously described.4 Patients with type 1 diabetes were excluded. Atherosclerotic cardiovascular disease was ascertained using diagnosis, procedure, and Current Procedural Terminology codes for ischemic heart disease, ischemic cerebrovascular disease, or peripheral arterial disease.4 GLP-1 RA exposure was defined as a VA or documented non-VA prescription for dulaglutide, exenatide, liraglutide, lixisenatide, or semaglutide within 180 days before or 100 days after the index visit.,Multivariable logistic regression quantified associations between patient-level characteristics with GLP-1 RA prescription. Variation in prescription between individual VA facilities was assessed by median rate ratio and 95% confidence interval (CI), which quantifies the likelihood that two randomly selected locations differ in their use of GLP-1 RA among similar patients. Facility-level utilization rate was divided into tertiles to assess associations of facility-level characteristics with GLP-1 RA utilization.,Of 209,460 patients with CKD, type 2 diabetes, and atherosclerotic cardiovascular disease, 19,780 (9.4%) were prescribed a GLP-1 RA. Those prescribed a GLP-1 RA were younger, more likely to be White, and have hypertension, systolic heart failure, or ischemic heart disease (Figure 1A). They had a higher mean (SD) body mass index, more PCP visits in the year before the index visit, and more visits with cardiology, endocrinology, and nephrology, although most did not see these specialties at all. In the multivariable model, female sex, higher body mass index, higher hemoglobin A1c, concomitant diabetes medication use, ischemic heart disease, and more PCP or endocrinology visits were associated with higher odds of GLP-1 RA prescription. Black and Hispanic patients were less likely to receive GLP-1 RA than White and non-Hispanic patients, respectively (Figure 1A). The difference by race was present among men (Black versus White odds ratio, 0.65; 95% CI, 0.62 to 0.69) but not women (odds ratio, 0.95; 95% CI, 0.74 to 1.20), race x sex interaction P = 0.01. Across individual VA facilities, GLP-1 RA prescription rate varied significantly (Figure 1B). The adjusted median rate ratio (95% CI) was 1.77 (1.64 to 1.91), indicating an unexplained 77% variation in treatment with GLP-1 RA for similar patients treated at two random locations. Increasing tertiles of GLP-1 RA utilization at the facility level were not associated with facility size, proportion of patients at that site who were Black or women, whether the site was a teaching facility, or whether the site had access to a cardiologist, endocrinologist, or nephrologist.,We found that GLP-1 RA use was low at only 9.4% of patients with multiple indications for use. There were racial and ethnic differences in the use of these agents, with Black and Hispanic patients less likely to receive them. Our findings are consistent with other studies demonstrating racial and ethnic differences in GLP-1 RA prescription in large national samples of patients with diabetes.5–7 Our results extend these findings to patients with CKD in the VA health care system, where access to care, insurance coverage, and medication copays are lower barriers to prescription. These differences were independent of other patient-level factors, such as hemoglobin A1c or ischemic heart disease that were also associated with GLP-1 RA use, likely due to more overt indications for treatment or clinician comfort with these medications. This suggests that other factors, such as clinician prescribing practices, may influence these differences. There was also an unexplained 77% variability between facilities in GLP-1 RA prescription. Facility-level variation was not associated with VA site-level racial makeup and had no significant association with facility-level characteristics. Other factors, such as clinician practice patterns or patient-level factors, not incorporated in this analysis may contribute to the unexplained facility-level variation. Limitations include that albuminuria was not ascertained. This important risk factor for the progression of CKD may affect clinical decision making about the prescription of GLP-1 RA and other medications.,Overall, the use of GLP-1 RAs was low. Efforts are needed to improve utilization at the health care system level by addressing racial and ethnic disparities and facility-level variation in prescription patterns."
77,77,495,37707829,https://www.doi.org/10.2215/CJN.0000000000000323,https://journals.lww.com/,"Clinical trials and observational studies in nephrology often evaluate the effect of treatment on time-to-event outcomes. Typical statistical analyses of time-to-event data summarize the effect of treatment using the hazard ratio (HR), the ratio between the hazard rates in the treatment and control groups. The hazard rate at a certain time is the instantaneous risk of the outcome for participants who have been event-free until that time point. The HR is often estimated using Cox proportional hazards model, which requires that the HR is constant over the study's follow-up period. The HR summarizes into a single metric the contrast of two hazard rates that may change over time and, as such, has been the preferred estimator of the treatment effect for time-to-event outcomes. Unfortunately, the requirement that the HR is constant over time is often not met in real-world data. In such a case, the use of the HR is incorrect, although it is often the default summary metric of choice in randomized trials.1 Even when the proportional hazards assumption is acceptable, use of the HR to communicate the effect of a diagnostic test or treatment is problematic for clinical decision making. This is because the HR, and probabilities more generally, are not easily understood.2 For example, if the baseline risk is low, then a HR of 0.5 may not translate into a substantial benefit. If the baseline risk is high, then a HR of 0.8 may be very clinically meaningful. Alternative metrics to summarize survival data, such as the restricted mean survival time (RMST), have been proposed but are underused.3,4 Here we describe the statistical properties of the RMST, illustrate its use in the context of the Systolic Blood Pressure Intervention Trial (SPRINT), and highlight particular advantages of the RMST metric in clinical research.5,The RMST is defined as the expected (or average) survival time in a patient population, on the absolute scale, over a fixed period of time (hence, restricted to a time period of interest). Because it is on the absolute scale, the estimate of the RMST is provided for an appropriate time unit (e.g., days, months, or years) and is obtained by calculating the area under the Kaplan–Meier survival curve, from the index time to the time point of interest (also called a time horizon; Figure 1). The RMST difference, defined as the difference in RMSTs between the two treatment groups, is an estimator of the treatment effect that captures the average delay in the occurrence of the clinical event of interest between the two groups. While the choice of time horizon may seem arbitrary, recent work has demonstrated that inference about the RMST can be made up to the minimum of the largest follow-up times in either arm of the study (either observed or censored), reducing the subjectivity of this choice; as such, extrapolation of the RMST difference outside the time horizon is not recommended.6 The estimation of the survival curve is straightforward in a randomized control trial where covariate balance is achieved by randomization. However, in observational studies, regression models are often needed to adjust for confounders. In these settings, obtaining adjusted estimates of the RMST is more challenging, though not impossible. For example, an analysis of covariance model can be used to adjust RMST estimates for baseline covariates.7 More generally, the RMST, or RMST difference, can also be estimated using any popular regression model (e.g., Cox proportional hazards model) because the RMST can be calculated using any estimator of the survival curve.8 When using model-based estimators of the survival curve however, all the assumptions and limitations of the model still apply. In the specific case of the Cox proportional hazards model, these include the assumption of proportional hazards and noninformative censoring, i.e., that censor times are independent of event times conditional on observed covariates. The RMST may be particularly useful as a single measure when fitting complex models with time-varying treatment effects, which might be needed when the proportional hazards assumption is not met. In cases when reporting of a single HR is inappropriate because the HR function measuring the treatment effect is time-varying, a single RMST difference is still a well-defined and interpretable metric of the treatment effect. Moreover, the RMST difference can have a valid interpretation even in the setting of crossing survival curves. More importantly, the RMST difference has a causal interpretation, while that is not necessarily true for the HR.1,9 The hazard at a time point describes the instantaneous probability of the event of interest among the subgroup of patients who have survived until that time point. The HR is the ratio of two hazard functions under treatment versus under control. If treatment affects the outcome, the subgroup of patients surviving until a given time point in the treatment arm may be systematically different from those in the control arm, and thus, the instantaneous risks in two systematically different subpopulations are not directly comparable. In that sense, the HR loses a causal interpretation, even in a randomized control trial. For example, suppose a particular treatment saves the lives of women, but not men. Then at an arbitrary time point, the proportion of women in the treatment arm will be higher than the proportion of women in the control arm, resulting in obvious covariate imbalance. Note that implementation of RMST calculations is available in all major statistical software (R, Stata, SAS).,As an illustration, we consider SPRINT, a landmark trial supporting clinical guidelines for intensive BP lowering in patients with CKD.10 SPRINT randomized patients to intensive (systolic BP target of <120 mm Hg) or standard (systolic BP target of <140 mm Hg) BP control and evaluated a composite primary outcome of time-to-first occurrence of a major adverse cardiovascular event (MACE): myocardial infarction, acute coronary syndrome, stroke, heart failure, or death attributable to cardiovascular disease.5 The estimated HR is 0.74 (95% confidence interval [CI], 0.63 to 0.86; P = 0.0002), indicating a 26% reduction in the hazard rate of MACE with intensive treatment. The 4-year RMST, or average MACE-free days in 4 years, in the intensive systolic BP group is 1409 days (95% CI, 1402 to 1415), compared with 1393 (95% CI, 1386 to 1400) average MACE-free days in 4 years in the standard systolic BP group (Figure 1). The estimated RMST difference is 15 days over 4 years (95% CI, 5 to 25 days; P = 0.002), indicating that the intensive BP control lengthened the time-to-first occurrence of MACE by 15 days over 4 years, compared with standard BP control. We chose a time horizon of 4 years for ease of interpretation; in the SPRINT trial, the RMST is well defined up to 4.75 years, which represents the largest follow-up time seen by patients in both treatment arms.,While the HR estimate of 0.74 suggests an enormous treatment effect, it is susceptible to misinterpretation in the absence of other information. The HR needs to be presented alongside the hazard rate over time in the control arm to be understood—otherwise relative measures of the treatment effect can be very misleading. However, such a hazard rate function is difficult to estimate reliably and is rarely reported in practice. By contrast, delaying the average time to MACE by 15 days over a 4-year period is easily understood and provides further context to the real effect of the treatment.,In summary, this example illustrates the value of reporting the RMST and the RMST difference in nephrology survival data analysis, in addition to commonly reported metrics such as the HR or absolute risk reduction. We encourage investigators to report the RMST to foster understanding of the magnitude of the treatment effect and enhance communication between clinicians and patients."
78,78,496,36357127,https://www.doi.org/10.2215/CJN.09340822,https://journals.lww.com/,"High plasma fibroblast growth factor 23 (FGF23) levels in patients with CKD have been associated with adverse cardiorenal outcomes in most studies, thereby providing a rationale for FGF23-lowering strategies. Potassium intake influences kidney phosphate handling and reduces FGF23 in individuals with normal kidney function1; whether this also applies to patients with CKD is unclear. In this study, we describe the effects of short-term KCl supplementation on mineral parameters in patients with stage 3b-4 CKD.,Data were analyzed from 125 participants who completed a single-arm 2-week intervention with 40 mmol/d KCl during the run-in phase of an ongoing randomized placebo-controlled trial with ≥75% adherence to supplements (92% of the total population). The primary aim of the trial is to address the long-term effects of KCl or potassium citrate versus placebo on kidney function.2 The study was approved by the Erasmus Medical Center Medical Ethics Committee (MEC-2017-226) and registered at ClinicalTrials.gov (NCT03253172). We calculated that 106 participants would provide 80% power to detect an effect of 10 relative unit (RU)/ml in C-terminal FGF23 (cFGF23),1 with a two-sided significance of 0.05. Plasma intact FGF23 (iFGF23, BiomedicaGruppe), cFGF23 (Kainos), α-Klotho (ImmunoBiological), C-reactive protein (CRP), IL-6 (Roche), and mineral parameters were measured at baseline and after 2 weeks.,The mean age was 66±11 years, 72% were male, the mean body mass index was 28.2±4.5 kg/m2, and the mean eGFR was 32±8 ml/min per 1.73 m2, while 51% used vitamin D supplementation and 2% used phosphate binders. After KCl supplementation, both 24-hour urinary potassium excretion (2.8±0.9–4.1±1.1 g/d, P<0.001) and plasma potassium increased (4.2±0.5–4.7±0.6 mEq/L, P<0.001). The 24-hour urinary chloride excretion also increased (5.2±1.9–6.4±2.4 g/d, P<0.001), whereas plasma chloride did not change (104±3–105±4 mEq/L, P=0.07). iFGF23 decreased (70.5 [46.6–105.6] to 62.9 [39.1–104.6] pg/ml, P=0.007), but cFGF23 did not (147.1 [105.9–223.8] to 130.3 [105.8–221.3] RU/ml, P=0.08, Figure 1). Furthermore, iFGF23:cFGF23 ratio decreased (0.53 [0.31–0.72] to 0.47 [0.30–0.72], P=0.02). Plasma phosphate (3.1±0.7–3.3±0.65 mg/dl, P=0.01) and tubular maximum reabsorption of phosphate per GFR (TmP/GFR; 0.63±0.20–0.68±0.20 mmol/L, P=0.002) increased, while 24-hour urinary phosphate excretion (78 [56–93] to 76 [58–92] mg/d, P=0.23) and 24-hour urinary sodium excretion did not change (152±60–150±69 mmol/d, P=0.73). Plasma 25(OH)-vitamin D decreased (28.8 [17.6–37.3] to 28.0 [17.6–36.1] ng/ml, P<0.001), and fractional calcium excretion increased (0.49% [0.29%–0.84%] to 0.55% [0.31%–0.89%], P=0.03). No significant changes were observed in parathyroid hormone (PTH; 67.0 [48.0–109.4] to 66.0 [43.4–94.3] pg/ml, P=0.12), α-Klotho (410 [477–564] to 424 [488–579] pg/ml, P=0.23), CRP (2.1 [1.2–4.4] to 2.3 [1.0–4.8] mg/L, P=0.72), or IL-6 (2.88 [1.53–4.91] to 2.54 [1.50–4.77] pg/ml, P=0.35).,Changes in FGF23 levels were positively correlated with changes in plasma phosphate (iFGF23: Spearman ρ=0.36, P<0.001; cFGF23: ρ=0.37, P<0.001) and TmP/GFR (iFGF23: ρ=0.29, P<0.001; cFGF23: ρ=0.29, P<0.001). In addition, changes in FGF23 were inversely correlated with changes in plasma chloride (iFGF23: ρ=−0.30, P<0.001; cFGF23: ρ=−0.22, P=0.01) and eGFR (iFGF23: ρ=−0.19, P=0.04; cFGF23: ρ=−0.37, P<0.001).,This study shows that a modest increase in potassium intake may lead to a small but significant reduction in iFGF23 levels in patients with advanced CKD, in line with our prior study in individuals with normal kidney function.1 Mechanistically, increased potassium intake inhibits sodium chloride cotransporter (NCC) expression in the distal tubule. Interestingly, osteoblasts, the main FGF23-producing cells, also express NCC,3 and NCC blockade reduced FGF23 transcription in murine osteoblasts and attenuated plasma FGF23 levels in patients with CKD.4 Moreover, reversing low intracellular chloride concentration may also reduce NCC phosphorylation.5 Thus, the KCl-associated reduction in iFGF23 might be mediated by increased osteocyte NCC expression or phosphorylation.,The decrease in FGF23 levels likely led to an increase in TmP/GFR and, in turn, to a temporarily decrease in urinary phosphate excretion and increased plasma phosphate levels, independent of PTH. Urinary phosphate excretion eventually returned to baseline levels because of unchanged phosphate intake. Although the positive, and not inverse, correlation between change in FGF23 and phosphate may seem counterintuitive, on a group level, KCl led to a decrease in iFGF23 and an increase in phosphate levels. Plasma vitamin D decreased, presumably as a consequence of the increase in plasma phosphate. The iFGF23:cFGF23 ratio decreased, suggesting either decreased FGF23 transcription or slightly increased cleavage of iFGF23. It is unlikely that inflammation mediated the FGF23-lowering effect of KCl as CRP and IL-6 levels did not change.,In this uncontrolled study, we could not account for dietary intake, plasma 1,25-dihydroxy vitamin D, or diurnal variation in phosphate homeostasis. Furthermore, we could not discriminate whether potassium, chloride, or both contributed to the reduction in plasma iFGF23. In the ongoing trial, participants are randomized to KCl, potassium citrate, or placebo for 2 years,2 allowing for more specific analyses.,In conclusion, the 2-week supplementation of 40 mmol/d KCl led to lower iFGF23, but not lower cFGF23 levels, while plasma phosphate levels increased. Further research should address whether long-term KCl provides a sustainable reduction in FGF23 levels, and whether this translates into improved clinical outcomes."
79,79,497,36719148,https://www.doi.org/10.1681/ASN.0000000000000018,https://journals.lww.com/,"Arnaud et al.1 make the important finding that AKI in tumor lysis syndrome (TLS) may occur in the absence of crystals, suggesting that other mechanisms may contribute to kidney damage besides the well-established mechanisms resulting from urate crystalluria with intratubular crystal precipitation. While these authors provide tantalizing evidence that extracellular histones may contribute to AKI by causing endothelial injury, we would also like to note that hyperuricemia in the absence of crystal deposition can also cause endothelial dysfunction, inflammation with the activation of inflammasomes, renal vasoconstriction, and reduced renal blood flow and GFR. In the cisplatin model of renal injury, hyperuricemia in the absence of crystal deposition exacerbated the renal injury, leading to local inflammation and worse kidney function.2 Modest elevations in serum uric acid have also been found to predict AKI from numerous causes, including after contrast administration, after cardiovascular surgery, and in TLS.3 Uric acid demonstrated linear correlation with serum creatinine and inverse correlation with kinetic eGFR in several clinical models. Pilot clinical trials suggest that lowering uric acid in subjects with mild to moderate hyperuricemia can reduce AKI (biomarkers) following cardiovascular surgery.4 In TLS, lowering uric acid attenuated rise in serum creatinine, decreased the incidence of AKI, and improved GFR. In a meta-analysis of 11 studies, treatment with allopurinol was associated with a significant increase in endothelium-dependent vasodilatation (even in asymptomatic patients with normal renal function), decrease in serum creatinine, and improvement in GFR.,In the current study, serum uric acid levels in patients with TLS, TLS-AKI, and non-AKI TLS were 8.36 mg/dl, 8.74 mg/dl, and 6.79 mg/dl, respectively. However, the majority of patients with AKI did not have crystalluria. There were no crystal deposits in TLS mice due to the presence of uricase in the mice liver that degrades uric acid into allantoin. These observations are consistent with a crystal independent role for uric acid in AKI. More studies need to evaluate noncrystalline drives of AKI in TLS. New detective work suggests that extracellular histones represent some of the criminals. We would suggest that soluble uric acid is also an accomplice. We recommend more studies to investigate the role of urate lowering therapy in AKI."
80,80,498,36220190,https://www.doi.org/10.2215/CJN.09460822,https://journals.lww.com/,"Diabetes mellitus is a leading cause of kidney disease. Accepting living kidney donors with diabetes mellitus remains a debatable issue. The necessity of this article's proposal resulted from recent Organ Procurement and Transplantation Network (OPTN) policy amendments regarding the acceptance of living kidney donors with diabetes mellitus (1).,For a frame of reference, the European Best Practices Guidelines qualify living kidney donors with preexisting diabetes mellitus only under “exceptional circumstances” (2). The British Transplantation Society offers the opinion-based recommendation that “diabetics can be considered for kidney donation after a thorough assessment of the lifetime risk of cardiovascular and progressive kidney disease in the presence of a single kidney” (3). According to European guidelines, impaired glucose tolerance is not an absolute contraindication to donation (recommendation 2C) (2). As per Kidney Disease Improving Global Outcomes (KDIGO), the decision to accept potential living kidney donors with prediabetes should be “individualized based on demographic and health profile in relation to the transplant program's acceptable risk threshold, including projected risk of kidney failure estimated from consideration of all baseline factors” (4). On the basis of systematic evidence review and the guideline framework, KDIGO includes another similar recommendation for individualization to allow for the consideration of very low–risk (generally older) individuals with type 2 diabetes mellitus for kidney donation (4).,Historically, OPTN and many other international practice guidelines prohibited living individuals with preexisting diabetes mellitus from kidney donation. However, on July 27, 2022, OPTN guidelines were loosened. Patients with type 2 diabetes mellitus are now being considered for donation “unless assessment of donor demographics or comorbidities reveals either evidence of end organ damage or unacceptable lifetime risk of complications” (1).,A significant challenge for clinicians with the evolving OPTN changes is the lack of a clear approach for assessing and accepting living kidney donors with preexisting diabetes mellitus. In this brief report, we intend to discuss the significant hurdles that clinicians face when evaluating living kidney donors with diabetes mellitus and propose an approach while evaluating potential living kidney donor candidates with preexisting diabetes mellitus.,The metabolic syndrome, a cluster of risk factors, has a clear and multifaceted detrimental effect on the progression of kidney disease. Because of the lack of robust evidence on long-term outcomes of living kidney donors with multiple risk factors for diabetes mellitus, exclusion criteria on the basis of prediabetic status or diabetes mellitus risk factors vary across transplant centers. In most guidelines, prediabetes and a history of gestational diabetes mellitus are not considered absolute contraindications to living kidney donation. For context, over a mean follow-up of 15.7 years, a study of 8280 donors, including 1826 with impaired fasting glucose at the time of donation and 6204 without, found similar survival and rates of kidney failure among donors with and without impaired fasting glucose (5).,Because diabetes mellitus is a major cause of kidney failure, accepting diabetic living kidney donors with additional risk factors raises substantial clinical concerns and should be approached with caution. Uncontrolled hypertension (HTN) is a significant and independent risk factor for the development of CKD, kidney failure, and graft failure in transplant recipients. Living kidney donors with preexisting HTN have a higher long-term risk of developing kidney failure compared with nonhypertensives (6). Obesity, another risk factor, has been linked in multiple studies to the development and progression of CKD. A higher body mass index was found to be directly associated with the development and progression of proteinuria in people without kidney disease, and obesity has been proven to be an independent risk factor for the development of kidney failure (7). In a broader sense, it has been shown that obesity and HTN have an additive effect on decreasing GFR compensation after kidney donation (8). Smoking has been shown to increase the risk of CKD, notably hypertensive nephropathy and diabetic nephropathy. Furthermore, numerous animal and human studies have shown that hyperlipidemia accelerates the rate of kidney disease progression (9).,In the absence of clear literature to assist in the selection of living kidney donors with diabetes mellitus, it has been essential to carefully construct a thoughtful approach that takes into account the risk considerations and the global risk posed by these. Our proposed eligibility criteria for living kidney donors with preexisting diabetes mellitus are illustrated in Figure 1. The American Diabetes Association (ADA) recommends annual diabetes screenings after the age of 45, which is the average onset age for type 2 diabetes mellitus (10). From a safety perspective, in the absence of supporting literature, we arbitrarily suggest that eligible living kidney donors be at least 55 years old, have had diabetes mellitus for at least 3 years, and have demonstrated a hemoglobin A1C of ≤7% on at least three occasions in the last 2 years in order to verify adequate control and adherence. We rule out candidates who are insulin dependent or take more than two oral antidiabetic medications. We advocate adhering to ADA's treatment guidelines, which still identify metformin as the most often used first-line oral antidiabetic medication. Sodium-glucose cotransporter 2 inhibitors and glucagon-like peptide 1 receptor agonists, with or without metformin, are acceptable first-line alternatives with proven cardiovascular and nephroprotective benefits, particularly in individuals with or at high risk of atherosclerotic cardiovascular disease, heart failure, and/or CKD (10).,Prior to donation, candidates must have a body mass index of <30 kg/m2, be nonsmokers, and have well-controlled hyperlipidemia with recommended values as established by the ADA guidelines (LDL cholesterol of <100 mg/dl, HDL cholesterol of >40 mg/dl in men and >50 mg/dl in women, and triglyceride levels of <150 mg/dl) as demonstrated by lipid profile testing within the past year (10). The surgeon will assess the donor's body habitus during the evaluation to determine its suitability for surgery.,Candidates with diabetes mellitus and HTN are strongly discouraged from kidney donation. Acceptance of diabetic living kidney donors on a single antihypertensive medication or who demonstrate slight deviations from our proposed criteria but otherwise are deemed suitable by the selection committee for donation will be considered on a case-by-case basis. Some data suggest that diabetic retinopathy may precede or predict diabetic nephropathy (11). We require donors to have a fundus examination to rule out diabetic retinopathy, which we use as a surrogate marker for diabetic microvascular damage. Eligible candidates must have a urine albumin-creatinine ratio of <30 mg/g or a urine protein-creatinine ratio of <200 mg/g. In addition to rigorous counseling, the donor’s and the program’s conjoint willingness to accept the high risk, and informed consent, we strongly recommend that living kidney donors maintain close follow-up with their primary care provider throughout their lifetime. Although we strive to help patients with kidney failure, we believe that a stringent approach to potential living kidney donors with diabetes mellitus should be taken to minimize harm and ensure their safety. Even though prior living kidney donors who develop kidney failure receive high priority on the OPTN kidney waiting list, granting donors with preexisting diabetes mellitus the highest priority merits serious consideration.,Because diabetes mellitus was formerly regarded as an absolute contraindication for kidney donation, there are currently no data comparing the lifetime risk of developing kidney failure in a diabetic donor with that of a diabetic nondonor and a nondiabetic donor. If kidney donation by patients with diabetes who meet specific criteria is implemented, both prospective and retrospective studies will be necessary to fill in these knowledge gaps. An evidence-based guideline developed through additional robust data to provide further guidance in selecting living kidney donors is desperately needed. Meanwhile, we are proposing a practical protocol to aid in the careful selection process of living kidney donors in order to protect the health and future welfare of prospective donors."
81,81,499,38015564,https://www.doi.org/10.1681/ASN.0000000000000270,https://journals.lww.com/,"In adults, 90% of body creatinine stores are contained in muscle, and with aging, a constant percentage of lean body mass is lost; men have a greater fractional loss of creatinine than women. Muscle wasting commonly occurs in patients with CKD, and muscle wasting becomes more apparent when CKD progresses to require dialysis.1,2 Given the limitations of measuring muscle mass using routine clinical assessment tools, serum creatinine has been considered a proxy for assessing muscle mass.3,In the steady state, serum creatinine is calculated from the difference between creatinine generation in muscle plus that derived from protein intake minus the elimination rate of creatinine by kidney function plus gastrointestinal secretion. In dialysis patients with minimal residual kidney function, serum creatinine is calculated from muscle mass and dietary intake of creatinine. These relationship responses provide an opportunity to evaluate how variations in muscle mass in different races affect creatinine metabolism. This topic has not been addressed in the dialysis population. From clinical and public health perspectives, it is necessary to examine this topic because serum creatinine values are higher among Black patients, and historically, a race coefficient has been used in the calculation of eGFR to account for higher muscle mass.4,Delgado et al. evaluated differences in serum creatinine in a secondary analysis of the ACTIVE/ADIPOSE (A Cohort Study to Investigate the Value of Exercise in ESRD/Analyses Designed to Investigate the Paradox of Obesity and Survival in ESRD) study of patients undergoing prevalent hemodialysis (n=501, who had been treated by dialysis for at least a year).5 They examined associations of race and ethnicity (groups classified as Black, Asian, Non-Hispanic White, and Hispanic patients) with serum creatinine and intracellular water (ICW) using whole-body multifrequency bioimpedance spectroscopy (BIS) to serve as a proxy for the size of muscle mass.5 Compared with values from non-Hispanic White participants, Black, Asian, and Hispanic participants had higher levels of serum creatinine after adjusting for covariates. Although ICW was associated with serum creatinine levels, it did not differ statistically among different races or ethnic groups. All races were significantly associated with serum creatinine concentrations with and without adjusting for ICW. This suggests that there are other determinants of serum creatinine beyond skeletal muscle mass.,This secondary analysis has several strengths, including the availability of multifrequency BIS data from a diverse cohort of hemodialysis patients with minimal residual kidney function (evaluations in individual patients who had measurements after a year of dialysis treatments). However, limitations include the inability to adjust for several confounders with a relatively small sample size, lack of some key covariates, and evaluation of the generalizability of results, including for other dialysis modalities. For example, the study did not exclude results from patients taking nutritional supplements that influence serum creatinine levels. Furthermore, details of dietary intake, especially the intake of animal proteins, were not evaluated (expected to be different among different races). The report also lacked details on residual urine output and whether tubular secretion of creatinine affected serum creatinine in those with some residual function. Data pertinent to social determinants of health were not available in the report, and they are known to contribute to variations in muscle mass and protein intake. Despite these limitations, the data offers several insights to fill our knowledge gap and argues for additional studies to examine non-GFR determinants of serum creatinine and other markers.,Regarding other factors affecting serum creatinine, we evaluated creatinine metabolism in a small number of patients with advanced CKD. We measured creatinine turnover in patients with severe CKD who were being treated by dietary protein restriction.6 Evaluation of creatinine metabolism revealed that the total creatinine metabolism rate was positively correlated with serum creatinine: As serum creatinine rose, an increasing fraction of creatinine produced was metabolized. In addition, there was evidence of extrarenal creatinine excretion expressed as a function of total creatinine clearance that could account for approximately 31% of renal clearance in these patients. Whether there are differences in extrarenal creatinine clearance among different races of patients undergoing dialysis needs to be studied. Factors affecting creatinine metabolism and how these factors change creatinine clearance could provide insights into mechanisms that affect changes in serum creatinine.,Current clinical methodologies to estimate muscle mass include multifrequency BIS and dual energy x-ray absorptiometry. These are inexpensive and less time-consuming than computed tomography and magnetic resonance imaging scans, which are often used in clinical research. Recently, point-of-care ultrasound is being used to assess muscle mass and could help clinicians identify factors influencing patients with sarcopenia.7 The ACTIVE/ADIPOSE cohort had BIS assessed before the dialysis session during the mid-week. Moreover, Iorio and colleagues have reported that BIS variables fluctuate in patients undergoing hemodialysis but remain constant and highly reproducible over 120 minutes after the end of hemodialysis, especially when aiming to quantify muscle mass.8 Future investigations may consider other modalities of body composition assessment and their utility among different races for the hemodialysis population.,The ACTIVE/ADIPOSE study cohort included a significant proportion of Asian patients being treated by hemodialysis. In non–dialysis-dependent CKD, Asian patients have lower values of muscle mass (compared with other ethnicities).9 Hence, compared with other races, the finding of similar creatinine values among Asian patients (compared with non-Hispanic White participants) should be explored further. For example, the group had similar creatinine values despite lower body mass index (than other races), lower ICW, and more effective dialysis clearance, suggesting that factors other than muscle mass have to be considered. Akin to the effort by Delgado et al., a similar study in the peritoneal dialysis population would also inform us of the influence of serum creatinine and muscle mass in different races and ethnicities.,In summary, the report of Delgado et al. is a welcome attempt to generate evidence to test the assumption that higher serum creatinine in Black patients can be attributed to a higher muscle mass. While they refute some of the prevailing assumptions, additional studies to confirm these findings with other modalities of body composition techniques, among those on other forms of dialysis, and adjusting for potential confounders are needed."
82,82,527,36808841,https://www.doi.org/10.1097/TP.0000000000004354,https://journals.lww.com/,"Therapeutic patient education (TPE) is a patient-centered process, fully integrated in health care, aiming to help patients with chronic disease advancing means of self-care, independence, and adaptation competencies, to maintain or improve their quality of life.1 Uniquely positioned, the French law has rendered TPE as a requirement of patient care since 2009.2 The biomedical model has focused on pathologies and treatments, in a disease management mode targeting mainly the improvement of compliance.1,3 A humanistic biopsychosocial model has recently emerged in France, leading to education approaches centered on the development of psychosocial and adaptation competencies.,In this context, a generalist national competency framework for healthcare professionals, helping teams to integrate both biomedical and psychosocial approaches in their TPE programs, has been published by the French health authorities.,By improving transplant patients’ autonomy, TPE aims to optimize successful outcomes, including improved quality of life, better coping with side effects or decreased risk of graft loss.3-7 In this scientific and regulatory context, French transplantation centers should offer a TPE program to all patients.,Approximately 5000 organ transplantations are currently performed at 58 centers in France, coordinated by the French Biomedicine Agency. Notably, centers have been challenged by limited funding resources, and geographic disparities have become obvious. In 2018, only 46 centers in 12 regions have been able to offer a program based on diverse competency frameworks established in each of the participating centers that communicate comparable competency and learning objectives.,TPE cannot be standardized among centers as the concept depends on local specificities and means. Thus, a unique reference competency framework rather than a standardized program may help harmonize existing while establishing new programs. Importantly, a nationally validated competency framework may contribute to upgrading the quality of practices. Although the majority of transplant teams in France are involved in TPE, healthcare professionals need training, and the reference competency framework may help to disseminate a common approach. In addition, evaluating the success of TPE is challenging, as a beneficial impact on both biomedical and psychosocial outcomes is expected. A common reference competency framework is expected to support evaluating the benefits of TPE in everyday practice and during clinical trials. Moreover, and in addition to the French experience, a harmonized competency framework may represent a new and useful tool for the international transplant community, harmonizing objectives when conducting international trials and comparing results in different settings. With that rationale, the French-Speaking Transplantation Society (Société Francophone de Transplantation) suggested a national competency framework in solid organ transplantation.,The METIS project, conducted between 2019 and 2021, aimed to elaborate a national competency framework in solid organ transplantation based on the Delphi method.8 A pilot group of 11 experts in the fields of transplantation, TPE, public health, and clinical research was charged to delineate the project and moderate each successive round of the process. An expert panel consisting of 54 physicians, 7 surgeons, 27 nurses, 13 pharmacists, 4 dieticians, 3 sports instructors and physiotherapists, 7 psychologists, and 14 patients originating from 37 adult and pediatric transplantation throughout the country was established. Participating patients had received kidney (n = 8), liver (4), kidney and liver (1), and heart transplants (1). Experts were asked to express their level of agreement about including various propositions into the national competency framework. The same list served as the main thread throughout the process, refined at each round of discussions. Propositions that had received a strong immediate or a relative consensus during 2 sequential rounds were included in the final competency framework.,A unique competency framework with relevance to all adult and pediatric recipients of solid organ transplants was considered most relevant by the pilot group. A first series of 76 propositions arose from a brainstorming session based on existing competency frameworks. Consensus was achieved after 3 rounds and 78 experts participated throughout the entire project. The Delphi process led to discarding 2 and adding 8 propositions, generating a framework including 82 adaptation, self-care, and safety competencies, categorized into 6 themes (knowledge about transplantation, experience of transplantation, treatment management, hygienic and dietary measures, monitoring and alerts, daily life) (Table 1). In addition to classical biomedical objectives such as managing treatment or adopting appropriate hygiene and dietary measures, the framework includes a variety of psychosocial skills. Asking for help in case of psychological suffering, socioprofessional or school difficulties, organizing social activities, or having a parenthood project are examples of major emotional, psychosocial, and socioprofessional matters that have been included the competency framework.,This METIS project implemented a unique, national competency framework for TPE in transplant patients and will be disseminated for implementation in all French transplantation centers. The framework includes a wide variety of self-care, safety, and adaptation skills related to transplantation, treatment, and follow-up modalities. By addressing psychosocial and emotional issues, the framework will help healthcare teams to support patients in managing their life after transplantation, thus contributing to improved outcomes. TPE should be based on a holistic approach and be offered during early and late stages posttransplantation.9 Therefore, timing relative to transplantation will have to be defined for each skill, and the competency framework will be combined with a user guide facilitating its effective utilization. As a next step, the framework will be submitted to the French Health Authorities and Agencies to assist in its implementation.,We present the first national competency framework for TPE. This framework, established by a group of experts representing patient and providers, is of significant relevance in solid organ transplantation, as it is expected to (i) aid transplantation teams in constructing new or implementing existing programs; (ii) contribute to harmonizing TPE programs and learning objectives; (iii) contribute to upgrading the quality of patient care; and (iv) help conduct multicenter clinical trials on TPE.,We thank all the experts for their participation in this consensus conference: Teresa Antonini, Nadia Arzouk, Carole Ayav, Louise Barbier, Karine Barelle, Hélène Barraud, Guillaume Baudry, Marina Bellet, Cindy Blondeau, Frédérique Bocquentin, Olivier Boillot, Marion Bolumar, Karim Boudjema, Marie Bourgeois, Cyril Breuker, Eric Buleux, Vincent Bunel-Gourdy, Fanny Buron, Karine Carrie, Françoise Chouzet, Filomena Conti, Michel Coulomb, Lionel Couzi, Alain Dabadie, Chloé Danet, Isabelle Danner-Boucher, Marilyne Debette-Gratien, Dominique Debray, Tristan Degot, Xavier Demant, Sébastien Dharancy, Agnès Didier, Stéphanie Domenge, Anne Dory, Jérôme Dumortier, Eric Epailly, Isabelle Eraud, Isabelle Etienne, Sandra Gaboriau, Florentine Garaix, Cyril Garrouste, Emmanuel Gastaud, Anne Grall-Jezequel, Jacqueline Gregorio, Caroline Guillotin, Malika Hammouda, Anne Hiegel, Laëtitia Idier, Helga Junot, Florence Lacaille, Laure Lalande, Magali Lamoine, Guillaume Lassailly, Emmanuelle Laurain, André Le Tutour, Pascal Lebray, Angélique Lecouf, Nazli Lefevre, Dorothée Lombardo, Paolo Malvezzi, Jonathan Messika, Karine Moreau, Magalie Mosnier, Béatrice Moulin, Florence Moulonguet, Dominique Navas, Estelle Nicolle, Robert Novo, Emelyne Paillasse, Sabine Pattier, Stéphane Penando, Evangéline Pillebout, Christophe Pison, Hélène Pluchart, Coralie Poulain, Xavier Pourrat, Delphine Rateau, Agnès Robert, Elena Romanelli, Fabienne Roques, Olivier Roux, Guy Sabatier, Françoise Smett, Jeanick Stocco, Leïla Temagoult, Emilie Terrenes, Denis Touchet, Marie-Hélène Tribot, Carole Vachias, Claire Vanlemmens, Ludivine Verdonk, Ludivine Videloup, Louise, Géraldine, Ophélie, Sophie, Marine, Valentine, and Julie (nurses at La Pitié-Salpétrière Hospital, Paris). We thank Olivier Chicaud for elaborating the website dedicated to the project and Karen Poole for reading and editing of the article’s English version."
83,83,547,36314999,https://www.doi.org/10.1097/TP.0000000000004421,https://journals.lww.com/,"As the protection provided by 2 doses of an mRNA vaccine substantially wanes over time and with the emergence of new viral variants,1 administration of booster doses was recommended for kidney transplant recipients (KTRs).2–4 Vaccination-associated immune activation might be associated with allorecognition and graft injury,5 and safety data in KTRs are insufficient.,This prospective, observational study aims to evaluate vaccine safety, the dynamics of anti-HLA antibodies and donor-derived cell-free DNA (dd-cfDNA),6–9 and seroconversion rates following the first booster dose of an mRNA vaccine. The 3-mo follow-up of 108 SARS-CoV-2 naive KTRs is reported. Anti-HLA antibodies, SARS-CoV-2 IgG S1/S2 antibodies, kidney graft function were measured at baseline (D0) and at 3 mo (M3); dd-cfDNA was analyzed in 79 KTRs.,Anti-HLA antibodies were measured with the single-antigen bead technology (LabScreen Mixed and LabScreen Single Antigen Luminex). The presence of donor-specific antibodies (DSAs) was assessed using HLA fusion 4.6.0 software (all One Lambda, Inc). The cutoff of 500 mean fluorescent intensity (MFI) was set as a threshold for DSA/anti-HLA antibody positivity.10 A significant increase in anti-HLA antibodies or DSAs was defined with the emergence of a de novo antibody with MFI >500 and a >50% increase in MFI of a preformed antibody. Calculated virtual panel-reactive antibodies were computed using the Eurotransplant Reference Laboratory calculator.11 The dd-cfDNA was measured with the Prospera test, with 1% cutoff.6 After vaccination, all study participants were contacted and asked about any reactions. SARS-CoV-2 S1/S2 IgG was tested using the LIAISON SARS-CoV-2 S1/S2 IgG chemiluminescence immunoassay (DiaSorin S.p.A., Italy). Cutoff of 9.5 arbitrary units (kAU/ml) was used to define seroconversion. One-sided, paired Wilcoxon test was used to compare the differences in MFI, dd-cfDNA, anti-SARS-CoV-2 IgG, and estimated glomerular filtration rate at D0 and M3.,Anti-HLA antibodies at the D0 visit were found in 27% of KTRs (sensitized KTRs); 4 became negative by the M3 visit. One KTR developed de novo anti-HLA antibodies. DSAs at the D0 visit were found in 6 KTRs; by the M3 visit‚ 2 became negative; in 4 KTRs‚ MFI remained stable. One KTR developed de novo DSAs with a low MFI. No increases in the average MFI nor in up to 2 immunodominant anti-HLA MFIs were found (Figure 1A–D). No increase in calculated virtual panel-reactive antibodies was found (Figure 1E–F).,There was no KTR with a new increase of dd-cfDNA above 1% at M3, nor with any overall increase in dd-cfDNA in the whole cohort including sensitized KTRs (Figure 2A,B). The dd-cfDNA at M3 increased in nonsensitized KTRs (Figure 2C); however, this increase was mild and far below the 1% threshold. Sensitized KTRs presented higher dd-cfDNA (Figure 2D).,No difference in estimated glomerular filtration rate was found (Figure 2E–G); no for cause graft biopsy was performed. Seroconversion was detectable in 52 (48%) at D0 and in 85 (81%) KTRs at M3, respectively.,In conclusion, we found no evidence of allosensitization or subclinical kidney allograft injury following the first booster dose.,We wish to thank the outpatient clinic nursing staff for obtaining patient samples; Transplant Laboratory technicians for sample handling; Petr Raska, MSc, and Michal Hojny, PharmD, for organizing the booster dose campaign at IKEM; Hedvika Cacarova for English proofreading; Cesar Escrig for fruitful discussion; and Natera Inc for blood sample processing and Prospera results."
84,84,549,36508651,https://www.doi.org/10.1097/TP.0000000000004457,https://journals.lww.com/,"In the article: “Four Decades of Clinical Liver Transplantation Research: Results of a Comprehensive Bibliometric Analysis” by Jiang et al, published in the October 2022 issue of Transplantation, in Figure 4, Panel B & C and in the corresponding cover image of the same issue, certain authors and institutions were visualized only as data points and not named in the figure. This visualization issue was not intentional and can be attributed to some limitations of the integrated features of the used software tool. Nevertheless, few important contributors felt negatively impacted by this visualization, kindly requesting a correction of the figure including their names. Therefore, we acknowledge this issue and provide an erratum with the corrected version of Figure 4 in which all data points are linked with their corresponding contributor or institution (see Figure 1)."
85,85,551,36253912,https://www.doi.org/10.1097/TP.0000000000004364,https://journals.lww.com/,"In 2019, Groopman et al set out to assess the diagnostic utility of exome sequencing in a diverse, all-cause, chronic kidney disease cohort of over 3000 individuals and demonstrated the existence of a monogenic form of kidney disease in approximately 10% of cases.1 Furthermore, the diagnostic yield was even higher (17.1%) among individuals with kidney disease of unknown etiology. This groundbreaking study brought to light how the high genetic and phenotypic heterogeneity of monogenic diseases, coupled with our limited appreciation for the full spectrum of their clinical manifestations, contributes to diagnosis delays and disease misclassifications. Today, with greater clinical access to molecular tests, we are increasingly able to establish the molecular diagnosis for a growing number of monogenic kidney diseases. Recent studies have demonstrated the clinical utility of genomic testing as part of the diagnostic evaluation of patients with kidney disease, which supports personalized management by guiding choice of therapy, informing targeted disease surveillance, and identifying at-risk family members for cascade screening.2-5,However, genomic tests are still relatively new diagnostic tools for most subspecialties of medicine and surgery and not widely used in routine care. Although various factors likely contribute to their limited clinical use, it is important to understand that broader implementation of genomic testing in the clinical domain relies largely on providers’ appreciation for a breadth of core knowledge specific to genomic medicine.6 This includes understanding specialized terminology (eg, penetrance, phenotype, carrier frequency, etc), various sequencing approaches (eg, single-gene tests, gene panels, exome and genome sequencing, etc), categories of genomic results (eg, polygenic risk scores, risk alleles, primary results and otherwise medically actionable secondary findings, variants of uncertain significance, etc), and an awareness of the complex ethical, legal, and technical considerations that come with genomic sequencing. Given the complexity of using genomic data in medical decision-making, as well as kidney experts, limited experience diagnosing monogenic diseases and using genomic technologies, a multidisciplinary approach is often needed to guide interpretation of raw genomic data, clinical correlation, and the integration of diagnostic findings in patient care.2-5,7-9,In this issue of Transplantation, El Ters et al share their experience using molecular testing in the evaluation of individuals presenting to Mayo’s adult kidney transplant clinic. In total, 30 prospective transplant recipients (the majority [n = 24, 80%] who had histopathologic evidence of focal segmental glomerulosclerosis pattern lesions on biopsy) plus an additional 5 prospective living donors underwent clinical-grade exome sequencing to try to identify an underlying monogenic form of kidney disease. In their study, they used a semi-masked approach to analyze the exome data by focusing their search to deleterious variants in a subset of 344 genes associated with hereditary forms of kidney disease. They considered potentially diagnostic variants to be those that met the American College of Medical Genetics and Genomics’ standards for classification as either pathogenic or likely pathogenic.10 The authors then reviewed these identified variants with a multidisciplinary team of molecular and clinical genetic specialists to ensure agreement in the adjudication of each variant’s classification and ensure the findings were consistent with the patients’ clinical presentations before deeming the findings diagnostic. They then reported that among the 30 prospective transplant recipients who underwent sequencing, 13 (43%) individuals were found to have a diagnostic variant in one of these known kidney disease genes. The most common findings they reported were deleterious variants in the COL4A3/4/5 genes, which were identified in 7 of these 13 individuals, and are diagnostic for type IV collagen-associated nephropathies, which encompasses Alport syndrome, thin basement membrane disease, hereditary nephritis, and some hereditary forms of focal segmental glomerulosclerosis. In addition, the authors report that 1 of the 5 prospective living donors that underwent sequencing was found to have a pathogenic/likely pathogenic variant in a gene associated with a hereditary form of kidney disease and was excluded from donation.,Overall, this study reinforces the clinical utility of using molecular testing in the diagnostic evaluation of patients with kidney disease, specifically within the workflow of a kidney transplant clinic. Although the authors do not describe a novel approach in their implementation of genomic testing in the context of kidney disease patients, they demonstrate how diagnostic findings can inform the selection of prospective living kidney donors among at-risk family members, underscoring the important role genomic testing can play in transplant clinics. Moreover, their study reminds us of the precaution providers must take when adopting clinical genomics in patient care and the value added by employing a multidisciplinary approach that includes genomic experts. Studies like this one offer institution-specific experiences and lessons learned that can inform future genomic implementation initiatives. Further study is needed to identify innovative approaches that effectively advance the operationalization of precision nephrology care that can be scaled for use across distinct workflows and institutions."
86,86,554,36949031,https://www.doi.org/10.1097/TP.0000000000004575,https://journals.lww.com/,"Although advances in immunosuppression strategies have markedly improved 1-y transplant survival to above 90%, depending on the organ type, ~40% to 70% of the allografts are still lost by 10 y.1-5 Long-term allograft survival is often perturbed by ongoing immunological injury that leads to premature scarring and allograft loss. Nonadherence to immunosuppressive therapy is increasingly recognized as a key contributor to ongoing alloimmune injury and shortened allograft survival.6 Nonadherence is prevalent in as many as 20% of the patients irrespective of the transplanted organ type, and the efforts to abate it are thwarted by our inability to accurately measure and address it from a behavioral perspective.7,8 Thus, metrics that promote a better understanding of patient behaviors, perceptions, experiences, and responses to treatment posttransplantation could provide better insight into nonadherence and lead to development of newer and effective mitigating strategies.,Although a variety of assessment measures such as patient self-reports, pill counts, prescription refill rates, and collateral reports by care providers have been used to assess nonadherence, their true clinical utility is limited by their subjectivity and the potential for bias. Calcineurin inhibitor (CNI) trough level variability has been used as a surrogate for immunosuppression nonadherence and has been shown to be associated with poor transplant outcomes. However, CNI variability is influenced by drug absorption and genetic factors that affect their metabolism and trough levels. Additionally, the usage of generic versions of CNIs could further impact CNI trough level variability because of drug formulation variability. Although electronic medication monitoring (EMM) with a microdevice (which records each time a pill bottle is opened) is still the most objective method for evaluating nonadherence, its applicability in routine clinical practice is limited by its poor accessibility and feasibility. In this regard, Basel Assessment of Adherence to Immunosuppressive Medication Scale (BAASIS) was developed as a 6-item self-report questionnaire that assesses the 3 quantifiable phases of medication adherence: (1) initiation (patient takes the first dose of a prescribed medication), (2) implementation (the extent to which a patient’s actual dosing corresponds to the prescribed dosing regimen), and (3) discontinuation (patient stops taking the prescribed medication against a clinician’s advice).9,In the current issue, Denhaerynck and colleagues10 assessed the validity and reliability of BAASIS by conducting a meta-analysis encompassing 26 studies and 12 109 adult transplant recipients. To prove validity, the authors first confirmed concordance of self-reported nonadherence assessed by BAASIS with other adherence measures such as EMM, collateral adherence estimates by both physicians and nurses, as well as other self-report adherence measures. Given the strong concordance between BAASIS self-report and EMM, and the excessive costs associated with EMM usage, BAASIS self-report could be an inexpensive alternative. Although there was a discordance between EMM assessment of drug holidays and BAASIS self-report, drug holidays were extremely rare (0.4%) in this extensive analysis that assessed 14 962 prescribed medication doses. Despite an overall statistically significant association between nonadherence assessment by BAASIS and CNI trough variability, the effect size was modest (odds ratio, 1.013; 95% confidence interval, 1.002-1.023; P = 0.02) and the associations were inconsistent among participating studies. However, CNI trough variability could be influenced by factors not related to therapy adherence. Next, they have established the association between BAASIS and psycho-behavioral constructs known to be associated with nonadherence such as depressive symptomatology and the variables included in the integrative model of behavioral prediction (barriers, self-efficacy, intention, and beliefs). Assessment of pooled data from 3 randomized controlled trials that evaluated the efficacy of tailored behavioral interventions to improve adherence revealed only a statistically nonsignificant trend (odds ratio, 0.7; 95% confidence interval, 0.49-1.01; P = 0.06) for the effect of the intervention on the BAASIS questionnaire responses. Of note, 2 of the 3 randomized controlled trials utilized EMM to assess medication adherence and found no effect of the studied interventions on EMM-based adherence assessment either. The 30% to 60% variability noted in BAASIS responses when assessed longitudinally could be due to the variations in adherence levels over time. Finally, the Flesch Reading Score for BAASIS was 70, suggesting easy readability (seventh grade level). Moreover, the BAASIS instrument is translated into 11 languages, allowing for a widespread usage internationally. Taken together, the authors demonstrated good validity and reliability of BAASIS as a self-report instrument to assess medication nonadherence in transplant recipients.,Despite the recognition of clinical and economic burden of therapy nonadherence in transplantation, routine clinical assessment of nonadherence is not implemented because of inconsistent methodologies and lack of objectivity. Thus, achieving effective, sustainable, and evidence-based adherence monitoring in real-world clinical settings is a major, yet attainable goal for the field. Although patient self-report of medication adherence is often considered as a bias-prone method, reliable patient-report instruments such as BAASIS can easily be integrated into clinical practice. The effort by the authors to comprehensively assess validity and reliability of a simple and affordable self-report instrument (BAASIS) by conducting a meta-analysis encompassing a large patient sample size that reflects a variety of clinical settings, a wide range of time-periods of assessment after transplantation (ranging from time 0 to a median of ~109 mo posttransplantation), and patient populations in multiple countries is commendable. Although the authors present compelling arguments to support the validity and reliability of BAASIS, the true clinical utility of this instrument could potentially be further enhanced if it is used in conjunction with other measures of nonadherence. Importantly, lack of data on measurable objective clinical end points such as biopsy proven acute rejection, detection of donor-specific antibody and graft or patient survival limits its true clinical applicability. Confirmation of its clinical predictive ability could potentially take this simple self-report instrument much closer to routine clinical application. Moreover, assessment of its validity and utility in select clinical settings (eg, after an episode of acute rejection, when nonadherence has been shown to be a key modifier variable for clinical outcomes) could allow risk-stratification and simplify recruitment of patients into future clinical trials testing novel behavioral/psychological/pharmacologic approaches aimed at mitigating immunosuppression nonadherence. Nonetheless, the meta-analysis by Denhaerynck and colleagues, which confirms the validity and reliability of BAASIS in a large patient cohort, is a major step forward toward the goal of developing objective and usable nonadherence assessment tools in clinical transplantation."
87,87,555,36808845,https://www.doi.org/10.1097/TP.0000000000004473,https://journals.lww.com/,"Chronic graft rejectionand toxicities of long-term immunosuppressive therapies impede the advancement of organ transplantation. Establishment of a state of tolerance, in which the immune system remains functional and competent but treats the donor as “self,” would prevent chronic rejection and obviate the need for long-term immunosuppression (IS). In the past 2 decades, several clinical protocols have achieved organ allograft tolerance. Furthermore, new biological compounds and cell therapies aimed at tolerance induction have begun to be clinical in tolerance trials.,The biennial Workshop brings together leaders in the field of transplant tolerance and its underlying science to exchange progress and ideas through an interactive format. The Fifth International Workshop was held from April 27 to 29, 2022, in Jersey City, New Jersey, and was renamed in honor of one of its earliest proponents and founders, Samuel Strober, who passed away in 2022. The 2-d meeting covered the status of pre-clinical and clinical trials designed to minimize or withdraw immunosuppressive drugs in kidney, liver, intestinal, and lung transplantation, and mechanistic studies to understand tolerance and identify potential predictors and biomarkers. Here we summarize the many exciting advances presented at the Workshop.,During the last 2 decades, hematopoietic chimerism-based regimens aiming at complete IS withdrawal (ISW) have been evaluated (Table 1). These regimens involve induction of either full (persistent) or mixed chimerism (persistent or transient) via bone marrow transplantation (BMT) or peripheral blood stem cell (PBSC) transplantation. Suzanne Ildstad (Talaris) updated the outcomes of a trial for extensively HLA-mismatched kidney transplantation (KTx) plus PBSC and “facilitating cell” transplantation at Northwestern University.1,2 With persistent high levels of donor chimerism, 26 of 37 recipients were successfully removed from IS, and 6 are currently IS-free for >10 y. A randomized, multicenter, phase III trial is now underway. Joseph Leventhal (Northwestern University) reported operational tolerance in 6 of 15 recipients of HLA-identical KTx with donor CD34 cell infusion following alemtuzumab induction without myelosuppressive conditioning.3 Stephan Busque (Stanford U) summarized a total lymphoid irradiation/antithymocyte globulin (TLI/ATG)–based protocol in which 24 of 29 HLA-identical KTx recipients discontinued IS for >5 y and 7 for >10 y.4,5 In an HLA-mismatched cohort, no IS-free allograft survival has yet been achieved. The addition of low-dose total body irradiation (TBI) or regulatory T cell (Treg) infusion is being evaluated. Daniel Brennan (Medeor) reported on a similar TLI/ATG/CD34 cell infusion trial in HLA-identical KTx recipients. Nineteen of 20 recipients successfully discontinued IS up to 2.6 y of follow-up, of which 2 with recurrent disease returned to IS. Tatsuo Kawai Massachusetts General Hospital (MGH) updated the trial of HLA-mismatched combined KTx and BMT (CKBMT).6-8 Using cyclophosphamide (CY), thymic irradiation, and anti-CD2 monoclonal antibodies with or without rituximab, 7 of 10 achieved IS-free allograft survival for >5 y and 4 for >10 y, with the longest at 17 y. Two recipients of a new conditioning regimen with additional fludarabine developed transient mixed chimerism without complications, and IS tapering is in progress. Dr Kyo Won Lee summarized trials at the Samsung Medical Center (Seoul).9 With their most recent regimen (CY, rituximab, fludarabine, thymic irradiation, and ATG), ISW was achieved in 11 of 19 recipients with transient mixed chimerism.,To minimize IS in HLA-mismatched KTx recipients, clinical trials with Treg have begun (Table 2). The Northwestern group infused Treg following alemtuzumab induction, and Thomas Wekerle (Medical U of Vienna) combined polyclonal recipient Treg with donor BMT.10 Dr Katharina Schreeb (Safety & Tolerability Study of Chimeric Antigen Receptor T-Reg Cell Therapy in Living Donor Renal Transplant Recipients [Steadfast] Trial, Sangamo Therapeutics) described their trial using HLA-A2–specific chimeric antigen receptor-engineered Treg, and Fadi Issa (Oxford U) described the Transplantation Without Overimmunosuppression Study, in which Treg are infused without induction therapy.11,Angus Thomson (U of Pittsburgh) reported on a phase I/II trial of pretransplant infusion of donor-derived regulatory dendritic cells in living donor liver transplantation (LTx) recipients. Fifteen patients received regulatory dendritic cells 1 wk before transplant12 under triple IS. Eligible participants underwent ISW based on 1-y protocol graft biopsy. Three of 13 enrolled patients have remained off all IS for >1 y so far, with clean 1-y biopsies.,Qizhi Tang (University of California, San Francisco) summarized their decade-long Treg therapy experiences demonstrating safety, persistence, and stability of Treg after infusion. She reported on the recently completed phase I/II trial (“Donor Alloantigen Reactive Tregs (darTregs) for Calcineurin Inhibitor (CNI) Reduction [ARTEMIS]”), designed to assess the ability of donor-specific Treg to enable IS minimization 2 to 6 y following living donor LTx. Safety and persistence of infused Treg were demonstrated, but poor Treg expansion precluded sufficient enrollment to assess efficacy. Generalized Treg activation, senescence, and selective reduction of donor reactivity after LTx may be responsible for the manufacturing challenge. Functional evidence for loss of donor-reactive conventional CD4 and CD8 T cells13 confirms alloreactive T-cell receptor (TCR) tracking studies demonstrating deletion of donor-reactive clonotypes.14,Sandy Feng (UCSF) reported on ISW trials in children with LTx. Significant proportions of eligibility biopsies show subclinical inflammation with interface activity and a transcriptional profile of attenuated T cell–mediated rejection.15 Among subjects who successfully discontinue IS and meet biochemical criteria for operational tolerance, some biopsies fail histological criteria for tolerance.16 Two novel predictors of failed ISW have emerged: (1) de novo donor-specific antibodies during IS reduction16,17 and (2) an increased number of immunological synapses between antigen-presenting cells and lymphocytes16,18 in the eligibility biopsy.,Josh Levitsky (Northwestern University) reported results of the Evaluation of Donor Specific Immune Senescence and Exhaustion as Biomarkers of Tolerance Post Liver Transplantation (OPTIMAL) trial (ITN056ST; PI: J. Markmann), a multicenter, prospective, open-label, noncontrolled study in which 61 liver recipients underwent gradual ISW without immunomodulation. Patients expected to have a >50% rate of ISW success based on age and time posttransplant in a prior study19 were recruited. Only 16% achieved full ISW without rejection, comparable with results of the Liver Immunosuppression Free Trial (LIFT) in Europe (13/80 successfully withdrawn; A. Sanchez-Fueyo et al, unpublished personal communication, April 28, 2022). These data, along with the A-WISH study,20 support the need for immunomodulatory strategies to achieve full ISW. James Markmann (MGH) reported on the generation of donor-specific Treg generated in mixed lymphocyte reaction cultures supplemented with belatacept.21 Three patients in The ONE Study KTx trial and 3 in the “Liver Transplantation With Tregs at MGH (LITTMUS)” LTx trial received Treg. All 3 recipients of Treg early post-KTx have successfully been weaned to tacrolimus monotherapy by 11 mo and remain rejection-free now for >6 y posttransplant. Notably, each patient demonstrated Treg-rich aggregates on biopsy, suggesting homing of the donor-specific Treg to the graft. In the “LiTTMUS” trial, Treg were given for >6 mo posttransplant, following lymphodepletion by cytoxan. Of 36 consented patients, 19 failed screening criteria, often because of the unavailability of splenocytes from deceased donors. Three patients have received cell infusions, of whom 2 have been weaned but later returned to IS. The third patient was successfully weaned <1 y ago.,Alberto Sanchez-Fueyo (King’s College London) provided an update on 3 clinical trials of ISW in stable LTx recipients. The biomarker-guided “LIFT” ISW trial in patients for >3 y posttransplant has just been completed. “Low-dose IL-2 for Treg Expansion and Tolerance (LITE)” explored the capacity of low-dose interleukin-2 given 2 to 6 y posttransplant to promote Treg expansion in liver recipients on tacrolimus monotherapy. Although low-dose interleukin-2 effectively expanded circulating Treg, it did not promote their trafficking to the transplanted liver but primed the allograft for rejection, which led to the trial’s early termination.22 “Safety and Clinical Activity of QEL-001 in A2-mismatch Liver Transplant Patients (LIBERATE)” is an ongoing phase I/IIa trial sponsored by Quell Therapeutics that explores the safety and biological activity of anti–HLA-A2 chimeric antigen receptor-engineered Treg23 in patients 1 to 5 y posttransplant.,Tomoaki Kato (Columbia University) summarized mechanistic studies suggesting that the infusion of donor CD34+ cells into intestinal transplantation (ITx) recipients at the time of peak lymphohematopoietic graft-versus-host response (LGVHR) might lead to durable mixed chimerism and tolerance in ITx recipients.24–27 In a pilot study (Ossium Health), 1 patient received donor CD34 cell infusion 11 d following transplantation and demonstrated high levels of multilineage peripheral blood chimerism.,Paul Szabolcs (University of Pittsburgh) reported preliminary studies in patients with primary immunodeficiency diseases and pulmonary failure who received donor BM following bilateral orthotopic lung transplantation. Following a previous successful 2010 index case, 6 patients received BMT subsequent to deceased donor lung transplants. Three of the 6 recipients are alive, and 1 patient with persistent mixed chimerism has been off IS for >3 y, demonstrating successful tolerance induction in a challenging disease setting.,Strategies to induce tolerance in nonhuman primate (NHP) allogeneic liver, kidney, and islet transplantation were discussed. Previous data suggested that transient chimerism induces tolerance in approximately 70% of recipients of CKBMT but not in recipients of BMT with nonrenal organ transplants. Achieving durable chimerism without graft-versus-host disease (GVHD) across major histocompatibility complex (MHC) barriers has been a major challenge.,Adam Griesemer (Columbia University) reported long-term survival off IS among NHPs receiving LTx with or without BMT and peritransplant depletion of memory T cells. Dixon Kaufman (University of Wisconsin) showed that belatacept augmented transient mixed chimerism rates in TLI/ATG-treated monkeys receiving haplotype mismatched KTx, although this approach did not improve tolerance induction. Kazuhiko Yamada (Columbia University) demonstrated >365 d of durable multilineage mixed chimerism without GVHD with kidney allograft tolerance following high-dose PBSC transplantation and delayed KTx across haplotype MHC barriers. This first-in-primate achievement involved minimal TBI with a short course of costimulatory blockade and tacrolimus. Thomas Fehr (University of Zurich) presented his work with Tatsuo Kawai using inhibitors of the antiapoptotic protein B-cell lymphoma 2 to promote chimerism with minimal TBI, achieving renal allograft tolerance in 5 of 6 monkeys. Bernhard Hering (University of Minnesota) reported long-term tolerance of MHC class-I disparate islet allografts in NHPs using a nonchimeric tolerance strategy involving peritransplant infusion of apoptotic donor leukocytes under short-term anti-CD40, rapamycin, etanercept, and tocilizumab treatment.,Thomas Spitzer (MGH) presented his experience with CKBMT in patients with hematologic malignancies and end-stage renal disease in HLA-identical and, with posttransplant CY (PTCy), haploidentical donors. This approach can attack the malignancy through a graft-versus-tumor effect while permitting KTx that would otherwise be precluded because of the underlying malignancy. Immune tolerance has been achieved with transient or durable chimerism.,Ephraim Fuchs (Johns Hopkins University) discussed the use of PTCy to control GVH and host-versus-graft (HVG) responses after partially HLA-mismatched (HLA-haploidentical)–related donor PBSC or BMT. The PTCy platform was successful in the treatment of hematologic malignancies and nonmalignant hematologic disorders. For nonmalignant indications, the combination of pretransplantation rabbit ATG and PTCy achieved durable engraftment with <10% acute or chronic GVHD, suggesting an approach to achieving tolerance to solid organ allografts.,Ran Reshef (Columbia University) focused on strategies to modify lymphocyte migration for GVHD prevention by blocking chemokine receptors such as C-C motif chemokine receptor 5, which seems to reduce visceral GVHD. Whereas short-term C-C motif chemokine receptor 5 blockade did not achieve statistically significant benefit in a prospective randomized trial, further studies suggested that extended administration may be advantageous. Trials of integrin blockade and fingolimod were also discussed.,Everett Meyer (Stanford University) presented updates on the development of Treg-based therapies for the prevention of GVHD. Prophylactic Treg administration improved GVHD- and relapse-free survival compared with contemporaneous controls.,Megan Sykes (Columbia University) summarized a TCR sequencing–based method identifying and tracking alloresponses. Long-term tolerant renal recipients showed the deletion of donor-specific T cells28 and early expansions of circulating donor-specific regulatory T cells.29 TCR tracking in both the GVH and HVG directions has provided evidence that LGVHR promotes multilineage chimerism in ITx recipients while attenuating HVG-reactive T cells.24,26,27 Single-cell RNA sequencing demonstrated the effector function of GVH-reactive T cells migrating to recipient BM, demonstrating LGVHR in situ.,Manikkam Suthanthiran (Cornell University) described how urine provides a glimpse into the status of the entire renal allograft. Urine-based mRNA profiling studies have demonstrated that (1) increased abundance of granzyme-B mRNA and perforin mRNA is associated with rejection; (2) increased urinary cell FOXP3 mRNA correlates with successful resolution of rejection; (3) a urinary cell 3-gene signature of T-cell CD3ε chain, interferon-gamma-inducible protein 10, and 18s rRNA is diagnostic of subclinical and clinical cellular rejection; and (4) increased urinary CD20 mRNA and increased ratios of cytotoxic T-lymphocyte–associated protein 4 to granzyme-B mRNA are associated with kidney allograft tolerance.30,31,James Mathew (Northwestern University) summarized studies addressing the hypothesis that sustained and sequential immunoregulation, anergy, exhaustion, and senescence may lead to deletional tolerance.32 He shared preliminary data suggesting increased donor-reactive T cells in blood and urine as diagnostic/predictive of rejection.,Emmanuel Zorn (Columbia University) discussed what alloantibodies “really” recognize. Donor HLA and minor histocompatibility antigens and self-antigens are all targets of antibodies associated with graft loss.33,34 Antibodies secreted by graft-infiltrating plasma cells during cardiac allograft vasculopathy recognize chemical modifications on self-macromolecules. These studies underscore the complexity of B-cell immunity to solid organ transplants.,Drs Bridges, Chandran, and Shaw (National Institutes of Health) presented updates from the National Institute of Allergy and Infectious Diseases Clinical Trials in Organ Transplantation, the Immune Tolerance Network, and the NHPCSG, respectively. Clinical Trials in Organ Transplantation in Children and Adults awards in 2021 included investigations of drug minimization, tolerogenic strategies, and reduction of IS-related morbidity in adults and children. The Immune Tolerance Network update focused on the design of future adaptive platform trials of LTx tolerance. The Nonhuman Primate Transplantation Tolerance Cooperative Study Group aims to develop safe and effective transplant tolerance protocols in preclinical NHP models. Projects supported through this consortium have led to the development or refinement of approaches used in multiple clinical trials.,The meeting underscored the rapid progress being made in understanding tolerance and rejection, developing cell therapy and biologic approaches, and bringing them to the clinic. The authors look forward to the next iteration of this workshop, anticipated in 2024, to discuss further advances in cell therapy and other novel modalities to facilitate ISW and durable hematopoietic chimerism across HLA barriers.,The Tolerance Workshop committee gratefully acknowledges the following industry funders of this event: CareDx, CSL Behring, Eurofins Transplant Genomics, Veloxis Pharmaceuticals, 10x Genomics, Ossium Health, ITBMed Biopharmaceuticals, Talaris Therapeutics. They also gratefully acknowledge the generous philanthropic donations to NewYork Presbyterian Hospital from Justin Foa and Candida Moss, as well as Mickey Jamal. They thank Jennifer Colozzi and Julissa Cabrera for assistance with the article."
88,88,560,35969003,https://www.doi.org/10.1097/TP.0000000000004327,https://journals.lww.com/,"Coronavirus disease 2019 (COVID-19) has caused a global pandemic of unprecedented magnitude. Among hospitalized patients, about a third develop adult respiratory distress syndrome (ARDS) with an average mortality rate of 16%.1 This is often difficult to manage, requiring unusually high levels of sedation, paralytics‚ and support with extracorporeal membrane oxygenation (ECMO). In selected cases, lung transplantation can be a viable treatment option for patients with irreversible lung damage who would otherwise inevitably face death.,Since the onset of the pandemic, the volume trend for lung transplant because of COVID-19–related lung disease (CRLD) has shifted with an uncertain trajectory. Initially, rapidly increasing number of lung transplants for COVID-19–related ARDS was observed.2 With the arrival of COVID-19 vaccines, an increasing numbers of therapeutics to treat COVID-19 infection‚ and the emergence of less virulent variants of COVID-19, the transplant volume for COVID-19–related ARDS has declined. Additionally, there is increasing evidence that patients with severe COVID-19–related ARDS who require prolonged support with invasive mechanical ventilation or ECMO have the potential to recover without lung transplantation.3,4 How many survivors of severe acute respiratory syndrome coronavirus 2 may need lung transplantation for fibrotic lung disease in the future is currently unclear. The United Network of Organ Sharing, which collects and reports data on transplant volumes in the United States, established COVID-19–related ARDS and COVID-19–related pulmonary fibrosis as listing diagnoses in their database in October of 2020. Since then, a total of 364 lung transplants for COVID-19 have been reported to date, 223 for COVID-19–related ARDS and 141 for COVID-19–related fibrosis, accounting for almost 10% of the entire US lung transplant volume in 2021, making it the third most common indication after interstitial lung disease and emphysema in that year (https://optn.transplant.hrsa.gov/data, last accessed June 6, 2022). Despite this, the body of literature to guide practice remains sparse and is largely limited to short-term follow-ups and single-center experiences.,Most available case reports on lung transplant for CRLD were published in the early phase of the pandemic, and the majority of recipients had COVID-19–related ARDS. Early reports highlight concerns regarding disease transmission and reactivation of severe acute respiratory syndrome coronavirus 2 in the allograft. Although successful lung transplantation has been reported in a patient with a positive COVID-19 polymerase chain reaction (PCR) at the time of transplant,5 most centers mandate 2 negative COVID-19 PCRs at least 24 h apart before listing for transplant. To our knowledge, no cases of COVID-19 reactivation have been reported in lung transplant recipients. Since donor-derived COVID-19 infection of a lung transplant recipient and participating operating room personnel has been reported even in the setting of negative COVID-19 PCR testing from the upper respiratory tract,6 potential lung donors in the United States are now required to undergo nucleic acid testing from the lower respiratory tract (optn.transplant.hrsa.gov).,The timing of transplant in patients with COVID-19–related ARDS is highly challenging and has varied from 41 d7 to 6 mo8 from the time of diagnosis in the published literature. The potential for delayed recovery has to be weighed against the risk of complications from prolonged critical illness. Experts suggest a minimum of 4 to 6 wk between the onset of COVID-19 ARDS and consideration for lung transplant and recommend additional evidence of irreversible structural lung damage.9 This time frame may be insufficient in some patients who have the potential for delayed recovery.3,4,Most evaluations for lung transplant candidacy occur on relatively stable patients in the outpatient realm. In contrast, transplant evaluations for potential candidates with CRLD frequently occur in an intensive care environment in patients with very high acuity and dependence on life support. These circumstances can necessitate significant variance to the transplant evaluation and interfere with the reliability and accuracy of neurocognitive and psychosocial assessments. In most case reports where such information was provided, patients were able to give consent, but it remains questionable how much critical illness affected patients’ ability to understand the complexities and risks that were involved in a transplant for CRLD. Inability to provide consent should be regarded as an absolute contraindication to the procedure unless there are exceptional circumstances. Limited data suggest that patients with CRLD who are unable to consent may be more prone to posttransplant depression, especially in cases of a prolonged and difficult recovery.7 Noncompliance may complicate the posttransplant course, especially in patients without prior medical history and need for long-term follow-ups.,Deconditioning and frailty as a result of prolonged critical illness are common barriers to transplant candidacy in patients with CRLD and have previously been associated with adverse outcomes in lung transplant recipients.10 Mobilizing critically ill patients with CRLD is highly resource and labor intensive‚ requires expertise of the care personnel‚ and may be unfeasible in patients who are too unstable. Ability to participate in physical therapy is generally considered essential to consider a patient with CRLD as a transplant candidate, but the degree of physical impairment in reported cases is highly variable. Many of the reported recipients were not ambulatory at the time of surgery,8,11 which is likely a major contributor to prolonged recovery times in these patients. Awake rehabilitation and the ability to maintain ambulation have previously been associated with improved outcomes in non-COVID patients requiring ECMO as a bridge to lung transplantation.12-14,Frequently, requests to evaluate patients with CRLD for lung transplant originate from hospitals without transplant capabilities. The majority of referrals will not be suitable for transplantation, and selecting appropriate candidates for transfer to a transplant center can be very challenging, especially if the patient requires additional life support to facilitate a safe transport. Several centers have proposed a protocolized interdisciplinary approach incorporating telemedicine assessments before accepting a patient for transfer.15,Most of the available literature commented on the unique intra operative challenges in patients with CRLD. The severe parenchymal destruction, extensive pleural involvement‚ and reactive lymphadenopathy often result in very difficult mobilization of the native lungs, resulting in longer surgery and cold ischemia times, increased need for intraoperative blood products, and a higher incidence of primary graft dysfunction.16 Secondary colonization or infection with bacterial or fungal organisms is a common complication of prolonged critical illness, mechanical ventilation, and pleural instrumentation. It is largely unclear how this affects posttransplant outcomes, although some cases of postoperative morbidity and mortality related to bacterial17 and fungal sepsis16 have been reported.,Most of the early available case reports only document short-term posttransplant outcomes on single patients or case series.7,8,11,16-18 Very recently, analyses of larger patient cohorts over a longer time span have been published: A retrospective analysis of the United Network of Organ Sharing database reported outcomes on 214 patients who received lung transplantation for COVID-19–related respiratory failure in the United States2 with a median follow-up of 1.9 mo. Patients were relatively young (mean age 52 y) and predominantly male (79.2%)‚ and included a high percentage of patients of Hispanic origin (36.6%), consistent with reports that have identified a higher risk of COVID-19–related respiratory failure in this group. Almost two thirds of the patients required ECMO before lung transplant. The mean lung allocation score was 87.5, indicating very high priority for donor lungs. The reported 30-d survival was 97.8% and approached that of patients who were transplanted for other indications. Another report summarized the experience of 30 consecutive patients who received lung transplantation for COVID-19–related ARDS at a single center and compared it to 72 patients with non-COVID-19–related end-stage lung disease in the same time period.19 Patients in the COVID-19 group were younger (53 versus 62) and had higher lung allocation scores (85.8 versus 46.7), shorter time on the transplant waitlist (11.5 versus 15 d), higher need for intraoperative blood transfusions (6.5 units of packed red blood cells versus 0), higher incidence of primary graft dysfunction (70% versus 20.8%), and need for permanent renal replacement therapy (13.3% versus 5.5%). Impressively, all 30 patients in the COVID-19 group were still alive at the time of the report with a median follow-up of 351 d.,The amount of available literature on lung transplantation for CRLD remains sparse. Limited experience exists in regard to patients who survive COVID-19 but develop pulmonary fibrosis necessitating consideration for lung transplantation at a later time.20 Even less is known about patients with preexisting lung disease‚ where COVID-19 exacerbates respiratory failure and leads to potential transplant candidacy.,The existing transplant registries are designed to capture demographics, clinical variables‚ and transplant-related outcomes for all organ donors and recipients. The International Society of Heart and Lung Transplantation is the only organization that tracks lung transplant–related outcomes globally. In the United States, transplant centers are required to report their outcomes to the Scientific Registry of Transplant Recipients. Both registries do not address many of the unique challenges in lung transplantation for CRLD regarding candidate selection, pretransplant functional status and comorbidities, and infectious complications and postoperative complications. For instance, duration of mechanical ventilation and ECMO support—both important metrics that can affect posttransplant outcomes in these patients—are not monitored by existing transplant registries. The global registry that is maintained by the Extracorporeal Life Support Organization has tracked outcomes of COVID patients who require ECMO support, but the database is lacking specifics about patients that proceed to lung transplantation (www.elso.org). However, adding variables to the existing databases to capture the circumstances of lung transplants for CRLD more accurately has limited relevance for other disease groups and is likely not feasible.,Given the above, our group has created a global database to obtain detailed clinical information on lung transplants that have been performed for CRLD. It is administered by the Northwestern University Feinberg School of Medicine and has received Internal Review Board approval. The database is open access, and transplant centers across the world have been invited to submit retrospective cases electronically. Because no protected health information is obtained, institutions should not require permissions from their home Internal Review Boardss or data usage agreements before entering cases. Most of the requested information is specific to the diagnosis of CRLD, including details on COVID-19 therapy before transplant, duration of ECMO and ventilator support, functional status, patient ability to consent, intraoperative requirements, secondary infections, and postoperative complications and outcomes (https://www.feinberg.northwestern.edu/sites/nutorc/Lung%20Transplant%20Registry/index.html). As of mid-May of 2022, a total of 89 cases have been entered into the registry: 75 from the United States, 10 from Europe, and 4 from South America. We believe that capturing the experience of lung transplant programs of different sizes from different countries offers important perspectives and will help guide clinical practice for this patient population in the future and encourage transplant centers to share their expertise in transplanting patients with this challenging condition."
89,89,568,37122085,https://www.doi.org/10.1097/TP.0000000000004600,https://journals.lww.com/,"In the current issue of Transplantation, the first ever publication on uterus transplantation (UTx) in males is published.1 The report marks a start for a very important area of research within the fields of transplantation and infertility.,In 2012, the Swedish team at the University of Gothenburg started the first clinical trial of UTx as a possible treatment for absolute uterine factor infertility (AUFI) in women.2 Women with AUFI have either congenital/surgical uterine absence or any abnormality (anatomic/functional) that impedes pregnancy. The proof-of-concept of UTx as an infertility treatment for women with AUFI came with the first live birth after UTx, which occurred in Sweden in September 2014.3 In that case, a 35-y-old woman with congenital uterine absence, as part of the Mayer-Rokitansky-Küster-Hauser syndrome, had been transplanted the year before with a uterus from a 61-y-old friend. A total of 9 live births, from 6 transplanted women, occurred from this Swedish live donor (LD) UTx trial of 2012–2013, which included 9 LD-UTx procedures.2 Already for this initial UTx introduction, the reproductive efficacy of the procedure proved to be high, with a 100% cumulative clinical pregnancy rate and 86% cumulative take-home-baby rate, among the study participants that underwent pregnancy attempts.4 Today, >90 UTx procedures have been performed worldwide, with >50 live births achieved.5,6,The great clinical success of female UTx within the initial clinical trial2 and in following clinical trials of both LD and deceased donor UTx procedures5,6 is most likely because of the meticulous and long-term research preparations before the commencement of the first clinical UTx. The Swedish team started already in 1999 an animal-based research project on UTx, initially in rodents but later also including large domestic species and nonhuman primates.7 This was to optimize and safeguard the procedure before introduction in 20122,4 as an experimental surgical procedure for female AUFI.,The UTx was initially intended as an infertility treatment for female AUFI, with the potentially largest groups being those with congenital uterine absence, as part of Mayer-Rokitansky-Küster-Hauser syndrome, or women of fertile age that had undergone hysterectomy due to cervical cancer, postpartum bleeding, or leiomyoma. However, the panorama of possible patient groups that may benefit from UTx has expanded and today also includes 2 distinct groups of women, with male (XY) genetic setups. Women of these groups have expressed interest in UTx as a possibility to gestational motherhood both by personal contacts to teams undertaking UTx trials and also with the interest documented in scientific publications.8,9 These 2 potential UTx groups with male chromosomes are women with complete androgen insensitivity syndrome (CAIS) and transsexual women that have undergone male-to-female (MTF) gender affirmation treatment. The CAIS condition is rare (<1:20 000) and occurs because of complete resistance to the action of androgens due to mutation(s) in the androgen receptor, causing the fetus and child to develop with normal external female phenotype but with absent internal genitalia and presence of testes.10 At puberty, there will typically be normal female breast and female distribution of adipose tissues, but estrogen treatment is needed for the full development of secondary sexual sex characteristics, such as pubic hair. Later on, vaginal dilatation and orchidectomy, because of the risk of malignant transformation of the gonads, may be needed. Concerning transsexual MTF, feminizing hormone therapy (antiandrogens and estrogen) will usually be started well before gender affirmation surgery, including breast augmentation, penilectomy, orchidectomy, and construction of vagina and labia.11 In many MTF women, sperm cryopreservation may occur before orchidectomy. A UTx procedure in a transwoman would provide the possibility for gestational motherhood and for genetic motherhood, the latter in the event that her cryopreserved sperms are used in the in vitro fertilization procedure, with donor oocytes or oocytes from a female partner.,The article by Yang and colleagues1 is a very good start for expectantly long-term research interest, by the Chinese group and other research teams, to investigate the feasibility of UTx in various male animal models toward human experimental clinical trials in transwomen or women with CAIS. My prediction is that there will be a 5- to 10-y research period before we are ready for human experimental trials in this area of UTx-in-male or before the research findings have found that it is not advisable to test this in humans. The research pioneers of male UTx from Xi’an, China,1 used a rat model with transplantation from Spraque-Dawley female rats into the groin area of orchidectomized male rats of the same inbred strain. The UTx method, with retrieval including excision of 1 uterine horn and creation of unilateral vascular pedicles of the common iliac, was adopted from our studies in the mouse and rat for female UTx.12,13 In the latter female animal models, the uterus was positioned intraabdominally with end-to-side anastomoses to the aorta/vena cava and common iliac in the mouse and rat, respectively. In the study by Chang and coworkers,1 anastomoses were on the femoral vessels. In this initial report, there was high-perioperative/postoperative mortality, and only 6 of 13 transplanted rats survived for evaluation 30 d after UTx. This shows that further improvements are needed in the surgical approach used and in postoperative management. Importantly, this methodological article1 showed that the histology of the uterine grafts was normal and thus demonstrated that this unilateral UTx surgical method, or modifications of this, can be used in further studies on UTx in males. The authors also conclude that the results may provide a reference for further studies of transsexual UTx in animals.,Human UTx in males may raise both ethical concerns and may involve challenges relating to the anatomical, physiological, and genetical differences between females and males. Concerning ethics, we have to weigh the potential benefits of a UTx-in-male intervention against its biophysical risks. However, it should be ethically apparent that the reproductive goals of MTF transgender women warrant equal consideration to those assigned cis females, as described as “that the principle of autonomy is not sex-specific” in the Montreal Ethics Criteria of UTx.14 It should be pointed out that in the event that UTx-in-male research in animals and humans is successful and that UTx-in-female before that develops into a clinically accepted treatment for female AUFI, refusal to perform the UTx procedure based on the presence of XY chromosomes may face legal and moral obstacles.,As mentioned above, there are differences between males and females, which may present difficulties in the development of male UTx. There is a difference in the shape of the female gynecoid pelvis compared with the narrower male android pelvis. This may influence the risk for abnormal fetal positioning during pregnancy in a male, as well as make the embryo transfer procedure, to achieve pregnancy, more difficult. Moreover, the transplanted uterus in a female pelvis would be supported with attachment to the round ligaments and sacrouterine ligaments, which are not present in a male pelvis.,An obvious fact is that orchidectomy should occur before any pregnancy attempt in an MTF transwoman, because antiandrogen treatment will increase the risk for genital birth defects, and in CAIS, because the fetus will have functional androgen receptors. Also, the effects of long-term androgen treatment on the uterus and its vasculature have to be taken into account in the event that the donor uterus to an MTF woman is from donor-hysterectomy in a female-to-male man.,One of the most challenging issues in male UTx would be possible negative influence by the type of vagina. A neovagina created in a male would not be lined by the normal vaginal epithelium because the vagina would be either an intestinal vagina, a split-skin-graft McIndoe vagina, or a vagina created by inversion of the skin after penilectomy. The altered vaginal microbiome, caused by the milieu of the abnormal vaginal epithelium, may negatively influence implantation after embryo transfer and may also lead to increased risks for ascending infections and septic abortions in the immunosuppressed uterus recipient.,It should be emphasized that pregnancies in a male will most likely have an unphysiological epigenetic influence on the fetus, and this may cause negative effects on development and health during childhood and adulthood. Thus, this important issue has to be thoroughly studied in animal models.,Taken together, because of anatomical, hormonal, fertility, obstetrical, and genetic complexity involvements, it is strongly recommended that any UTx attempt in genetically XY humans should be preceded by systematic animal research to maximize safety and efficacy, as was performed for genetically XX females. The article by Yang and colleagues1 is a first step in that direction."
90,90,572,36584367,https://www.doi.org/10.1097/TP.0000000000004417,https://journals.lww.com/,"Of all organs retrieved for transplantation from deceased donors, the heart is the most susceptible to ischemic injury. Moreover, this susceptibility increases with increasing donor age.1 As a consequence, many heart transplant programs place an upper limit on donor age of 60 y or even less. For these reasons, hearts are retrieved from a minority of deceased donors.2 Although the advent of heart transplantation from donation after circulatory death (DCD) donors offers hope of expanding the donor pool,3 the large majority of donors’ hearts are retrieved from heart-beating donation after brain death (DBD) donors. In the latter scenario, hearts are arrested by the administration of 1 L of cold preservation solution. Following retrieval, the heart is placed in a bag containing 1 L of cold crystalloid solution. This bag is then placed in a second bag also containing 1 L of cold solution and then packed in a portable ice box for transport from the donor to recipient hospital.4 This technique has provided reasonably reliable donor heart preservation for periods up to 3 h; however, as the ischemic time increases further, there is a progressive increase in the risk of severe early graft dysfunction and associated mortality.5,Historically, donor heart preservation research largely focused on modifications to the cardioplegia and storage solution with the aims of minimizing ischemic injury, preventing the development of myocardial edema during storage, and preventing reperfusion injury following implantation. Currently, all donor heart preservation solutions rely on profound hypothermia to slow myocardial metabolism. Although myocardial metabolism is slowed‚ it is not arrested. In the absence of oxygen delivery, myocardial viability is maintained via anaerobic metabolism‚ which rapidly depletes high energy stores and results in progressive lactic acidosis. This in turn activates the sodium hydrogen exchanger, resulting in intracellular sodium and then calcium overload. More recently, it has been found that lactic acidosis also activates necroptosis via an acid-sensing ion channel (ASIC1a) in the cardiomyocyte membrane.6,Placing the donor heart in a bag of cold solution and then in a portable ice box is simple but not without its problems. Direct contact of the myocardium with ice can produce freezing injuries with myocyte necrosis and early graft dysfunction. Conversely, poor insulation may allow excessive temperature rise during transport and subject the heart to warm ischemic injury. In this edition of the journal, Zhu and colleagues from Stanford University report their experience with the Paragonix SherpaPak temperature-controlled hypothermic storage system for donor heart preservation and transport.7 Its main stated advantage over traditional static cold storage (SCS) is more precise temperature control during transport with the heart maintained between 4.5° and 6.5° centigrade. The authors switched from SCS to using the SherpaPak system (Paragonix Technologies, Cambridge, MA) in June 2020. They compared the outcomes of the first 62 heart transplant recipients with a historical cohort. Overall, recipients of hearts transported with the SherpaPak device were older‚ and the mean total allograft ischemic time was significantly longer in the SherpaPak group at just over 4 h. Despite these unfavorable baseline characteristics, postoperative requirement for perioperative transfusion of blood products was lower in the SherpaPak group‚ and major clinical outcomes‚ including primary graft dysfunction, hospital length of stay‚ and survival‚ were similar between groups. Although the findings appear promising‚ there are several limitations. Firstly‚ this is a single-center retrospective analysis with small numbers. Furthermore, as acknowledged by the authors, their findings are confounded by the fact that the surgical team modified their surgical implant technique after the introduction of the SherpaPak system (although unrelated to the use of SherpaPak), resulting in a significantly shorter warm ischemic time for donor hearts retrieved with the SherpaPak system.,The manufacturer has established an observational postmarket registry (called the Guardian Heart Study, NCT04141605) to monitor the clinical outcomes of heart transplant recipients whose hearts have been retrieved using the SherpaPak system. According to the manufacturer’s website, >1700 heart transplants have been performed using the SherpaPak system across 60 Heart Transplant Units in the United States and Europe. An abstract of the first 383 US patients included in the registry reported lower rates of severe primary graft dysfunction and a trend of improved survival for recipients of hearts retrieved with SherpaPak compared with SCS.8 Unfortunately, the registry shares many of the limitations of the study by Zhu and co-authors. As Zhu and co-authors conclude in their study, an adequately powered randomized multicenter randomized trial is warranted to establish whether the SherpaPak system provides superior donor heart preservation compared with SCS. Given the rapid uptake of SherpaPak, by multiple Transplant Units, such a trial appears quite feasible and would provide more compelling evidence than an observational registry.,SherpaPak is recommended for the storage of standard criteria donor hearts for up to 4 h. There are still many donor hearts that are not covered by these recommendations‚ including DCD hearts, hearts from older DBD donors, donor hearts requiring high levels of inotropic support, and those demonstrating impaired global or regional left ventricular function. A study of the use of SherpaPAk in the transport of DCD hearts that have been resuscitated in situ using thoracoabdominal normothermic regional perfusion has been registered on the clinical.gov website (NCT05038943)‚ but no results have as of yet been reported.,The other major innovation in donor heart preservation has been in machine perfusion‚ whereby an oxygenated solution is perfused through the heart at either hypothermic or normothermic temperature.9 Normothermic machine perfusion has enabled direct procurement and resuscitation of hearts from DCD donors and may also enable successful transplantation of marginal hearts from brain-dead donors. Initial experience with hypothermic machine perfusion also shows promise‚ suggesting improved preservation of hearts from DBD donors.10 Both technologies are considerably more expensive than SherpaPak, which in turn is substantially more expensive than SCS. Nonetheless, if these innovations in donor heart preservation improve the quality of donors’ hearts and expand the donor pool, thereby making heart transplantation safer and more available to patients with end-stage heart failure, then the expense will be worth it."
91,91,575,36706066,https://www.doi.org/10.1097/TP.0000000000004499,https://journals.lww.com/,"Following syngeneic or autologous stem cell transplantation, a syndrome closely resembling allogeneic graft versus host disease (GvHD) occurs in 5% to 20% of recipients,1,2 although there are no genetic differences between the cell therapy product and the host tissues. In a cohort of patients receiving transplants from an identical twin to treat aplastic anemia, GvHD-like features only developed among those recipients treated with pretransplant conditioning.3 Moreover, in allogeneic stem cell transplantation, host resident memory T cells persist in peripheral tissues even after myeloablative conditioning and contribute to the pathogenesis of GvHD.4 These findings imply that the inflammatory response consequent upon pretransplant conditioning can trigger T-cell autoreactivity. Among the factors that have been suggested to contribute to this phenomenon are destruction with inadequate reconstitution of T regulatory cells (Treg) in the peritransplant period, and disruption of the gut microbiome with reduced diversity and impaired production of short-chain fatty acids (reviewed in Rafei and Jenq5). Here, we propose an additional mechanism to account for these findings.,Differences in minor histocompatibility antigens (MiHA) between the donor and recipient of hematopoietic stem cell transplants are well-recognized precipitants of GvHD in the setting of HLA-matched sibling and unrelated donation. MiHA arise through genetically encoded polymorphism in the source proteins of the endogenous peptide antigens that are presented by donor and recipient HLA. The capacity of MiHA to precipitate GvHD reflects the sensitivity of the donor immune cell precursors toward a small proportion of peptide antigens. By analogy, we hypothesize that pretransplant conditioning can trigger donor T-cell reactivity against the host through the generation of both conventional and unconventional peptide epitopes that are not present in the thymus during T-cell development. Autoreactive T cells able to recognize this “induced/altered self” are not removed by negative selection6 and persist within the mature T-cell repertoires of both the donor and recipient. Observations made in the context of autologous or syngeneic transplantation are also relevant to matched and haploidentical stem cell transplantation and solid organ transplantation in which matched and allogeneic HLA are often coexpressed by the donor organ. In these settings, autoreactive T cells could synergize with T cells responding to MiHA and with those directly or indirectly recognizing allogeneic major histocompatibility complex (MHC) to augment damage to target tissues. Both memory T cells and previously naive T cells could contribute to this phenomenon. In addition to self-restricted self-peptides and indirectly presented peptides processed from allogeneic donor MHC, self-peptides directly presented by donor MHC or MiHA could also be subject to induction/modification but, although they may influence the alloresponse, they would not be targets for autoreactivity.,Transplantable organs are subjected to many sources of cellular stress (Figure 1). Donor brain death is accompanied by a catecholamine storm, erratic perfusion of peripheral tissues, and a surge in proinflammatory cytokines and chemokines.7 Circulatory death exposes donor organs to an extended period of warm ischemia,8 whereas some degree of ischemia-reperfusion injury affects all solid organ transplants. Recipient preparation for hematopoietic stem cell transplantation typically involves both irradiation and administration of cytoreductive chemotherapy. The early stages of alloreactive T-cell responses, whether directed toward the graft or the host, are marked by the release of cytokines and other soluble mediators that act upon the target tissues. These factors may alter tissue proteomes and immunopeptidomes in various ways. Among these are the generation of proteasome-spliced, or “hybrid” peptides,9 and immunoproteasome-dependent peptides, modification of the amino acid side chains of peptides or their source proteins,10-13 and the utilization of noncanonical start codons and alternative open reading frames (ORFs)14,15 for protein translation, in parallel with the standard templates. The resulting “dark proteome” yields novel peptide epitopes termed “cryptic” because standard analysis pipelines do not reveal them.16 Beyond directly altering the interacting surfaces of the T-cell receptor-peptide-MHC (pMHC), posttranslational modification (PTM) of proteins can perturb classical antigen processing pathways, exposing epitopes that are normally degraded before MHC loading. An increased density of previously rare epitopes may be sufficient to stimulate weakly reactive T cells past the threshold of activation.17,18,Exposure of cells to interferon gamma (IFN-γ) increases the diversity of the peptide antigens presented by MHC molecules (ie, the immunopeptidome), substantially augmenting presentation of both conventional linear and proteasome-spliced peptides.9 IFN-γ upregulates expression of numerous potential source proteins and induces transcription of the specialized immunoproteasome subunits β1i, β2i, and β5i.19 Immunoproteasomes differ in proteolytic activity from constitutive proteasomes, giving rise to more hydrophobic peptides,19 some of which are exclusively produced by the immunoproteasome. Although medullary thymic epithelial cells express both constitutive proteasomes and immunoproteasomes,20 participation of immunoproteasome-dependent self-peptides in negative selection may be limited. Immunoproteasome-dependent antigens have been implicated in the development of autoimmunity,21 whereas selective inhibition of the b5i subunit blocks development of diabetes in a model in which an immunoproteasome-dependent epitope is expressed in pancreatic islets.22 Consistent with immunoproteasome activity, conventional self-peptides that do not cause negative selection in the thymus have been detected in cells treated with IFN and in rejecting grafts, where they were presented by syngeneic MHC.23 Proteasome-mediated peptide splicing involves the joining of 2 noncontiguous peptide segments, either from the same protein (cis-splicing) or from different proteins (trans-splicing), resulting in novel, nongenomically templated peptide antigens.24,PTM of amino acids can occur spontaneously, through exposure to reactive oxygen species, or via the action of enzymes such as acetyl or methyltransferases, kinases, ubiquitin ligases, glycosyltransferases, and tissue transglutaminase 2.17 Ischemia-reperfusion injury induces the expression of peptidyl-arginine deiminases, which convert arginine residues to citrulline,25 whereas protein cysteinylation (disulfide bond-formation with free cysteine) occurs as a consequence of acute inflammation.26 Irradiation of tumors increases neoantigen presentation and subsequently broadens the repertoire of responding T cells,27,28 suggesting that irradiation of normal tissues may also give rise to neoepitopes. Finally, oxidative stress is associated with dysregulation of protein translation14,15 promoting presentation of cryptic peptides.,The immunogenicity of PTM, spliced, and cryptic peptide neoepitopes is increasingly well documented. Deamidation of asparagine yields both aspartyl and isoaspartyl residues. Isoaspartyl modifications are linked to increased immunogenicity of self, tumor, and viral vector capsid antigens.29-31 Acetylation and citrullination feature prominently in the pathogenesis of autoimmune disorders,18,32,33 whereas cysteinylation and disulfide bond rearrangements have been implicated in both alloreactivity and autoreactivity.34,35 Tumor antigens derived from alternative ORFs elicit CD8+ T-cell responses,36,37 and an H-2Kb-restricted self-peptide derived from an upstream ORF of α-tubulin is recognized by alloreactive T cells.38 Immunogenic spliced peptides have been isolated from human renal cancer,39 melanoma cells,40 acute myeloid leukemia,41 and murine cells infected with Listeria monocytogenes,42 whereas hybrid epitopes formed by the splicing of insulin peptides with fragments of other islet proteins are recognized by autoreactive T cells from human subjects with type 1 diabetes.43 The altered self-peptide concept of de novo autoreactivity in transplantation does not exclude the contributions of dysbiosis or Treg loss to autoimmune components of GvHD or rejection; rather, these factors may act synergistically to promote autoreactivity.,Although it has been well recognized that the graft microenvironment influences transplant outcomes, this has been viewed primarily as a consequence of innate immunity through mechanisms such as the promotion of leukocyte infiltration, upregulation of costimulatory ligand expression on antigen-presenting cell, and interference with Treg functions. Recently, intrarenal B-cell populations from kidneys undergoing chronic rejection were shown to contain cells recognizing tissue-specific autoantigens (rather than alloantigens), some of which were only present in inflamed kidneys.44 Given that these antibodies were isotype-switched and extensively mutated, a role for CD4+ T cells that recognize inflammation-related neoepitopes presented by B cells and promote autoantibody production through the provision of T cell help seems likely.,Based on our hypothesis, we would predict that transplanted organs procured from deceased donors would be more likely to trigger autoreactivity than those obtained through living donation. Localized tissue injury can also result in systemic inflammation,45,46 in turn setting up the conditions under which de novo autoimmunity could occur in organs other than the graft. Similarly, we anticipate that organs exposed to environmental antigens would harbor a tissue immunopeptidome incorporating more altered self-peptides with a greater propensity for recognition by autoreactive T cells. In part, this may relate to augmentation of graft inflammation due to innate signaling by microbial products, although this does not explain all the relevant findings. The half-life of male to female C57BL/6 skin grafts colonized by commensal bacteria in otherwise germ-free recipient mice is shortened compared with that of sterile grafts to germ-free recipients.47 Pattern-recognition receptor signaling alone is not sufficient to drive the acceleration of graft rejection resulting from Staphylococcus epidermidis colonization of donor skin,47 but metabolic or biochemical alterations in colonized skin might change the immunopeptidome with a resultant autoreactive response that hastens graft destruction. Mice primed against S epidermidis or Staphylococcus aureus mounted a brisk immune response against sex-matched syngeneic skin grafts colonized by these commensals, resulting in moderate graft damage in the absence of alloreactivity.48 In parallel to a memory response against the commensal organisms per se, autoreactive memory against induced/altered self-peptides could explain these findings. There may also be a role for molecules like MR1 that present bacterial metabolites.49,Discovery of immunogenic altered self-epitopes expressed by host target tissues and transplantable organs is feasible. Novel immunopeptidomic workflows enable the unbiased capture of the full spectrum of immunogenic peptides. Cryptic epitopes can be revealed using methods such as immunopeptidogenomics, whereby whole transcriptome data are translated in 3 frames and used to create a database of potential proteins resulting from use of alternative ORFs,16,50-53 whereas further recently established pipelines are suitable for the detection of spliced24,54 and other PTM55 peptides. These pipelines can be used in combination with cellular assays for the identification of immunogenic epitopes that are recognized by alloreactive and autoreactive CD8+ T cells,56 thus providing a means of testing the hypothesis proposed herein (Figure S1, SDC, https://links.lww.com/TP/C668).,Dissection of the relative contributions of altered self-peptide antigens, dysbiosis, and Treg loss to GvHD could be attempted using available experimental approaches. If T cells specific for matched MHC complexed with altered self-peptides were depleted (eg, using toxin-coupled57,58 or column-bound tetramers), any resulting amelioration of GvHD severity would suggest a functional role for those pMHC ligands, distinct from the effects of dysbiosis or Treg depletion. This could be compared with the effects of fecal microbiota transfers or regulatory T-cell transfer, although it should be noted that those interventions may also change the peptide landscape by reducing inflammation in the host tissues. Other predictions flowing from the hypothesis are also amenable to empirical testing. Mouse models of donation after brain death or circulatory death versus living donation could be combined with haploidentical (F1 to parent) organ transplantation, with examination of the tissue immunopeptidome and T-cell responses in the different groups. Broad-spectrum antibiotic treatment of donors or recipients or manipulation of extra-intestinal microbiota alone could be accompanied by the assessment of changes to the immunopeptidome and to auto and alloreactive T-cell responses.,New workflows to interrogate the full diversity of the immunopeptidome, along with a robust methodology for assessing pMHC recognition by alloreactive or autoreactive T cells, allow us to ask whether a proinflammatory or metabolically altered microenvironment causes modifications of the pMHC epitopes at the heart of allorecognition and altered self-recognition, thus rendering donor organs or host GvHD target tissues more immunogenic. Recognizing the unknown contributors to rejection and GvHD will allow us to develop strategies to address these."
92,92,576,37220336,https://www.doi.org/10.1097/TP.0000000000004479,https://journals.lww.com/,"We intend to elaborate on the current status, diversity in access, equity, and economics of solid organ transplantation (SOT) in India. Currently, India performs the third largest volume of organ transplants after the United States and China.1-3 Notably, <10% of patients with organ failure have access to SOT worldwide. The Indian Ministry of Health estimates a gap of approximately 175 000 kidneys, 50 000 livers, hearts, and lungs in addition to 2500 pancreas. TheNewsletter Transplant (2018 data) reported on 12 758 patients wait-listed for kidneys, 4173 for liver, 425 for heart, 75 for lung, 48 for pancreas, and 6 for small bowel transplants (SBTs) for in India.4 Deceased donation rates have remained stable and <1 per million population from 2013 to 2021. To achieve self-sufficiency in organ donation, the estimated deceased donation rate in India will need to achieve a rate of 62 per million population. The distinguishing feature of Indian transplant programs is their predominant reliance on living donation (87%), with only 13% of the kidney transplant volume deriving from deceased donors (DD).1-3 There are also stark geographic differences in India with active DD programs in South and West India, whereas DD has not been initiated in Northeast India. Moreover, donation after the circulatory determination of death and kidney-paired exchange programs are still underdeveloped in India (<1%) and should be promoted.,Kidney (8232 DD, 54 442 living donor, and 62 674 total) and liver (4339 DD, 12 255 living donor, 8 domino, and 16 602 total) are the predominantly transplanted organs in India and only 1322 heart transplants, 773 lung transplants, 133 pancreas transplants, 75 kidney–pancreas transplants, and 19 SBTs have been performed from 2013 to 2021 (Figure 1). There has been growth in successful outcomes of extrarenal transplants during the past decade, and deceased donation rates have increased since 2007/2008, which was probably the watershed moment in deceased donation in the country. Figure 1 shows transplant activities in India (2013–2021) as listed in the GODT with data on access to non–kidney organ transplants in India shown in Figure 2A–G). GODT: Donation and transplantation in India, Southeast Asia region, and Global (2019–2021) are shown in Table 1. During the coronavirus disease 2019 (COVID-19) pandemic, transplantation rates initially declined in India5 and gradually recovered with the implementation of National Organ and Tissue Transplant Organization transplant guidelines6,7 and the ability to cope with the pandemic. Table 2 summarizes milestones in SOT in India8-19,At the time of this report, there were <10 government institutions performing heart transplant (Figure 2B), and only 3 government hospitals (Postgraduate Institute of Medical Education and Research, Chandigarh; All India Institute of Medical Sciences, New Delhi; and Rajiv Gandhi Government General Hospital, Chennai) have an established lung transplant (LT) program in India. Until now, Rajiv Gandhi Government General Hospital, Chennai, has been performing combined heart–lung transplant among government hospitals. A total of 441 transplants LTs (Figure 2C; isolated LT, n = 339 and combined heart–lung transplant, n = 102) have been done in India until March 2021 (Tamil Nadu, n = 340; Telangana, n = 45; Karnataka, n = 29; Maharashtra, n = 23; Punjab, n = 1; Rajasthan = 1; Kerala, n = 1; and Delhi n = 1 have been performed in India). Twenty-five LTs have been done in India for COVID-19–affected lungs.20,The majority of SBTs (>50%) are currently performed in Maharashtra (Jupiter Hospital Pune, Jupiter Hospital, Mumbai, and Global Hospital, Mumbai) with additional activities at Medanta Hospital, Delhi, Apollo Hospital and Dr Rela Institute, Chennai, Osmania General Hospital, Hyderabad, and Apollo and Fortis Hospital, Bangalore (Figure 2D).,Pancreas procurement rates remain low, partly related to strict acceptance criteria.19 At present, only a few centers across the country with trained surgeons offer pancreas transplants. These centers include Postgraduate Institute of Medical Education and Research, Chandigarh (n = 38), Institute of Kidney Diseases and Research Center and Dr H L Trivedi Institute of Transplantation Sciences (IKDRC-ITS), Ahmedabad (n = 19), AIIMS, Delhi, Sahyadri Super Speciality Hospital and Ruby Hall in Pune, Apollo Hospital, Global Hospitals, Dr Rela Institute in Chennai, Global Hospitals and Apollo Hospitals, Bangalore, Jubilee Hills and Yashoda Hyderabad, Amrita Institute of Medical Sciences, Kochi, and Kovai Medical Centre and Hospital, Coimbatore (Figure 2E).,At present, only a few centers across the country with trained surgeons offer this procedure; 26 hand transplants have been performed at the time of this report. Centers include Amrita Institute of Medical Sciences, Kochi, Global hospital, Mumbai, Jawaharlal Institute of Postgraduate Medical Education and Research, Pondicherry,21 Global hospital, Chennai, KEM hospital, Mumbai, and Stanley Medical College, Chennai (Figure 2F).,More than 90 UTs have been done worldwide. India has been an early adapter of this novel approach. Two live births occurred from a total of 11 living donor UTs in India (9 UTs at Galaxy Care Private Hospital, Pune, and 2 UTs at IKDRC-ITS, Ahmedabad; Figure 2G).,The Government of India enacted the Transplantation of Human Organs Act (THOTA) in 1994 to curb organ trading and to promote deceased organ donation. Seven states in India (Andhra Pradesh, Telengana, Karnataka, Uttarakhand, Mizoram, Meghalaya, and Tripura) are yet to adopt the THOTA (Figure 2H). Seven North-eastern states (Sikkim, Mizoram, Meghalaya, Manipur, Arunachal Pradesh, Tripura, and Nagaland) and 3 union territories (Andaman and Nicobar Lakshadweep, and Ladakh) do not have a State Organ and Tissue Transplant Organization (SOTTO; Figure 2I). Three large states (Chhattisgarh, Telangana, and Uttarakhand) are currently in the process to implement a SOTTO. The North-eastern states, including Arunachal Pradesh, Mizoram, Meghalaya, Nagaland, Sikkim, and Tripura, in addition to the union territories Andaman and Nicobar, Dadra and Nagar, Haveli and Daman, Diu, and Lakshadweep do not have transplant centers, even after 75 y of India’s independence (Figure 2J). Organ Transplantation task force of National Academy of Medical Sciences pointed out that Organ Transplantation task force of National Academy of Medical Sciences pointed out that the financial grant under the National Organ Transplantation Program22 is underused because of the lack of sufficient staffing in the National Organ and Tissue Transplant Organization, the Regional Organ and Tissue Transplant Organization, and SOTTO. Clearly, more support of full-time staff focusing on organ donation and transplantation without giving additional charges to these posts will need to be allocated. Documenting outcome data will be critical for policy decisions. There is also insufficient documentation on transplant recipients and the long-term health of live donors. Compliance in these areas is lacking, although documentation is mandated by the THOTA.,Only 20% of kidney and <5% of liver, heart, lung, and multiorgan transplants are performed in government hospitals.23 High costs of transplants at private hospitals are, in general, not affordable for most patients in the absence of governmental healthcare plans. To improve the current status of organ transplantation in government hospitals in India, there is a need for improved infrastructure, funding mechanisms, effective policies, trained personnel, a better networking and organ allocation system, a unified approach for organ procurement and transport, and a nationwide registry, governed by effective and integrating leadership.24 Moreover, there is an urgent need to make organ donation and transplantation affordable in India, including the coverage of immunosuppressants, organ preservation, and required consumables. Each SOT should be covered under Governmental healthcare plans adapted to local needs. The inclusion of organ transplantation under the Ayushman Bharat Pradhan Mantri Jan Arogya Yojana health plan represents a step forward in providing Government support for SOT. Dedicated infrastructure and organ transplant departments/units staffed by trained dedicated faculty will ideally be implemented in at least 1 public sector hospital per state. Multiorgan retrieval teams should be established in all general surgery departments in parallel to educated and trained intensivists and trauma units in institutions that can initially operate as non-Transplant Organ Retrieval Centers and, if needed, be transformed into transplant centers.,Today, <10% of patients with end-stage kidney failure in India have access to transplantation. Mortality because of limited access and high costs of dialysis is therefore high. IKDRC-ITS, Ahmedabad and the Gujarat Dialysis Program have provided effective examples that may serve as models in each state, providing free-of-charge dialysis and transplants. Those efforts have recently been recognized by the opening of an additional 850-bed unit at IKDRC Ahmedabad for multiorgan transplants in addition to 188 satellite dialysis centers under the One Gujarat, One Dialysis program by Prime Minister Narendra Modi. This initiative aims to provide free of charge or subsidized kidney and transplant care providing Gujarat Government supported universal health.,The clinical implementation of SOT in India has been laborious and great disparities in access to transplantation remain. Transplant activities rest largely on living donation, an approach that will not be sustainable for kidney and liver transplantation and also not available for most other organs. Moreover, there is disproportionate focus on kidney transplantation with a lack of progress for extrarenal organ transplants in addition to tissue or cell transplantation. The lack of a solid DD transplant system also comes with increased potential for organ trafficking, coercion, and transplant tourism that will need to be closely monitored and prevented. Moreover, there is a strong necessity for documenting national data to improve safety, quality, efficacy, epidemiology, and ethics. We therefore urge the government to take actions addressing transparency, allocation, policies, oversight, and safety of organ transplantation. India has an opportunity to use its full potential to expand transplantation in accordance with World Health Organization Guiding Principles on human cell, tissue, and organ transplantation.,The authors are grateful for the editing support that they have received from Stefan G. Tullius, MD, PhD, Harvard Medical School, Boston, MA."
93,93,586,37309031,https://www.doi.org/10.1097/TP.0000000000004688,https://journals.lww.com/,"Social media has radically reshaped modern scientific discourse. The field of transplantation is unique in intersecting medicine, technology, immunology, ethics, and public policy like no other. Social media can rapidly disseminate this information globally.1-5 This was most evident during the coronavirus disease 2019 pandemic when transplant teams published several studies demonstrating the effects of vaccination in posttransplant patients.6-11,Social media is an entirely different means of communication compared with traditional media. It spreads the news in an accessible manner and makes it easy to share content too. Measuring the impact of this phenomenon in transplantation is still a novel concept.12-16 Because of the increasing adoption of social media by transplant teams, we conducted a survey to assess how social media is being used to promote current and new research in transplantation.,We electronically distributed surveys to subscribers to the weekly email of newly published articles in the Transplantation or Transplantation Direct journals, those who have submitted an article to the Transplantation or Transplantation Direct journals in the past 5 y, members of The Transplantation Society (TTS), and subscribers to newsletters from the TTS (Tribune and Tribune Plus) from November 17 to December 31, 2022. Surveys were distributed via email to the Transplantation journals’ editors, authors, and reviewers, through the TTS Tribune Plus newsletter, on social media (Twitter, LinkedIn, and Facebook), and the TTS web site. The survey link was reposted to social media pages several times throughout the dissemination period and 1 follow-up email was sent on December 12, 2022. The survey was hosted by Qualtrics (Provo, UT) and was available on mobile phones and computers. This study (s22-00681) was deemed exempt by the NYU Langone Health Institutional Review Board.,This survey study was designed to describe respondent profession and areas of scientific interest, how and why they use social media for professional purposes, their awareness of various engagement measures, and how they plan to use it for professional purposes. Five screener questions were included before the survey to ensure that participants were aged ≥18 y and that they were involved in the Transplantation journal or the TTS community.,The 23-item web-based survey consisted of 21 closed-ended questions and 2 free-text response questions. Nine of the closed-ended questions included free-text response options such as “other” and “please list the languages you use.” The survey was developed by a team with expertise in survey development, qualitative research, social media research, and transplantation. Nine researchers (outside the project team) pilot-tested the survey to enhance face validity before distribution.,All analyses were performed using SAS OnDemand for Academics Version 9.4 (SAS Institute Inc). Frequencies, percentages, and medians were calculated to describe the survey sample. Free-text response questions were analyzed using inductive categorization. The first coder inductively categorized the responses using Microsoft Excel version 16.63.1 and met with the second coder to reach a consensus on categories and ensure reliability.,See Table 1 for a summary of key findings.,From a total of 110 respondents, 6 were removed because of ineligibility and a further 10 because of not providing any responses. Four respondents submitted incomplete responses, but these were included in the analysis (n = 94).,Seventy-two percent of respondents reported that they were either physicians or surgeons, and the majority of them worked in hospitals (74%) and universities (71%). More than half reported that their primary area of scientific focus was clinical science (60%) and that their primary area of focus was on the kidneys, liver, or pancreas (60%). Respondents worked in 32 different countries but mostly in the United States (25%), Italy (8%), Australia (8%), India (6%), and Spain (5%).,Ninety-four percent of respondents reported using at least 1 social media platform for professional purposes (median = 2) and 95% reported using 1 in the past 30 d. The top 2 platforms were LinkedIn (56%) and Twitter (54%), followed by Facebook (28%), YouTube (21%), and Instagram (15%). Very few respondents reported using TikTok, Snapchat, or others (ie, Mastodon and ResearchGate). Respondents used these platforms to identify new publications (61%), learn (56%), conference updates (45%), engage with opinion leaders (44%), discuss with colleagues (40%), network (49%), and for other reasons such as recruiting study participants (9%). Most respondents (60%) reported that they did not follow Transplantation or Transplantation Direct Twitter accounts before taking the survey.,More than half of respondents (54%) reported using languages other than English to communicate on social media for professional purposes. The top 5 languages other than English were Spanish, Italian, French, German, and Dutch.,Seventy-one percent of respondents have had their work shared via social media, but only 54% have had their institution share their work on social media. Respondents shared their work on social media through Tweets, retweets, or likes (50%), visual abstracts (36%), LinkedIn (32%), Facebook (26%), online journal clubs, webinars, and blogs (19%), live tweeting (15%), Instagram (13%), YouTube (9%), and other methods on Mastodon, Research Gate, and TikTok (6%).,Social media benefits respondents professionally in the following ways: keeping up with cutting-edge research (65%), identifying collaborations (41%), participating in informal discussions with colleagues (37%), increasing their standing in their field (31%), becoming aware of new calls for grants and deadlines (24%), and mentoring and being mentored (15%).,Respondents listed several reasons why they chose specific social media platforms, including to increase visibility and impact of work; to share information with colleagues, patients, and research participants; because certain platforms (like Twitter and LinkedIn) are deemed more “professional” because they are used by colleagues; because they have a user-friendly interface; and because they already use these platforms for personal purposes.,The Altmetric Attention Score is an automatically calculated, weighted count of all of the attention a research output has received on the basis of volume, sources, and authors.17 Before taking the survey, more respondents were aware of the Altmetric donut of their publications (64%) rather than the Altmetric score (53%).18 Moreover, only 24% of respondents were aware of their h-index before taking the survey. This latter readout is intended to represent both the productivity and the impact of the scholars’ work.19,Most respondents reported that they were very (39%) or somewhat (46%) likely to use visual abstracts to disseminate their research. Sixty-eight percent said that they found visual abstracts created through Tidbit to be more appealing than the classic visual abstracts created by Transplantation.,More than half of the respondents (61%) reported that they would consider using social media to promote patient education and research engagement, 30% were not sure, and 9% would not consider it.,The following concerns about using social media were noted: information quality (because of lack of online moderators, oversimplification, or exaggeration of results), inability to control responses from disruptive users, inability to control information misuse, privacy concerns, time-consuming or distractive nature of the platforms, and, finally, the preponderance of social media platforms toward the younger generations.,This survey has demonstrated significantly diverse purposes driving their active engagement of transplant teams on social media. The majority of respondents were clinicians from the United States/Europe working in hospitals and universities. They broadly used LinkedIn and Twitter to learn; keep up with novel research and conferences; and improve their reputation, network, and identify new collaborations. Interestingly, most respondents would use social media in the future to increase patient engagement. This is important because modern-day patients play active roles in their health and engage regularly with other patients through online communities.20,21,Previous surveys of surgeons identified Facebook as the most regularly used social media platform, whereas only 20% regularly used Twitter.12,22 In comparison, we found that LinkedIn and Twitter are the most commonly used platforms. Sandal et al16 recommended that social media platforms should be used to engage with the transplantation community, including patients and donors. Almost half of our respondents reported that they already used these platforms to engage with opinion leaders and networks, but far fewer engaged with patients on social media. These results highlight that social media users in the transplantation community are a heterogenous cohort with different professional goals.,Measuring the impact of social media was not assessed in previous surveys of the scientific community on social media.12,13,16 Among our respondents, we found that half were aware of the Altmetric score/donut; however, few knew their h-index. Future studies will delineate if such metrics are indicators of future citations or merely the reputation of the individual disseminating the work. Scientific journals may also need to look to other industries for inspiration. For example, the marketing industry prefers economically tangible metrics such as site traffic, brand loyalty, sales, and customer engagement.23,24,Importantly, the vast majority of our respondents would consider using social media for future patient engagement. Patients actively use platforms such as Twitter and Instagram, so it is reasonable that tailored posts democratize healthcare literacy by raising awareness of drug trials and health policies.25-27,Finally, our respondents also raised professionalism concerns. The unchecked nature of posts combined with unverified user credentials risks disseminating false information.28 Individuals posting inaccurate information or displaying unprofessional traits risk their credibility as well as that of their profession and institution.13,16 Another vital issue is patient privacy. Transplant patients are limited to tertiary medical centers for their care, so clinicians discussing cases online have to be tactful to protect patient confidentiality.,The key limitations of our survey are selection bias and the low response rate. This survey was shared through social media channels (Twitter/LinkedIn) as well as the mailing lists of the Transplantation journal and TTS. This would have attracted responses from those already engaged in scientific scholarship and social media. Nevertheless, our data demonstrated survey engagement from a global audience with diverse backgrounds. This is important because it demonstrates the wide reach of the transplantation community online. With the increasing uptake of social media use, this will only improve.,Overall, our survey has demonstrated that scientists and physicians in Transplantation are increasingly using social media to promote their work and interconnect. We recommend that future studies focus on how social media use impacts the creation of purpose-built physician/patient communities and the promotion of organ donation and patient engagement."
94,94,592,37389648,https://www.doi.org/10.1097/TP.0000000000004716,https://journals.lww.com/,"Kidney transplant recipients (KTRs) are at higher risk of poorer outcomes post–severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection. Although vaccination and antispike monoclonal antibodies (mAbs) may have mitigated risk, the emergence of more immune-evasive Omicron subvariants (eg, Omicron XBB/BQ.1.1) has raised concern. Failure of mAbs to neutralize XBB/BQ.1.1 has been reported.1 Additionally, sera of KTRs vaccinated with monovalent messenger RNA vaccines could not neutralize Omicron in vitro.2 We describe outcomes in a vaccinated single-center cohort of KTRs over 1 y (January 1–December 28, 2022) during which successive Omicron subvariants predominated community transmission, including Omicron XBB.,Demographic and clinical data were extracted from the electronic medical record, including information on vaccination and therapeutics. Antivirals were offered to all eligible patients unless contraindicated; remdesivir was initially offered for 3 d, with extension based on physician discretion. Patients with undetectable SARS-CoV-2 immunoglobulin G response were offered mAbs. Study outcomes were progression to severe coronavirus disease 2019 (COVID-19), acute kidney injury (AKI), and COVID-19–related readmission. Readmission within 30 d of symptom onset was defined as COVID-19 related if readmission (after initial hospitalization) was related to symptom persistence, disease progression, or relapse. KTRs who notified transplant coordinators of infection within a week of onset were recalled for clinical assessment and triaged based on severity/home isolation suitability to either initial hospitalization in our isolation ward or in a home-based virtual ward where remote monitoring via telemedicine and home visits by family physicians were conducted for clinical review/treatment. This allowed measurement of antispike antibody titer at symptom onset and determination of variant via a polymerase chain reaction. The cohort was initially stratified by infection period (BA.1/2/4/5/XBB) based on local surveillance data.3 Omicron XBB predominated (≥50% of circulating strains) from October 2022.3 During transitions between predominant strains, spike gene target failure on polymerase chain reaction was used to distinguish variants. Undetectable SARS-CoV-2 immunoglobulin G was defined as <50 AU/mL. Multivariate logistic regression was used to identify factors independently associated with AKI/severe COVID-19/readmission. Institutional review board approval was obtained (CIRB:2021/2823).,Overall, 427 cases were included, of which 74 (17.3%) were attributed to Omicron XBB (Table S1, SDC, https://links.lww.com/TP/C811); 92.3% of cases (393/427) received ≥3 messenger RNA vaccine doses; one-third (32.9%; 135/410) had undetectable anti-spike antibodies. In the Omicron era, only 2.6% of cases (11/427) progressed to severe COVID-19, 10.8% (46/427) sustained AKI, and 3.5% (15/427) were readmitted. However, 66.0% of cases (282/427) received treatment. There was no difference in outcomes (Table S1, SDC, https://links.lww.com/TP/C811) during Omicron XBB and preceding variant periods on univariate analysis (AKI: 10.8% versus 10.8%; odds ratio [OR] = 1.00; 95% confidence interval [CI], 0.45-2.25; readmission: 4.2% versus 3.4%, OR = 1.20, 95% CI = 0.33-4.37; severe COVID-19: 0.0% versus 3.1%, P = 0.224) possibly because of increased booster uptake, residual immunity from prior infection, and higher antiviral usage during the XBB period (Table S1, SDC, https://links.lww.com/TP/C811). On multivariate analysis, undetectable anti-spike antibodies were independently associated with anti-spike readmission (Table 1). In the subset with undetectable anti-spike antibodies who received treatment before XBB emergence (N = 119), treatment with mAbs was associated with lower odds of severe COVID-19 (adjusted OR = 0.11; 95% CI, 0.02-0.62; P = 0.013; Table 1). Rates of severe COVID-19 ranging from 6.3% to 22.2% were reported in various KTR cohorts during the Omicron era,4,5 albeit during waves driven by earlier variants (Table S2, SDC, https://links.lww.com/TP/C811). Association of mAbs with reduced severity among serological nonresponders before Omicron XBB emergence highlights the need for updated therapeutics."
95,95,593,37677940,https://www.doi.org/10.1097/TP.0000000000004782,https://journals.lww.com/,"Renal dysfunction is a well-established risk factor for heart transplant mortality, for which simultaneous heart kidney transplantation (SHKT) has been widely adopted. Current consensus recommends consideration for SHKT based on an estimated glomerular filtration rate (eGFR) <30 mL/min/1.73 m2, or presence of additional risk factors such as small kidney size or proteinuria in those with higher eGFR.1 There is graduated benefit based on the degree of renal dysfunction, with superior survival in patients requiring pretransplant dialysis proceeding to SHKT (median 12.4 y), compared with their heart alone transplant (HAT) counterparts (median 9.9 y).2 As a consequence, SHKT numbers have increased exponentially, with a 650% increase in activity over the 20-y period from 2000 to 2019.2 Organ availability will inevitably limit this ongoing increase, as SHKT diverts available organs away from patients with end-stage kidney disease. Moreover, a recent retrospective analysis of the UNOS Registry found that many patients who met the consensus guideline for SHKT had excellent posttransplant outcomes after HAT.3 This is of particular importance as the OPTN/SRTR 2020 annual data reports for transplantation described median time to heart transplant of 2.7 mo, whereas the median has not been calculable in kidney transplants since 2009 as <50% of each year’s cohort has undergone transplant.4,5,In 2018, there was a change to the heart allocation policy (HAP) in the United States, aimed at reducing waitlist times for the highest priority patients and reducing waitlist mortality. Under the new HAP, patients on temporary mechanical circulatory support are given the highest priority, whereas patients supported with durable left ventricular assist device (LVAD) in the absence of complications are listed at status 4. Overall, the waitlist time has decreased from 112 to 39 d.6 Even among durable LVAD patients, median waitlist has decreased from 140 to 37 d, with the important caveat that the majority of transplanted patients belonged to higher status categories, indicating they had already experienced complications.6 Stable status 4 LVAD patients had a lower incidence of transplantation at 360 d.6 UNOS registry analysis suggests no difference in waitlist survival in bridge to transplant (BTT) LVAD patients since the new HAP; however, 1-y posttransplantation survival appears worse (91.7% old era versus 83.4% new era, log-rank P < 0.001).7,Outcomes in patients supported with durable LVAD proceeding to SHKT also appear less promising since the change in HAP. In a recent UNOS analysis of BTT LVAD patients with stage 3 or higher chronic kidney disease (CKD), survival at 1 y was 80.3% following SHKT, compared with 88.3% following HAT, with this difference persisting out to 5 y (65.5% versus 75.7%, respectively).8 Among LVAD patients receiving pretransplant dialysis, 1-y survival was reported as unacceptably low for both HAT and SHKT (82.6% for HAT and 76.3% for SHKT).8,In this issue of Transplantation, Fraser et al have further analyzed the UNOS registry to describe the discrepant outcomes in LVAD patients proceeding to SHKT compared with HAT, with a specific focus upon differences in posttransplant outcomes following the new HAP in 2018.9 They compared outcomes following SHKT before and after the HAP change, as well as SHKT compared with HAT for each era. Their analysis highlights the evolution in practice, with SHKT constituting 13.6% of transplants following the new HAP, compared with only 0.03% prior. Patients in the new era proceeding to SHKT appeared to have higher eGFR and were less likely to be on dialysis. However, the authors highlight these patients may have been sicker than their historic counterparts, with more frequent requirement for intensive care unit admission prior to transplant. The salient finding is of worse 1-y survival in SHKT patients following the change in HAP. Furthermore, they noted a difference in survival between SHKT and HAT following the policy change, a difference that was not apparent under the old HAP.,Postulated reasons for era-based differences in SHKT survival are the lower waitlist status for clinically stable LVAD patients, with higher priority only applied once patients had experienced life-threatening complications. These complications increase the risk of life-threatening complications such as vasoplegia and massive blood loss in the immediate posttransplant period.10 An unintended consequence of the change in HAP is increased ischemic time and travel distances for donor hearts, potentially increasing the risk of severe primary graft dysfunction.7 This risk could be mitigated by oxygenated machine perfusion of donor hearts, which reduces ischemic time and importantly allows the surgeon more time to explant the recipient heart and ensure hemostasis before implantation of the donor heart.11,Another important consideration is whether changes in practice with wider adoption of SHKT is resulting in more comorbid patients proceeding to dual organ transplantation. Posttransplant dialysis data were not reported in the present study, which may be important, as severe acute kidney injury with requirement for dialysis has historically favored survival in SHKT compared with HAT patients.2 It is also unclear whether the short-term difference in survival continues beyond the first year, although the early separation in survival curves with parallel trajectory, in conjunction with previous data published by Atkin et al suggests that it does.8,There appears to be a clear and alarming signal for increased mortality following SHKT in durable LVAD recipients with CKD, possibly related to the change in HAP, resulting in rapid transplantation of patients with major LVAD-related complications and delayed transplantation of clinically stable LVAD-supported patients. If ongoing analysis suggests the change in HAP has negatively impacted the posttransplant survival of BTT LVAD patients with or without CKD, efforts to better risk stratify and reprioritize these patients will be necessary to reverse this trajectory. One potential alternative approach for urgently listed LVAD patients with CKD is kidney after heart transplantation with a safety net as proposed in the recently published American Heart Association Scientific Statement on dual organ transplantation.12 Better identification of adverse prognostic factors in LVAD-supported patients with CKD and better preservation of donor organs are key."
96,96,605,36199169,https://www.doi.org/10.1097/TP.0000000000004375,https://journals.lww.com/,"After intestinal transplantation (ITx), up to 24% of patients with premorbid obesity will have persistence or relapse of obesity.1 This is important because obesity is an independent risk factor for graft loss, delayed graft function, and reduced patient survival.2 Bariatric surgery is the most effective and durable treatment for morbid obesity. Beneficial effects of bariatric surgery are in part due to altered gastrointestinal physiology and gut–brain–endocrine signaling pathways, which regulate hunger, satiety, and nutrient metabolism.3 In regard to the transplanted intestine, it is unclear whether these pathways can still be therapeutically used because there is intestinal lymphatic disruption and enteric denervation.4 When one of our young intestinal graft recipients suffered from morbid obesity and inquired about bariatric surgery, we decided to consider this possibility. We want to share our experience because, to our knowledge, bariatric surgery has not been performed on recipients of an intestinal graft. The institutional review board and patient consented for this case report (S65120).,A 46-y-old female patient underwent combined liver-ITx for short bowel syndrome and intestinal failure–associated liver disease following mesenteric ischemia. She was previously overweight with a body mass index (BMI) of 29 kg/m2. At the time of ITx, she weighed 58 kg (BMI: 19 kg/m2). Within 1-y post combined liver-ITx, she regained her premorbid weight. She developed a metabolic syndrome: arterial hypertension treated with amlodipine and hypercholesterolemia treated with statins. Lifestyle changes, diet and exercise, did not result in significant weight loss. There were no signs of nonalcoholic fatty liver disease; a liver biopsy was not performed. There was no diabetes. Nine years after combined liver-ITx, at a weight of 106 kg and BMI of 35 kg/m2, laparoscopic sleeve gastrectomy was performed after careful multidisciplinary consideration (Figure 1). Despite extensive intra-abdominal adhesions, the procedure was uneventful. To better assess anatomy, surgery was performed with endoscopy assistance. At 1.5-y follow-up, her weight decreased to 78 kg (BMI: 26 kg/m2, 29% total weight loss). The procedure had no effect on liver or intestinal graft function, nor on absorption of immunosuppressive drugs. There was a temporary decline in renal function due to reduced oral intake, which fully recovered. Within 6 mo after sleeve gastrectomy, arterial hypertension and hypercholesterolemia resolved, and medication was stopped.,Sleeve gastrectomy was the preferred procedure for our patient. Not having to alter the gastrointestinal tractus, as opposed to bypass procedures, had important benefits. Firstly, adhesiolysis of the transplanted small bowel could be avoided. Secondly, the uptake of medication is more reliable after sleeve gastrectomy. This is especially true for immunosuppressive drugs, which are primarily taken up by the duodenum.5,To conclude, laparoscopic sleeve gastrectomy was safely performed, with expected outcome in terms of weight reduction. The effect of transplanting the gastrointestinal tract on its function and neurohormonal physiology requires further investigations. In future cases, we plan to test for gut hormones, such as glucagon-like peptide-1, before and after bariatric surgery. These insights could additionally aid in unraveling the complex pathophysiology of obesity and metabolic syndrome. Bariatric surgery should be considered to treat motivated patients with morbid obesity after ITx."
